■ R 수업  

 * R 수업 일정

  1. R 기본 문법을 익히는 수업 (1주일)
        - R 을 배워야하는 이유
        - R 의 자료구조
        - R 의 연산자
        - R 의 함수 
        - R 에서의 조인
        - R 에서의 서브쿼리 
        - R 에서의 그래프 
        - R 에서의 if 문과 loop 문 사용법 

  2. R 을 활용한 머신러닝 (2주일)

        - 머신러닝의 종류 3가지
            1. 지도학습 :
                  분류 : 의사결정트리, 신경망, 
                         서포트 백터 머신, 나이브 베이즈,
                         knn 
                  회귀 : 선형회귀 
            2. 비지도 학습 : K-means 
            3. 강화학습     

  3. 통계수업 (오전 통계 수업, 오후 포트폴리오 제작)
         - 데이터의 종류 
         - 데이터 수치 요약 
         - 확률과 확률변수 
         - 확률과 표본분포
         - 통계적 추정 
         - 통계적 가설검정 

  R 포트폴리오 --->  개별로 만들어도 되고 
                     그룹으로 만들어도 된다. 

  데이터 분석가 와                      딥러닝 연구원 
     ↓                                     ↓
  R , python , 기본 통계지식 필수 사항    python, 딥러닝
    
    포트폴리오                             포트폴리오 

■  R 을 배워야하는 이유

 " 데이터 분석가 모집요망에 SQL 과 R , Python 은 필수이다 "

 카카오톡이 2010년 3월에 서비스를 시작한 이후
 폭발적으로 성장해서 2012년 말에 8000 만명의 가입자를 넘어
 섰다.
  
 하루 평균 43분간 카카오톡을 하고 하루 방문자가 2700만명,
 하루 최대 메세지 이용건수가 42억건에 달함.

 문자해 ---> 카톡해

 이런 성공에도 불구하고 마땅한 수익모델이 없어서

 2010년 41억 적자 
 2011년 153억 적자 

         <------ 카톡 사용자들에 대한 data분석이 있음 다음
                 부터 
 2012년에 애니팡의 인기에 힘입어 흑자라인으로 들어섰다.
 개발자 통장에 하루 1억씩 입금 
 
 중간에 카톡 사용자들을 어떻게 분석 

 1. 누가(성별,연령)
 2. 무엇을(게임 아이템)
 3. 언제 ( 아이템 구매시간)
 4. 얼마나 (아이템 구매빈도)
 5. 어떻게 (결제 방법)

■ R 설치 

  - 기본 R 프로그램

  - R studio  <--- SQL gate, spider, pycham 와 같은툴 

■ 작업 디렉토리 변경 및 emp.csv  로드 하는 방법 

> setwd("d:\\data")
> 
> emp <- read.csv("emp.csv", header=T)
> 
> emp

■ SQL 과 R 의 차이 ?

 " 아주 긴 SQL 코드를 R 코드로는 단순하게 작성 할 수 있다"

SQL>  select  deptno, sum(decode(job,'SALESMAN',sal,0) ),
                      sum(decode(job,'ANALYST',sal,0) )
       from  emp
       group  by deptno ;

R>  attach(emp)

R>  tapply(sal, list(deptno,job), sum) 

  " 데이터를 시각화 할 수 있다 "

R> pie(emp$sal, col=rainbow(14) ) 

■ R 이란 무엇인가 ?

 뉴질랜드의 aukland 대학의 robert gentlman 과 
 Ross ihaka 가 1995년에 개발한 소프트웨어이고 
 데이터 분석을 위한 통계 및 그래픽스를 지원하는
 무료 소프트웨이다.

■ R 을 왜 사용해야하는가 ?

 1. R  is  free
 2. data 분석을 위해서 가장 많이 쓰는 통계 플랫폼
 3. 복잡한 데이터를 다양한 그래프로 표현할 수 있다.
 4. 분석을 위한 데이터를 쉽게 저장하고 조작할 수 있다.
 5. 누구든지 유용한 패키지를 생성해서 공유할 수 있고
    새로운 기능에 대한 전달이 빠르다.
 6. 어떠한 os 에서도 설치가 가능하다.
    심지어 아이폰에도 설치가 된다. 

■ R 의 자료구조 
 
    그림  

1. vector  : 같은 데이터 타입을 갖는 1차원 배열구조 
2. matrix  : 같은 데이터 타입을 갖는 2차원 배열구조
3. array   : 같은 데이터 타입을 갖는 다차원 배열구조 
4. data.frame : 각각의 데이터 타입을 갖는 컬럼으로 
                이루어진 2차원 배열구조 
                (rdbms 의 테이블과 유사함)

예:     오라클       vs       R

      desc   emp            str(emp)

5. list  :  서로 다른 데이터 구조(vector, data frame,
            matrix, array) 인 데이터 타입이 중첩된 구조 


■ 기본 데이터 검색 

문제1.  emp 데이터 프레임에서 이름과 월급을 출력하시오 !

> emp[ 행 , 열 ]

> emp[       , c("ename","sal") ]
               ↑
             combine 

문제2. 월급이 3000 인 사원들의 이름과 월급을 출력하시오 !

> emp [ emp$sal == 3000, c("ename","sal") ]

> attach(emp)

> emp [ sal == 3000, c("ename","sal") ]

문제3. 월급이 2000 이상인 사원들의 이름과 월급을 출력하시오!


> emp[ emp$sal >= 2000 , c("ename","sal") ]

■ R 에서 사용하는 연산자 총정리 

1. 산술 연산자 :  *   /  +  -
2. 비교 연산자 :  >,  <, >= , <=, ==, != 
3. 논리 연산자 :   &  : and (백터화된 연산)
                  &&  : and (백터화 되지 않은 연산)
                   |  : or  (백터화된 연산)
                  ||  : or  (백터화 되지 않은 연산) 
                   !  : not  

■  백터화된 연산   vs  백터화되지 않은 연산 

 예:  x <- c(1,2,3)
      x > c(1,1,1)  &  x < c(3,3,3)

      x  <- 1
      x  > -2  &&  x < 2 

문제4. 직업이 SALESMAN 이 아닌 사원들의 이름과 월급과 직업을
       출력하시오 !

> emp [ job != "SALESMAN", c("ename","sal","job") ]

문제5. 직업이 SALESMAN 이고 월급이 1000 이상인 사원들의
       이름과 월급과 직업을 출력하시오 ! 


■ R studio 설치  


■ R shiny 설치 

install.packages("shiny")

library(shiny) 

runExample("01_hello") 

runExample("08_html")

runExample("02_text")

runExample("03_reactivity")

runExample("04_mpg")

runExample("05_sliders")

runExample("06_tabsets")

runExample("07_widgets")

runExample("08_html")

runExample("09_upload")

runExample("10_download")

runExample("11_timer")

■ 연결 연산자  

       오라클      vs        R
 
         ||                 paste   

문제6.  아래와 같이 결과를 출력하시오 !  

SQL>  select  ename || '  의  직업은  ' || job
        from  emp;

> paste( emp$ename , ' 의 직업은  ' , emp$job) 

install.packages("data.table")

library(data.table)

data.table(  paste( emp$ename , ' 의 직업은  ' , emp$job) )

■ 기타 비교 연산자 

       SQL          vs            R  

1.     in                        %in%
2.    like                       grep
3.   is  null                    is.na
4.   between .. and             emp$sal >= 1000 &
                                emp$sal <= 3000

문제7. 직업이 SALESMAN, ANALYST 인 사원들의 이름과 직업을
       출력하시오 ! 

>  emp[행,열]

> emp[ emp$job %in% c("SALESMAN","ANALYST"), c("ename","job") ] 
                    ↑
                  combine 

문제8. 직업이 SALESMAN, ANALYST 가 아닌 사원들의 이름과 직업을
       출력하시오 ! 

> emp[!emp$job %in% c("SALESMAN","ANALYST"), c("ename","job") ] 

문제9. 커미션이 null 인 사원들의 이름과 월급과 커미션을 출력
       하시오!

> emp[ is.na(emp$comm),  c("ename","sal","comm") ]

※ R 에서의 null 값 :
                                        검색하는 방법 

      1. NULL  ( 아무것도 없다 )   ---->  is.null()
      2. NA    ( 결손값 )          ---->  is.na()
                   ↓
         어느 부분이 없거나 잘못되서 불완전한 상태 
      3. NaN   (비수치)            ---->  is.nan()         
          ↓
       Not a Number 

※ NULL( 아무것도 없다) 를 활용하는 때는
   반복문으로 처리할 오브젝트의 초기값을 NULL 설정 

   x <- NULL
   for (i in 1:10)  x <- append(x,i*i)
   x

문제10. 커미션이 NA 가 아닌 사원들의 이름과 월급과 커미션을 
        출력하시오 !

> emp[ ! is.na(emp$comm),  c("ename","sal","comm") ]

문제11. 이름의 첫번째 글자가 A 로 시작하는 사원들의 이름과 
        월급을 출력하시오 !

> emp[ grep("^A.*", emp$ename), c("ename","sal") ] 

설명:    ^ : 첫번째 
         $ : 마지막 
         . : 한자리수 
         * : wild card(%) 

문제12. 이름의 끝글자가 T 로 끝나는 사원들의 이름과 월급을 
        출력하시오 !

             ^A
> emp[ grep("T$.*", emp$ename), c("ename","sal") ] 

문제13. (점심시간 문제)  이름의 두번째 철자가 M 인 사원들의 
        이름과 월급을 출력하시오 ! 

■ 중복제거 

     SQL         VS           R     

    distinct               unique  

문제14. 부서번호를 출력하는데 중복제거해서 출력하시오 !

> unique(emp$deptno)

> library(data.table)

> data.table("부서번호" = unique(emp$deptno) ) 
                ↑
              컬럼명 

■ 정렬 작업 

      SQL        vs         R  

   order  by             1. data frame 에서 order 옵션
                         2. doBy 패키지를 설치하고 
                            orderBy 함수를 사용 

문제15. 이름과 월급을 출력하는데 월급이 높은 사원부터 
        출력하시오 !

> emp <- read.csv("emp.csv", header=T)
   
> str(emp) 

> emp[ order(emp$sal, decreasing=T), c("ename","sal") ]
   ↑
 Data frame  :  emp[ 행 , 열 ] 

문제16. 이름과 입사일을 출력하는데 먼저 입사한 사원부터 출력
        하시오 !

> emp[ order(emp$hiredate, decreasing=F), c("ename","hiredate")]

문제17. 직업이 SALESMAN 인 사원들의 이름과 월급과 직업을
        출력하는데 월급이 높은 사원부터 출력하시오 ! 

> emp [ emp$job =='SALESMAN', c("ename","sal","job") ]

> x <- emp [ emp$job =='SALESMAN', c("ename","sal","job") ]

> x 

> str(x) 

> x [ order( x$sal, decreasing=T),    ]  

* 모든 변수를 다 보고 싶을때 

> ls()  

* x 라는 변수를 지워 버리고 싶으면 ?

> rm(x)

> ls()  

문제18.  이름와 월급을 출력하는데 월급이 높은 사원순으로
         출력하는것을 doBy 패키지를 이용해서 출력하시오 !

install.packages("doBy")

library(doBy)

orderBy( ~-sal, emp[    , c("ename","sal") ] )

문제19. 직업이 ANALYST 가 아닌 사원들의 이름과 월급과 직업을
        출력하는데 월급이 높은 사원부터 출력하시오 !

orderBy( ~-sal, emp[emp$job !='ANALYST' , c("ename","sal","job")])

문제20. 카페에서 crime_loc.csv 를 내려받고 R 에 로드한후에
        살인이 일어나는 장소와 건수를 출력하는데
        살인이 일어나는 건수가 높은것부터 출력하시오 !

crime_loc <- read.csv("crime_loc.csv", header=T)

attach(crime_loc)

orderBy( ~-건수, crime_loc[범죄=='살인', c("장소","건수") ])

문제21. 범죄의 유형이 무엇이 있는지 중복제거해서 출력하시오 !

답 :  data.table('범죄 유형'=unique(crime_loc$범죄))


성호가 찾은 변수 다 지우는 코드

rm(list=ls())

문제22. 부서번호가 10번, 20번인 사원들의 이름과 월급과 부서번호를
        출력하는데 월급이 높은 사원부터 출력하시오 ! 

library(doBy)

orderBy( ~-sal, emp[emp$deptno %in%  c(10,20),
                   c("ename","sal","deptno") ] ) 

■ 함수 

 1. 문자함수
 2. 숫자함수
 3. 날짜함수
 4. 변환함수
 5. 일반함수 

■ 문자함수 

       SQL       vs       R   

      upper             toupper 
      lower             tolower
      substr            substr
      replace           gsub 

문제23. 이름과 직업을 출력하는데 소문자로 출력하시오 ! 

library(data.table)

data.table(이름=tolower(emp$ename), 직업=tolower(emp$job) ) 

cbind(tolower(emp$ename), tolower(emp$job) )

문제24. (R 로 함수 생성)  아래와 같이 이름을 물어보게하고
        이름을 입력하면 해당 사원의 이름과 월급이 출력되는 
        R 코드를 작성하시오 !

find_sal <- function()  {  

            response <- readline(prompt ='이름을 입력하세요')
            x <- emp[emp$ename==response,c("ename","sal") ]
            print (x)

                        }
find_sal() 

문제25. 위의 문제를 다시 해결하는데 이름을 소문자로 입력해도
        출력되게 하시오 !

find_sal <- function()  {  

         response <- readline(prompt ='이름을 입력하세요')
         x <- emp[emp$ename==toupper(response),c("ename","sal") ]
         print (x)

                        }
find_sal() 


       SQL       vs       R   

      upper             toupper 
      lower             tolower
      substr            substr
      replace           gsub 

문제26. (substr함수)  이름의 두번째 철자가 M 인 사원들의 이름과
        월급을 출력하는데 substr 함수를 사용해서 출력하시오 !

emp [ substr(emp$ename,2,2) =='M', c("ename","sal") ]

              행                          열 

※ 설명 :  substr( x, start, stop )

           substr(emp$ename,2,2)      S M I T H  
                                      1 2 3 4 5
           이름의 두번째 자리부터 두번째 자리까지 

문제27. 이름을 출력하고 이름 옆에 이름의 첫번째 철자부터 세번째
        철자까지 출력하시오 !

 KING    KIN
 SMITH   SMI
   :      :

>  data.table(이름=emp$ename, 이름=substr(emp$ename,1,3) )

       SQL       vs       R   

      upper             toupper 
      lower             tolower
      substr            substr
      replace           gsub 

■ gsub  설명  --->   gsub('h','H', text )

                      특정 text 에서 소문자 h 를 대문자 H 로
                      변경해라 ~

문제28. 이름과 월급을 출력하는데 월급을 출력할때에 숫자 0 을
        * 로 출력하시오 ! 

 data.table( emp$ename,  gsub( 0 ,'*', emp$sal) )


 select  ename, replace(sal,0,'*')
   from  emp;

문제29.  이름과 월급을 출력하는데 월급을 출력할때에 
         숫자 0,1,2 를 * 로 출력하시오 !

 data.table( emp$ename,  gsub( '[0-2]' ,'*', emp$sal) )


 딥노이드 회사 -->   웹브라우져를 건강보험 심사평가원에 구현 


■ 숫자 함수 

     SQL         vs        R  

1.  round                 round 
2.  trunc                 trunc
3.  mod                    %%
4.  power                   ^   (예: 2^3)  

문제30. 6의 9승을 출력하시오 ! 

  6^9 

문제31. 10을 3으로 나눈 나머지값이 무엇인가 ? 

  10%%3

문제32. 이름과 연봉을 출력하는데 연봉은 월급에 12를 곱해서
        출력하고 컬럼명을 한글로 연봉으로 출력되게 하시오 !

> data.table(이름=emp$ename, 연봉=emp$sal*12) 

문제33. 이름과 연봉을 출력하는데 연봉이 높은것부터 출력하시오!

 library(doBy)

 orderBy( ~-연봉, data.table(이름=emp$ename, 연봉=emp$sal*12) )

문제34. 위의 결과를 다시 출력하는데 round 함수를 이용해서
        아래와 같이 백단위에서 반올림되게 하시오 !

  4:  JONES 35700   --------->   36000  

 > ?round 

library(doBy)

orderBy( ~-연봉, data.table(이름=emp$ename, 
                            연봉= round(emp$sal*12,-3) )  ) 

  3  5  7 0  0
 -5 -4 -3 -2 -1

※ R 의 round 의 특징 

   122 ------------------------------- 123 
                    122.5

 round(122.5)  --->  122 선택

   123 -------------------------------- 124
                     123.5 

 round(123.5)  --->   124 선택 

    " R 은 짝수를 좋아한다 " 

문제35. 문제34번 결과를 다시 출력하는데 백자리 포함해서
        그 이후를 다 버려서 출력하시오 ! 

library(doBy)

orderBy( ~-연봉, data.table(이름=emp$ename, 
                            연봉= trunc(emp$sal*12,-3) )  )

※ 설명 : trunc 는 소수점 이후만 가능하다.

         3  5  6  .  5  6  7
        -3 -2 -1  0  1  2  3
   
■ 날짜 함수 

       오라클       vs        R   
 
       sysdate               Sys.Date() 
       add_months            difftime 
      months_between         사용자 정의 함수 
       last_day              사용자 정의 함수 
       next_day              사용자 정의 함수 

문제36. 오늘 날짜를 출력하시오 !  

  Sys.Date() 

문제37. 이름, 입사한 날짜부터 오늘까지 총 몇일 근무했는지 출력
        하시오 ! 

  날짜 - 날짜 

  날짜로 변환하는 함수 ?

    오라클      vs      R  

    to_date            as.Date

data.table( emp$ename, Sys.Date() - as.Date(emp$hiredate) )

문제38. 오늘날짜의 달의 마지막 날짜를 출력하시오 !

SQL>  select  last_day(sysdate) from dual;

R  :  install.packages("lubridate")
      library(lubridate)

      last_day <- function(x) {  

           ceiling_date( x , "month") - days(1) 

                              } 

      last_day( Sys.Date() ) 

      ceiling_date( Sys.Date(), "month")  

문제39. (오늘의 마지막 문제)  last_day 함수 처럼 first_day 함수도
        생성하시오 ! 

  first_day( Sys.Date() )

   2019-02-01 

  ?lubridate 

■ 어제 배웠던 내용 복습 

 1. R 을 왜 배워야하는지 ?
 2. R 기본 문법 
       - R 로 데이터 검색하는 방법 
       - R 에서의 연산자 
       - R 에서의 함수 
              1. 문자 함수
              2. 숫자 함수
              3. 날짜 함수   <---------- 

                       오라클          vs          R  
                    - add_months                 difftime
                    - months_between            사용자 정의함수
                    - last_day                  사용자 정의함수
                    - next_day                  사용자 정의함수 

              4. 변환 함수
              5. 일반 함수 

문제40. 오라클에 접속해서 오늘부터 돌아오는 월요일의 날짜를 
        SQL 로 출력하시오 !


set ORACLE_SID=xe  <-- xe 서비스(db이름) 를 이용하겠다 

sqlplus  scott/tiger    

SQL>  select  next_day(sysdate,'월요일')
         from  dual;

R>  next_day( Sys.Date(), '월요일') 

답 :   next_day <- function(x, day) {
             for  ( y  in 1:7 ) {
               check_date  =  x + y 
               if ( format(check_date,'%A') == day) {
                    print (check_date)
                                                    }
                                }
                                     }


                       오라클          vs          R  
                    - add_months                 difftime
                    - months_between            사용자 정의함수
                    - last_day                  사용자 정의함수
                    - next_day                  사용자 정의함수 

■ 변환 함수 

    오라클            vs        R  

    to_char                 as.character  --> 문자로 형변환
    to_number               as.integer  --> 숫자로 형변환
    to_date                 as.Date   --> 날짜로 형변환

                            as.Factor --> factor 형으로 변환하는 
                                          함수 

                            format  함수 
                               
                              %Y ---> 년도
                              %m ---> 달
                              %d ---> 일
                              %A ---> 요일 
 
문제41.  이름, 입사한 요일을 출력하시오 !  

data.table( emp$ename, format(as.Date(emp$hiredate),'%A') )

문제42. 내가 무슨 요일에 태어났는지 출력하시오 ! 

 format( as.Date('1992-06-09'),'%A')

문제43. add_months 함수를 R 로 생성하시오 !

SQL> select  add_months(sysdate, 100)
       from  dual;

힌트 : Sys.Date() + months(100)

구현 :  add_months( Sys.Date(), 100 )

        2027/06/15
답:

■ 매번 패키지를 라이브러리 하기 번거롭고 매번 아래의 
   명령어로 emp 데이터 프레임을 만들기 번거롭다면 
   
  emp <- read.csv("emp.csv",header=T) 
  library(lubridate) 
  library(data.table) 

  아래의 위치의 Rprofile.site 파일에 위의 명령어를 넣으면
  R 실행할때 마다 자동 실행된다. 

C:/Program Files/R/R-3.2.3/etc/ Rprofile.site


생각해야할 문제 : R 로  months_between  만들기 

 입사한 날짜부터 오늘까지 총 몇달 근무했는지 출력하시오 !

SQL> select  months_between(sysdate,hiredate)
      from  emp; 

R> months_between( Sys.Date(), emp$hiredate ) 

■ 일반 함수 

      오라클         vs        R  

  1. nvl 함수                is.na
  2. decode 함수             ifelse
  3. case  함수              ifelse 

문제44. 이름, 월급, 등급을 출력하는데 월급이 1500 이상이면
        등급을 A 로 출력하고 아니면 B 로 출력하시오 !

SQL>  select  ename, sal, 
              case  when  sal >= 1500  then  'A'  
                    else  'B'  end  as "등급"
       from   emp;

R>  data.table( emp$ename,emp$sal, ifelse(emp$sal>=1500,'A','B'))

문제45. 이름, 월급, 등급을 출력하는데 월급이 3000 이상이면 A 를
        출력하고 월급이 1500 이상이고 3000 보다 작으면 B 를 
        출력하고 나머지 사원들은 C 를 출력하시오 ! 

R> data.table(emp$ename,emp$sal,
           ifelse(emp$sal>=3000,'A',
                  ifelse(emp$sal>=1500,'B','C')))

문제46. is.na 함수를 이용해서 커미션이 NA 인 사원들의
        이름과 커미션을 출력하시오 !

SQL> select  ename, comm
       from   emp
       where  comm  is  null; 

R> emp[ is.na(emp$comm), c("ename","comm") ]

■ R 실행할때 자동으로 emp.csv 불러오는 방법 

C:\Program Files\R\R-3.5.2\etc

Rprofile.site.txt  <--- 이 파일을 열어서 아래의 source 명령어를
                        붙여넣으세요 

source("d://start.R",local=TRUE, echo=TRUE,  encoding = 'UTF-8'))  

 
* start.R 의 내용은 아래와 같다. 

library(shiny)
library(data.table)
library(doBy)
library(lubridate)
library(utils)

setwd("d:\\data") 
emp <- read.csv("emp.csv", header=T)
crime_loc <- read.csv("crime_loc.csv", header=T)

-- 또는 아래와 같이 함수로 만들어서 실행되게 하시오

# start
start_func <- function(){
  eval( library(data.table) )
  eval( library(lubridate) )
  eval( setwd('d:\\Rdata') )
  emp <<- eval(read.csv('emp.csv',header=T))
  crime_loc <<- eval(read.csv('crime_loc.csv',header=T))
  eval( attach(emp) )
}
start_func()

문제47. 이름과 커미션을 출력하는데 커미션이 NA 인 사원들은 
        no comm 으로 출력되게 하시오 !

SQL>  select  ename,  nvl(to_char(comm),'no comm')
        from  emp; 


R>  data.table( emp$ename, emp$comm,
                ifelse(is.na(emp$comm), 'no comm', emp$comm) )  


■ R 에서의 그룹 함수 
  
     Oracle         vs          R  

1.    max                      max
2.    min                      min
3.    sum                      sum
4.    avg                      mean 
5.    count                    length (세로)
                               table  (가로) 
 
문제48. 최대월급을 출력하시오 !

 max(emp$sal) 

문제49. 직업이 SALESMAN 인 사원들중에서 최대월급을 출력하시오 !

 x <- emp[ emp$job =='SALESMAN','sal' ]

 x2 <- max(x) 

 emp[ emp$sal == x2 & emp$job=='SALESMAN', c("ename","sal") ]

문제50. 직업, 직업별 최대월급을 출력하시오 !

SQL> select  job, max(sal)
      from  emp
      group  by  job;

R>  aggregate( sal~job, emp, max )

문제51. 부서번호, 부서번호별 최소 월급을 출력하시오 ! 

SQL> select  deptno, min(sal)
       from  emp
       group  by  deptno;

R>  aggregate( sal~deptno, emp, min) 

문제52. 부서번호, 직업, 토탈월급을 출력하시오 !

SQL> select  deptno, job, sum(sal)
       from  emp
       group  by  deptno, job; 

R> aggregate( sal~deptno+job, emp, sum )

문제53.  부서번호, 부서번호별 최대월급을 출력하는데
         부서번호별 최대월급이 높은것부터 출력하시오 !
         (점심시간 문제)  

library(doBy)

orderBy( ~-sal, aggregate(sal~deptno, emp,max) )

  deptno  sal
1     10 5000
2     20 3000
3     30 2850

문제54. 위의 컬럼명을 한글로 부서번호, 토탈월급으로 변경하시오!

부서번호 토탈월급
1     10 5000
2     20 3000
3     30 2850

 library(doBy)

 x <- orderBy( ~-sal, aggregate(sal~deptno, emp,max) )

 names(x) <- c("부서번호","토탈월급")

 x 

문제55. 직업, 직업별 인원수를 출력하시오 !

SQL> select  job,  count(*)
       from  emp
       group  by  job;  

1. 세로로 출력 

R> aggregate(  empno~job, emp, length)  

2. 가로로 출력

R> table(emp$job)  

문제56. 위의 결과를 막대 그래프로 그리시오 !

x <-  table(emp$job)

barplot(x , main="직업별 인원수", col=rainbow(5), density=50)  

문제57. 직업, 직업별 평균월급을 출력하시오 !

 SQL> select job, avg(sal)
        from  emp
        group by  job; 

 세로:  aggregate( sal~job, emp, mean )

 가로:  tapply( emp$sal, emp$job, mean ) 

문제58. 위의 결과를 다시 출력하는데 소숫점 이하는 안나오게 하시오!

 세로:  x <- aggregate( sal~job, emp, mean )
      
        data.table('직업'=x$job,'평균월급'=trunc(x$sal))

 가로: x2 <- tapply( emp$sal, emp$job, mean ) 
       trunc(x2)

문제59.  입사한 년도, 입사한 년도별 평균월급을 가로로 출력하시오 !

    1980   1981   1982    1983
    2392   3939   2919    2929

x <- tapply( emp$sal, format( as.Date(emp$hiredate),'%Y'), mean) 

trunc(x) 

> year(hiredate)
 [1] 1981 1981 1981 1981 1981 1981 1981 1981 1981 1981 1980 1982 1983 1982
> 
> month(hiredate)
 [1] 11  5  5  4  9  2  8 12  2 12 12 12  1  1
> 
> day(hiredate)
 [1] 17  1  9  1 10 11 21 11 23 11  9 22 15 11

> trunc(tapply(sal,year(hiredate),mean))

문제60. 위의 결과를 막대 그래프로 그리시오 ! 

답:

x <- trunc(tapply(sal,year(hiredate),mean))

barplot(x , main="년도별 평균월급", col=rainbow(4), density=50)  

문제61. 직업, 부서번호, 직업별 부서번호별 토탈월급을 출력하시오 !

SQL> select  job, sum( decode(deptno,10,sal) )  as "10",
                  sum( decode(deptno,20,sal) )  as "20",
                  sum( decode(deptno,30,sal) )  as "30"
       from  emp
       group by job; 

JOB                        10         20         30
------------------ ---------- ---------- ----------
SALESMAN                                       5600
CLERK                    1300       1900        950
PRESIDENT                5000
MANAGER                  2450       2975       2850
ANALYST                             6000

R> tapply( emp$sal,  list(emp$job,emp$deptno), sum )

            10   20   30
ANALYST     NA 6000   NA
CLERK     1300 1900  950
MANAGER   2450 2975 2850
PRESIDENT 5000   NA   NA
SALESMAN    NA   NA 5600

문제62. 위의 결과중에 NA 를 0 으로 출력 되게하시오 !

 x <-  tapply( emp$sal,  list(emp$job,emp$deptno), sum )

 x[is.na(x)] <- 0

 x  

문제63. 직업, 입사한 년도(4자리), 직업별 입사한 년도별 토탈월급을
        출력하시오 !

       ANALYST   CLERK   PRESIDENT  SALESMAN 
1980
1981
1982
1983

 attach(emp)

 x <- tapply( sal, list(year(hiredate), job) , sum )

 x[is.na(x)] <- 0 

     ANALYST CLERK MANAGER PRESIDENT SALESMAN
1980       0   800       0         0        0
1981    3000   950    8275      5000     5600
1982    3000  1300       0         0        0
1983       0  1100       0         0        0

문제64. 위의 결과에서 column name 과 row  name 을 각각 출력하시오!

  colnames(x)
  rownames(x) 

문제65. 문제64변의 결과를 막대 그래프로 시각화 하시오 !

barplot( x, col=rainbow(7),  legend=rownames(x), beside=T) 

문제66. 직업, 직업별 토탈월급을 원형( pie ) 그래프로 그리시오 !

 rm(x)

 x <- tapply( emp$sal, emp$job,  sum )

 pie( x , col = rainbow(5) , density=80 ) 

문제67. 위의 그래프를 3D 로 출력하시오 !

 install.packages("plotrix")
 library(plotrix)

 pie3D( x, explode=0.1, labels = rownames(x) )

문제68. 문제67번 시각화 결과 옆 즉 직업 옆에 비율도 같이 
        출력되게 하시오 !

 x <- tapply( emp$sal, emp$job, sum )
 
 x2 <- aggregate(sal~job, emp, sum) 

 pct <- round(x2$sal/sum(emp$sal) * 100,1) 
 pct 
 20.7 14.3 28.5 17.2 19.3

 lbls <- x2$job
 lbls  

 lbls2 <- paste(lbls, ':', pct, '%' )
 lbls2

[1] "ANALYST : 20.7 %"   "CLERK : 14.3 %"     "MANAGER : 28.5 %"  
[4] "PRESIDENT : 17.2 %" "SALESMAN : 19.3 %" 
 
 pie3D( x, explode=0.1, labels = lbls2 )

문제69. crime_loc.csv 를 가지고 범죄유형이 살인인 장소와 건수를
        출력하시오 !  

crime_loc <- read.csv("crime_loc.csv", header=T )

crime_loc[ crime_loc$범죄 =="살인", c("장소","건수") ] 

문제70. 위의 결과를 x5 변수에 담고 아래의 결과로 막대 그래프로
        그리시오 !

 x5 <- crime_loc[ crime_loc$범죄 =="살인", c("장소","건수") ] 

 x6 <- tapply(x5$건수, x5$장소, sum )

 x6 

답:  barplot(x6, col=rainbow(14))

문제71. 서울시 물가 데이터(price.csv) 를 내려받고 
        price 라는 변수에 입력하시오 !

price <- read.csv("price.csv", header=T) 

문제72. tapply 함수를 이용해서 전통시장과 대형마트간의
        물품별 가격 평균을 출력하시오 !

            대형마트         전통시장
  고등어      720               500
   무         100               120
    :          :                 :
    :          :                 :

x <- round( tapply( price$A_PRICE, list(price$A_NAME,price$M_TYPE_NAME), mean) )

x[is.na(x)] <- 0

x

문제73. 위의 결과를 다시 출력하는데 가격이 높은것부터 출력되게
       하는데 대형마트를 기준으로 출력되게하시오 ! 


orderBy(~-대형마트,x)


■ R 로 조인하는 방법

      오라클          vs           R  

    equi join                  
 non equi join                   merge  
    outer join
    self  join  

문제74. dept.csv 를 내려받아 dept 라는 변수에 로드하고 
        이름과 월급과 부서위치를 출력하시오 !

 dept <- read.csv("dept.csv", header=T)

 x <- merge( emp, dept, by="deptno") 

 x[     , c("ename","sal","loc") ] 

문제75. 부서위치가 DALLAS 인 사원들의 이름과 월급과 부서위치를
        출력하시오 !

 x[ x$loc=='DALLAS'   , c("ename","sal","loc") ] 

문제76. 커미션이 NA 인 사원들의 이름과 부서위치와 커미션을 
        출력하시오 !

 x[  is.na(x$comm)  , c("ename","loc","comm") ] 

문제77. 이름과 부서위치를 출력하는데 오라클의 outer join 과 같은
        결과를 출력하시오 !

SQL>  select  e.ename,  d.loc
        from  emp  e, dept  d
        where  e.deptno (+) = d.deptno ;

R> x <- merge( emp, dept, by="deptno", all.y=T) 
               ↑   ↑
               x    y 

   x[   , c("ename","loc") ] 

문제78. 아래의 SQL 의 결과를 R 로 구현하시오 !

SQL> select  e.ename, d.loc
       from  emp  e  full  outer  join  dept   d
       on ( e.deptno = d.deptno );

R> x <- merge( emp, dept, by="deptno", all=T) 

   x [    , c("ename","loc") ]

문제79. 이름과 자기의 직속상사의 이름(관리자)을 출력하시오 !

SQL> select  e.ename,  m.ename
       from  emp  e,  emp  m
       where  e.mgr = m.empno ;  

R>  x <- merge( emp, emp,  by.x="mgr", by.y="empno")  
    x 
    x [     ,c("ename.x","ename.y") ]  
   
문제80. 위의 결과를 다시 출력하는데 관리자 보다 더 많은 월급을 
        받는 사원들의 이름과 월급을 출력하시오 !

SQL> select  e.ename, m.ename
        from  emp   e,  emp   m
        where  e.mgr = m.empno and  e.sal >  m.sal ;

R>  x [ x$sal.x > x$sal.y   , c("ename.x","ename.y") ]

문제81. 위의 결과 데이터인 사원이름과 직속상상의 이름을 출력하는
        데이터를 가지고 아래와 같이 시각화를 하시오 !
        ( 사원 테이블의 조직도를 그리시오 !)

install.packages("igraph")
library(igraph)

x <- merge(emp,emp, by.x ="mgr", by.y="empno")

a <- x[    ,c("ename.x", "ename.y") ]

b <- graph.data.frame(a, directed=F)
plot(b) 

문제82. 위의 그래프를 구글의 googleVis 를 이용해서 emp 테이블의
        관계도를 시각화 하시오 !

install.packages("googleVis")
library(googleVis)

a <- merge(emp,emp, by.x="empno",by.y="mgr", all.y=T)

org <- gvisOrgChart(a, idvar="ename.y",parentvar="ename.x",
     options=list(width=600, height=250, size='middle',allowCollapse=T))

plot(org)

문제83. 부서위치, 부서위치별 토탈월급을 출력하시오 !

   NEW YORK   CICAGO   DALLAS    BOSTON 
     3929      3994      9393      NA

답:
x <- merge(emp, dept, by='deptno')
x
tapply(x$sal, x$loc, sum)

문제84. 위의 결과를 막대 그래프로 시각화 하시오 !

x2 <- x[is.na(x)] <- 0

bar(   ?     )   

■ 팩터(factor) 자료형이란 ?

 - 범주(값의 목록) 을 갖는 vector  

 - 종류가 2가지 인데  

       1. nominal  -> level 의 순서의 값이 무의미하다.
                      (알파벳 순서로 정의)
       2. ordinal  -> level 의 순서의 값을 직접 정의해서 
                      원하는 순서로 정의 할 수 있다.
예제:

  a <-  c("middle","low","high")
  str(a)

  b <- factor(a)
  str(b)

  c <- factor(a, order=TRUE, level=c("low","middle","high") )
  str(c) 

  max(c)
  min(c)
  c[ order(c, decreasing=F) ]
  
■ 요번주 일정 

 1. R 기본문법 (1주일)
 2. R 을 활용한 머신러닝 (2주일)
 3. R 과 파이썬 활용한 통계수업 (2주일)  

   월    화    수    목     금
       


 3월 26일에 최종 포트폴리오 발표 

 발표시간 : 30~40분 

문제85. 이름과 월급과 부서위치를 출력하시오 !

 setwd("d:\\data")
 emp <- read.csv("emp.csv",header=T)
 dept <- read.csv("dept.csv",header=T)

 x <- merge(emp,dept, by="deptno")

 x[     ,c("ename","sal","loc") ] 

문제86. 부서위치, 부서위치별 토탈월급을 출력하시오 !
 
세로 :  
      x <- merge(emp,dept, by="deptno", all.y=T)

      x[     ,c("ename","sal","loc") ]
 
      aggregate(x$sal~x$loc, x, sum) 

      aggregate(x$sal~x$loc, x, sum, na.action=na.pass )
 
     x$loc x$sal
1   BOSTON    NA  <--------- na.action=na.pass 를 써야 출력됨 
2  CHICAGO  9400
3   DALLAS 10875
4 NEW YORK  8750


가로 :   tapply( x$sal, x$loc, sum ) 

 BOSTON  CHICAGO   DALLAS NEW YORK 
      NA     9400    10875     8750 

문제87. 위의 결과를 구글 막대 그래프로 그리시오 !
 
 x2 <- aggregate(x$sal~x$loc, x, sum, na.action=na.pass )

 #install.packages("googleVis")
 library(googleVis)
 library(data.table)  

 x3 <- data.table(x2)
 x3
 x4 <- gvisBarChart(x3)

 plot(x4) 

문제88.  아래와 같이 결과를 출력하시오 !  
         ( 입사한 년도별 부서위치별 토탈월급을 출력하시오 )

           1980     1981   1982    1983 
NEW YORK   
DALLAS
CHICAGO
BOSTON 

답 :  x <- merge(emp,dept, by="deptno", all.y=T)

      tapply( x$sal, list(x$loc, year(x$hiredate)), sum )

         1980 1981 1982 1983
BOSTON     NA   NA   NA   NA
CHICAGO    NA 9400   NA   NA
DALLAS    800 5975 3000 1100
NEW YORK   NA 7450 1300   NA

문제89. 위의 결과 데이터를 일반 막대 그래프로 그리시오 !

      x <- merge(emp,dept, by="deptno", all.y=T)

      x2 <- tapply( x$sal, list(x$loc, year(x$hiredate)), sum )

      x2[is.na(x2)] <- 0

      barplot( x2, col=rainbow(4), beside=T, legend=rownames(x2) ) 

문제90. 지하철 1-4호선 승하차 승객수.csv 를 R 로 로드해서
        line no 컬럼과 time 컬럼을 이용해서 구글 모션차트를 
        그리시오 !

 line <- read.csv("d:\\data\\1-4호선승하차승객수.csv", header=T)
 line
 
 t1 <- gvisMotionChart( line, idvar="line_no", timevar="time")
 
 plot(t1)

문제91. 지하철5-8호선.csv 를 내려받고 구글 모션 차트를 그리시오!


서울지하철_5-8호선_이용현황_시간대별.csv


 line2 <- read.csv("d:\\data\\서울지하철_5-8호선_이용현황_시간대별.csv", header=T)
 line2
 
 t1 <- gvisMotionChart( line2, idvar="호선명", timevar="시간")
 
 plot(t1)

■ 집합 연산자 

        오라클           vs            R  

  1.  union  all                      rbind
  2.  union                           rbind + unique
  3.  intersect                       intersect 
  4.  minus                           setdiff 

문제92. 아래의 SQL 의 결과를 R 로 구현하시오 !

SQL>  select  ename, sal, deptno
        from  emp
        where  deptno  in  (10,20)
      union  all
      select  ename, sal, deptno
        from  emp
        where deptno = 10;

R>  rbind( emp[emp$deptno %in% c(10,20), c("ename","sal","deptno")],
           emp[emp$deptno == 10, c("ename","sal","deptno") ] ) 

※ rbind :  위아래의 결과를 하나의 결과로 출력
   cbind :  양옆으로의 결과를 하나의 결과로 출력 

문제93. 부서번호, 부서번호별 토탈월급을 아래와 같이 출력하는 SQL을
        R 로 구현하시오 !

SQL:  select  deptno, sum(sal)
       from  emp
       group  by rollup(deptno);

R>  rbind( aggregate( sal~deptno, emp, sum), 
           c("토탈값: ", sum(emp$sal) )  ) 

문제94. 아래의 SQL 의 결과를 R 로 구현하시오 !

SQL> select  ename, sal, deptno
        from  emp
        where  deptno  in  (10,20)
     union
     select  ename, sal, deptno
        from  emp
        where deptno = 10 ; 

※ 설명: union all 과 union 의 다른점은 union 은 중복제거가 된다. 

R> unique( 
   rbind( emp[emp$deptno %in% c(10,20), c("ename","sal","deptno")],
           emp[emp$deptno == 10, c("ename","sal","deptno") ] ) 
          )

문제95. 아래의 SQL 의 결과를 R 로 구현하시오 !

SQL>  select  ename, sal, deptno
        from  emp
        where deptno in (10,20)
        minus
      select  ename, sal, deptno
        from  emp
        where deptno = 10 ;

R>   x <- setdiff( emp[emp$deptno %in% c(10,20), "ename"],
                   emp[emp$deptno == 10, "ename"] )
 
     x 

     emp[ emp$ename  %in%  x , c("ename","sal","deptno") ] 

문제96. 아래의 SQL 의 결과를 R 로 구현하시오 !

SQL>  select  ename, sal, deptno
        from  emp
        where  deptno  in (10,20)
     intersect
     select  ename, sal, deptno
        from emp
        where  deptno = 10; 

R>  x <-  intersect( emp[emp$deptno %in% c(10,20), "ename"],
                     emp[emp$deptno == 10, "ename"] )
 
    x

    emp[ emp$ename  %in%  x , c("ename","sal","deptno") ] 

■  R 로 서브쿼리 구현하기 

 * 오라클의 서브쿼리 3가지

  1. single  row  subquery
  2. multiple  row subquery
  3. multiple  column subquery

문제97. JONES 의 월급보다 더 많은 월급을 받는 사원들의 이름과
        월급을 출력하시오 !

SQL> select  ename, sal
       from  emp
       where  sal >  ( select  sal
                        from  emp
                        where ename='JONES') ;

R> jonesal <-  emp[ emp$ename=='JONES', "sal" ]

   emp[ emp$sal >  jonesal , c("ename","sal") ] 

문제98. 사원 테이블에서 가장 많은 월급을 받는 사원의 이름과 
        월급과 직업을 출력하시오 !

 maxsal <- max(emp$sal)

 emp[ emp$sal == maxsal , c("ename","sal","job") ]  

문제99. 전국에서 등록금이 가장 비싼 학교이름과 등록금을 출력하시오!
        ( 전국_대학별등록금통계_현황.csv) 

setwd("d:\\data")
univ <- read.csv("전국_대학별등록금통계_현황.csv", header=T)

head(univ)

attach(univ)

univ[ 등록금.A.B. == max(등록금.A.B.), ] 

문제100. KING 에게 보고하는 사원들의 이름과 월급을 출력하시오!
    
SQL> select  ename, sal
        from  emp 
        where  mgr = ( select  empno
                         from  emp
                         where ename='KING') ;

R>  king_empno <- emp[ emp$ename=='KING', "empno" ]
    emp[ emp$mgr == king_empno ,c("ename","sal") ] 

   ename  sal
NA  <NA>   NA  <---------- 이 부분을 제거하려면 ? na.omit 사용 
2  BLAKE 2850
3  CLARK 2450
4  JONES 2975

  na.omit( emp[ emp$mgr == king_empno ,c("ename","sal") ] ) 

문제101. 관리자인 사원들의 이름을 출력하시오 !

SQL>  select  ename
          from  emp
          where empno  in  ( select  mgr
                                from  emp );

R>  data.table( emp[ emp$empno %in% emp$mgr , "ename" ] )

문제102. 관리자가 아닌 사원들의 이름을 출력하시오 ! 

SQL>  select  ename
          from  emp
          where empno not in  ( select  mgr
                                  from  emp
                                  where mgr  is  not null );

R>  data.table( emp[ ! emp$empno %in% emp$mgr , "ename" ] )

문제103. 작년에 아파트에서 가장 많이 발생한 범죄 유형이 무엇인지
         출력하시오 ! ( crime_loc.csv 사용) 

crime_loc <- read.csv("crime_loc.csv", header=T)

head(crime_loc) 
  범죄     장소  건수
1 절도   아파트 25389
2 절도       집 37787
3 절도 고속도로   151
4 절도     노상 62560
5 절도     상점 29977
6 절도 시장노점  1239

 x <- crime_loc[ crime_loc$장소=='아파트', c("범죄","건수")]

 x [ x$건수==max(x$건수) ,    "범죄" ] 

문제104.(점심시간 문제) 강력범죄가 가장 많이 발생하는 요일은 
        언제인가 ? (crime_day.csv 를 로드받고 하세요) 


■ 순위 출력을 R 로 구현하는 방법 

       오라클        vs          R  

      dense_rank                rank 

문제105.  이름, 월급, 월급에 대한 순위를 출력하시오 ! 

 data.table( emp$ename,  emp$sal,  
             rank(-emp$sal, ties.method="min") )

※   min :  오라클의 rank 와 같다.
    first :  오라클의 rank 와 같은데 순위가 같은 데이터가 있으면 
             인덱스 순서가 먼저 나온 데이터를 높은 순위로 
             부여한다.

          인덱스 이름  월급 순위 

    예:     2 : FORD   3000  2
            3 : SCOTT  3000  3

문제106. 위의 결과를 다시 출력하는데 순위를 1위부터 정렬해서 출력
         하시오 ! 

 library(doBy)

 x <- data.table(이름=emp$ename, 월급=emp$sal,  
                 순위=rank(-emp$sal, ties.method="min") )

 orderBy(~ 순위, x)  

문제107.  여자들이 많이 걸리는 암과 건수와 순위를 출력하시오 !
          ( cancer2.csv 를 로드해서 수행하시오)

> unique(cancer2$성별)
[1] 남녀전체 남자     여자  

cancer <- read.csv("cancer2.csv",header=T)

cancer <- unique(cancer)

cancer2 <- cancer[ cancer$성별=="여자" & cancer$암종 != "모든암", ]

x <- data.table(cancer2$암종, cancer2$환자수,
                rank(-cancer2$환자수, ties.method="min")  )

names(x) <- c("암","건수","순위") 

x

orderBy(~순위, x)

문제108. 2009년도에 서울시에서 교통사고가 일어난 장소와 건수와
         순위를 출력하시오 ! ( car_accident.csv 를 이용) 

car  <- read.csv("car_accident.csv", header=T) 
x <- car[ car$year2 == 2009 & car$loc == '서울',]
y <- data.table( x$loc_desc , x$cnt , rank( -x$cnt , ties.method = 'min'))
orderBy(~V3, y)

■ R shiny 사용방법 

https://shiny.rstudio.com/gallery/

■ R shiny 가 무엇인가 ?

  R 의 강력한 그래픽 기능과 통계 분석 능력을 이용하고,
  사용자 상호작용을 쉽게 만들 수 있는 언어를 말한다.

  샤이니 패키지를 이용해서 편하게 사용자 인터페이스(User Interface)
  를 이용할 수 있다.

■ R 샤이니 기본 골격 

       유져 인터페이스       와         서버  
            ↓                           ↓
        frontier                       backend tier 

■ 샤이니 기본 예제1 

install.packages("DT")

library(DT)
library(shiny)
library(ggplot2)

head(mpg)

str(mpg)

summary(mpg)

■ 샤이니의 큰 기본 골격

1. 화면 개발

ui <-  ......

2. 서버단 개발 

server <-  .....

3. 실행하는 명령어 

shinyApp(ui = ui, server = server)

문제109. 샤이니 기본 화면을 실행하시오 ! 

library(shiny)

# Define UI ----
ui <- fluidPage(
  
)

# Define server logic ----
server <- function(input, output) {
  
}

# Run the app ----
shinyApp(ui = ui, server = server)

문제110.  ui 에  아래의 내용을 추가하시오 ! 

library(shiny)

# Define UI ----
ui <- fluidPage(
  titlePanel("title panel"),
  
  sidebarLayout(
    sidebarPanel("sidebar panel"),
    mainPanel("main panel")
  )
)

# Define server logic ----
server <- function(input, output) {
  
}

# Run the app ----
shinyApp(ui = ui, server = server)

문제111. sidebarLayout 이 오른쪽에 나오게 하시오 ! 

library(shiny)

# Define UI ----
ui <- fluidPage(
  titlePanel("title panel"),
  
  sidebarLayout(position = "right",
    sidebarPanel("sidebar panel"),
    mainPanel("main panel")
  )
)

# Define server logic ----
server <- function(input, output) {
  
}

# Run the app ----
shinyApp(ui = ui, server = server)


문제112. 사용자 인터페이스의 글씨를 아래와 같이 확인하시오

library(shiny)

# Define UI ----
ui <- fluidPage(
  titlePanel("My Shiny App"),
  sidebarLayout(
    sidebarPanel(),
    mainPanel(
      h1("First level title"),
      h2("Second level title"),
      h3("Third level title"),
      h4("Fourth level title"),
      h5("Fifth level title"),
      h6("Sixth level title")
    )
  )
)

# Define server logic ----
server <- function(input, output) {
  
}

# Run the app ----
shinyApp(ui = ui, server = server)



문제113. 아래의 필요한 전체 기능을 출력하시오 !

library(shiny)

# Define UI ----
ui <- fluidPage(
  titlePanel("Basic widgets"),
  
  fluidRow(
    
    column(3,
           h3("Buttons"),
           actionButton("action", "Action"),
           br(),
           br(), 
           submitButton("Submit")),
    
    column(3,
           h3("Single checkbox"),
           checkboxInput("checkbox", "Choice A", value = TRUE)),
    
    column(3, 
           checkboxGroupInput("checkGroup", 
                              h3("Checkbox group"), 
                              choices = list("Choice 1" = 1, 
                                             "Choice 2" = 2, 
                                             "Choice 3" = 3),
                              selected = 1)),
    
    column(3, 
           dateInput("date", 
                     h3("Date input"), 
                     value = "2014-01-01"))   
  ),
  
  fluidRow(
    
    column(3,
           dateRangeInput("dates", h3("Date range"))),
    
    column(3,
           fileInput("file", h3("File input"))),
    
    column(3, 
           h3("Help text"),
           helpText("Note: help text isn't a true widget,", 
                    "but it provides an easy way to add text to",
                    "accompany other widgets.")),
    
    column(3, 
           numericInput("num", 
                        h3("Numeric input"), 
                        value = 1))   
  ),
  
  fluidRow(
    
    column(3,
           radioButtons("radio", h3("Radio buttons"),
                        choices = list("Choice 1" = 1, "Choice 2" = 2,
                                       "Choice 3" = 3),selected = 1)),
    
    column(3,
           selectInput("select", h3("Select box"), 
                       choices = list("Choice 1" = 1, "Choice 2" = 2,
                                      "Choice 3" = 3), selected = 1)),
    
    column(3, 
           sliderInput("slider1", h3("Sliders"),
                       min = 0, max = 100, value = 50),
           sliderInput("slider2", "",
                       min = 0, max = 100, value = c(25, 75))
    ),
    
    column(3, 
           textInput("text", h3("Text input"), 
                     value = "Enter text..."))   
  )
  
)

# Define server logic ----
server <- function(input, output) {
  
}

# Run the app ----
shinyApp(ui = ui, server = server)

■ R 에서 그래프 그리는 방법

 1. 막대 그래프
 2. 원형 그래프
 3. 라인 그래프
 4. 특수 그래프 (지도, 소리 시각화, 워드 클라우드)
 5. 사분위수 그래프 (평균,중앙값, 이상치 )

■ 막대 그래프

문제114. emp 테이블의 월급으로 기본적인 막대 그래프를 그리시오

barplot(emp$sal) 

문제115. 위의 그래프의 제목을 Salary Bar Chart 라고 이름을 
         붙이시오 !

barplot(emp$sal, main = "Salary Bar Chart") 

문제116. 막대 그래프의 x 축에 사원 이름을 붙이시오 !

barplot(emp$sal, main = "Salary Bar Chart",
        names.arg = emp$ename, ylab="Salary" ) 

문제117. 막대 그래프의 색깔을 입히시오 !

barplot(emp$sal, main = "Salary Bar Chart",
        names.arg = emp$ename, ylab="Salary",
        col=('blue')  ) 

문제118. 아래와 같이 치킨집 년도별 창업건수를 막대 그래프로 
         시각화 하시오 !

create_cnt <- read.csv("창업건수.csv", header=T)
drop_cnt   <- read.csv("폐업건수.csv", header=T)

barplot( create_cnt$치킨, main="년도별 치킨집 창업건수",
         names.arg=create_cnt$X, col=('blue'),
         ylim=c(0,1600) ) 

문제119. 치킨집 년도별 창업건수, 폐업건수를 아래와 같이
         같이 막대 그래프로 시각화 하시오!

graphics.off() 

x <- rbind( create_cnt$치킨, drop_cnt$치킨)  
x

barplot( x, main="년도별 치킨집 창업,폐업",
         names.arg=create_cnt$X, col=c("blue","red"),
         ylim=c(0,4000), beside=T) 

문제120. 위의 그래프에 legend 를 달아서 파란색이 창업이고 
         빨간색이 폐업이다 라고 하시오 !

graphics.off() 

x <- rbind( create_cnt$치킨, drop_cnt$치킨)  
x

barplot( x, main="년도별 치킨집 창업,폐업",
         names.arg=create_cnt$X, col=c("blue","red"),
         ylim=c(0,4000), beside=T,
         legend=c("창업","폐업")   ) 

문제121. 카페(커피음료) 가 얼마나 창업하고 얼마나 폐업하는지
         막대 그래프로 시각화 하시오 ! 

c <- rbind(create_cnt$커피음료, drop_cnt$커피음료) 


문제122. R 샤이니로 막대 그래프 시각화를 자동화 시키는 
         기본 코드를 돌려보시오 ! 


library(shiny)
library(datasets)

setwd("d:\\data") 
create_cnt <- read.csv("창업건수.csv", header=T)

# Define UI ----
ui <- fluidPage(    
  
  # Give the page a title
  titlePanel("년도별 업종별 창업현황"),
  
  # Generate a row with a sidebar
  sidebarLayout(      
    
    # Define the sidebar with one input
    sidebarPanel(
      selectInput("region", "업종:", 
                  choices=colnames(create_cnt)[-1]),
      hr(),
      helpText("Data from AT&T (1961) The World's Telephones.")
    ),
    
    # Create a spot for the barplot
    mainPanel(
      plotOutput("phonePlot")  
    )
    
  )
)

# Define server logic ----
server <-function(input, output) {
  
  # Fill in the spot we created for a plot
  output$phonePlot <- renderPlot({
    
    # Render a barplot
    barplot(create_cnt[,input$region], 
            main=input$region,
            names.arg= create_cnt$X,    
            ylab="Number of Telephones",
            xlab="Year")
  })
}

# Run the app ----
shinyApp(ui = ui, server = server)

문제123. 샤이니로 아래와 같이 창업과 폐업을 같이 출력되게하는
         자동화 화면을 구현하시오 !


  arg.legend = list(x='topright')  


답:
barplot(rbind(opn[,input$type],cls[,input$type]), 
            main=input$type,
            ylab="Number of Open Stores",
            xlab="Year",
            names.arg = opn$X,
            col = cm.colors(2),
            ylim = c(0,max(rbind(opn[,input$type],cls[,input$type]))+2000),
            beside = T)
    legend('topright',c('open','closed'), pch=15,col=cm.colors(2))


전체코드:
library(shiny)
library(datasets)
setwd('d:/data')
create_cnt <- read.csv('창업건수.csv',header=T)
drop_cnt <- read.csv('폐업건수.csv',header=T)
# Define UI ----
ui <- fluidPage(    
  
  # Give the page a title
  titlePanel("년도/업종별 창업건수"),
  
  # Generate a row with a sidebar
  sidebarLayout(      
    
    # Define the sidebar with one input
    sidebarPanel(
      selectInput("region", "업종:", 
                  choices=colnames(create_cnt)[-1]),
      hr(),
      helpText("업종선택")
    ),
    
    # Create a spot for the barplot
    mainPanel(
      plotOutput("opendrop")  
    )
    
  )
)

# Define server logic ----
server <-function(input, output) {
  
  # Fill in the spot we created for a plot
  output$opendrop <- renderPlot({
    
    # Render a barplot
    barplot(rbind(create_cnt[,input$region],drop_cnt[,input$region] ), 
            main=input$region,
            names.arg = create_cnt$X,
            col = c('blue','red'),
            ylab="cnt",
            xlab="Year",
            beside = T,
            legend('topright',c('창업','폐업')
            )
  })
}

# Run the app ----
shinyApp(ui = ui, server = server)


■ 데이터 분석 시각화 가격 

  테블로 분석 시각화 툴 -->  한사람당 300만원(슈퍼유져),
                             일반유져 120만원 x 100 명 

                              1년,2년 라이센스 

■ 원형 그래프를 샤이니로 시각화 

문제124.  사원 테이블의 월급을 원형 그래프로 그리시오 !

 pie(emp$sal)

문제125. 위의 그래프를 다시 출력하는데 누구의 월급인지 
         명시되게 하시오 !

 pie( emp$sal, main="Salary Pie Chart",
      labels=emp$ename, col=rainbow(15) ) 

문제126. 위의 그래프에 월급에 비율을 붙여서 출력하시오 !

 sal_labels <- round( emp$sal/sum(emp$sal) * 100,1) 
 sal_labels 

 sal_labels2 <- paste( emp$ename, sal_labels, "%") 
 sal_labels2 

 pie( emp$sal, main ="Salary Pie Chart",
      labels=sal_labels2, col=rainbow(15) ) 

문제127. 2014년도 업종별 창업 비율을 아래와 같이 원형 그래프로 
         그리시오 !

create_cnt <- read.csv("창업건수.csv", header=T)

x2 <- create_cnt[create_cnt$X=='2014', -1 ]
x2 

cnt_labels <- round(x2/sum(x2) *100,1) 
cnt_labels
t(cnt_labels) 

cnt_labels2 <- paste(colnames(cnt_labels),t(cnt_labels),'%')

pie( t(cnt_labels), col=rainbow(7), labels=cnt_labels2) 

문제128. 2013년도 업종별 창업 비율을 아래와 같이 원형 그래프로
         그리시오 !

create_cnt <- read.csv("창업건수.csv", header=T)

x2 <- create_cnt[create_cnt$X=='2013', -1 ]
x2 

cnt_labels <- round(x2/sum(x2) *100,1) 
cnt_labels
t(cnt_labels) 

cnt_labels2 <- paste(colnames(cnt_labels),t(cnt_labels),'%')

pie( t(cnt_labels), col=rainbow(7), labels=cnt_labels2)

문제129. 위의 스크립트를 편하게 사용할 수 있도록 샤이니로
         구현하시오 !


library(shiny)
library(datasets)

setwd("d:\\data") 
create_cnt <- read.csv("창업건수.csv", header=T)

# Define UI ----
ui <- fluidPage(    
  
  # Give the page a title
  titlePanel("년도별 업종별 창업현황"),
  
  # Generate a row with a sidebar
  sidebarLayout(      
    
    # Define the sidebar with one input
    sidebarPanel(
      selectInput("region", "업종:", 
                  choices=create_cnt$X),
      hr(),
      helpText("Data from AT&T (1961) The World's Telephones.")
    ),
    
    # Create a spot for the barplot
    mainPanel(
      plotOutput("phonePlot")  
    )
    
  )
)

# Define server logic ----
server <-function(input, output) {
  
  # Fill in the spot we created for a plot
  output$phonePlot <- renderPlot({
    
    # Render a barplot
   
x2 <- create_cnt[create_cnt$X==input$region, -1 ]
cnt_labels <- round(x2/sum(x2) *100,1) 
cnt_labels2 <- paste(colnames(cnt_labels),t(cnt_labels),'%')
pie( t(cnt_labels), col=rainbow(7), labels=cnt_labels2)

  })
}

# Run the app ----
shinyApp(ui = ui, server = server)

■ 라인(plot) 그래프 

문제130. 아래의 점(plot) 그래프를 그리시오 !

graphics.off()

cars <- c(1,3,6,4,9)
cars

plot(cars) 

문제131. 위의 그래프에 파란색 선을 그리시오 !

graphics.off()

cars <- c(1,3,6,4,9)
cars

plot(cars, type="o", col="blue") 

문제132. 차와 트럭의 판매된 댓수를 라인 그래프로 시각화 하시오!

graphics.off()

cars <- c(1,3,6,4,9)
trucks <- c(2,5,4,5,12)

plot( cars, type="o", col="blue", ylim=c(0,12) )

# 그래프 창을 닫지 말고 바로 이어서

lines( trucks, type="o", pch=22, lty=2, col="red") 

※ 설명:   pch = 21 : 동그라미         lty=1 : 직선
           pch = 22 : 네모             lty=2 : 점선 





문제133. 다시 위의 2개의 그래프를 아래의 순서대로 시각화 하시오!

graphics.off()

cars <- c(1,3,6,4,9)
trucks <- c(2,5,4,5,12)

g_range <- range(0,cars, trucks)

plot( cars, type="o", col="blue", ylim=g_range,
       axes=FALSE, ann=FALSE) 

axis(1, at=1:5, lab=c("mon","tue","wed","thu","fri"))

axis(2)
box()

lines(trucks, type="o", pch=22, lty=2, col="red")

legend( 1, 12, c("cars","trucks"), col=c("blue","red"), 
        cex=0.8, pch=21:22, lty=1:2 )

※ 설명: cex = 0.8  : 글씨 크기 

문제134. 위의 코드를 활용해서 치킨집의 창업/폐업 현황을 
         라인 그래프로 시각화 하시오 !

drop_cnt <- read.csv("폐업건수.csv", header=T) 
g <- create_cnt$치킨집
m <- drop_cnt$치킨집
g_range <- range(0, g, m)
plot(g, type="o", col="blue", ylim=g_range, 
     axes=FALSE, ann=FALSE)
axis(1, at=1:10, lab=create_cnt$X)
axis(2)
box()
lines(m, type="o", pch=22, lty=2, col="red")
title(main="치킨집 창업/폐업현황", col.main="red", font.main=4)
title(xlab="Days", col.lab=rgb(0,0.5,0))
title(ylab="Total", col.lab=rgb(0,0.5,0))
legend(5, g_range[2], c("창업","폐업"), cex=0.8, 
       col=c("blue","red"), pch=21:22, lty=1:2);

문제135. 위의 업종별 창업/폐업 현황을 샤이니로 자동화 하시오 !


■ 그래프 
 
 1. 기본 막대 그래프
 2. 기본 원형 그래프 
 3. 기본 라인 그래프 
 4. ggplot2 막대 그래프
 5. plotly  원형 그래프
 6. plotly  라인 그래프 

■ ggplot2 막대 그래프 그리기 

 install.packages("ggplot2")

 xdata <- as.factor(create_cnt[ , "X"])
 ydata <- as.factor(create_cnt[ , "치킨집"])
 xdata
 ydata 
 
 fdata = data.frame(x=xdata, y=ydata)
 fdata 

 library(ggplot2)
 
 ggplot(fdata) + 
 geom_bar(aes_string(x='x',y='y', fill='x'), stat="identity",
 show.legend=T) 

문제136. csv 파일을 불러와서 x 축과 y 축을 정해서 
         위의 ggplot 막대 그래프를 그리는 샤이니 코드를 
         작성하시오 ! 

■ plotly  원형 그래프

install.packages("plotly")

library(plotly)

plot_ly(create_cnt, labels = ~colnames(create_cnt)[-1], values=~as.factor( create_cnt[create_cnt$X == '2013',-1] ), type='pie')

문제137. (점심시간 문제) 문제136번은 ggplot 으로 막대그래프를
         그리는 코드였는데 이번에는 plotly 로 원형 그래프가
         출력되는 샤이니 코드를 작성하시오!

         년도 선택하는 select input box 하나만 나오게 

답:

############## set this file location to working directory ##########################
packages <- 'rstudioapi'
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
library('rstudioapi')
current_dir<-dirname(rstudioapi::getSourceEditorContext()$path)
setwd(current_dir)

package_in<-function(p_name,option=1){
  packages <- p_name
  if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
    install.packages(setdiff(packages, rownames(installed.packages())))
  }
  if (option==1){
    library(p_name,character.only = TRUE)
  }
}

###########################1. 패키지 설치##########################################

package_in('shinydashboard')
package_in('shiny')
package_in('ggplot2')
package_in('plotly')

######################### 2. 화면 개발 ###########################################

sidebar <- dashboardSidebar(
  sidebarMenu(
    fileInput("file1", "Choose CSV File",
              multiple = FALSE,
              accept = c("text/csv",".xlsx",".txt",
                         "text/comma-separated-values,text/plain",
                         ".csv")),
    
    menuItem("Plot",
             menuSubItem('Barplot',tabName='barplot'),
             menuSubItem('Piechart',tabName='piechart')
    )
    
    
  )
)


body <- dashboardBody(
  
  tabItems(
    
    ##### bar plot
    tabItem(tabName = "barplot",
            sidebarPanel(
              selectInput("in_sel_bar_xVar","x Variable:", choices = NULL),
              selectInput("in_sel_bar_yVar","y Variable:", choices = NULL)
            ),
            mainPanel(
              plotOutput('plot_bar')
            )
    ),
    ##### piechart
    tabItem(tabName = "piechart",
            sidebarPanel(
              selectInput("in_sel_pie_xVar","x Variable:", choices = NULL)
            ),
            mainPanel(
              plotlyOutput('plot_pie')
            )
    )
  )
)



ui<-dashboardPage(
  dashboardHeader(title='my graph'),
  sidebar,
  body
  
)




######################3. 서버단 개발 ########################################


server <- function(input, output,session) {
  options(warn = -1)
  options(shiny.maxRequestSize = 30*1024^2)
  
  
  
  
  dataload<-reactive({
    req(input$file1)
    
    file1 = input$file1
    data1 = read.csv(file1$datapath)
    
    
    updateSelectInput(session, "in_sel_bar_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_bar_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_pie_xVar", choices = data1[,1])
    return(data1)
    
  })
  
  ####nomal_bar
  output$plot_bar <- renderPlot({
    table_in<-dataload()
    
    xdata<-as.factor(table_in[,input$in_sel_bar_xVar])
    ydata<-as.factor(table_in[,input$in_sel_bar_yVar])
    fdata=data.frame(x=xdata,y=ydata)
    
    
    ggplot(fdata) + 
      geom_bar(aes_string(x='x',y='y',fill='x'),stat = "identity",show.legend=F)
    
    
  })
  
  output$plot_pie <- renderPlotly({
    table_in<-dataload()
    
    plot_ly(table_in, labels = ~colnames(table_in)[-1], values=~as.factor( table_in[table_in[,1] == input$in_sel_pie_xVar,-1] ),type='pie')
    
    
  })
}

######################### 4. 샤이니 실행 ###############################

shinyApp(ui = ui, server = server)

■ 그래프 
 
 1. 기본 막대 그래프
 2. 기본 원형 그래프 
 3. 기본 라인 그래프 
 4. ggplot2 막대 그래프
 5. plotly  원형 그래프
 6. plotly  라인 그래프 
 7. 소리를 그래프로 시각화 하는 방법 

■   소리를 그래프로 시각화 하는 방법


 * 소리를 시각화 하기 위한 R 코드

 install.packages("tuneR")

 library(tuneR)

 audio <- readWave("output.wav")

 play(audio)

 head(audio@left, 1000)

 plot(head(audio@left,1000) ) 

문제138. 원더걸스의 so hot 을 시각화 하시오 !

 audio1 <- readWave("sohot.wav")

 play(audio1)

 head(audio1@left,1000)

 plot( head(audio1@left,1000) )

문제139. 정상적인 심장박동 소리를 포함해서 비정상적인
         심장 박동 소리를 각각 시각화 하시오 !

 normal.wav  <--- 정상 심장 박동 소리

 문제가 있는 심장 박동 소리들 

 ar.wav
 mr.wav
 ps.wav

graphics.off()
par(mfrow=c(2,2))
par(mar=c(1,1,1,1))

audio1 <- readWave("normal.wav")
audio2 <- readWave("ps.wav")
audio3 <- readWave("mr.wav")
audio4 <- readWave("ar.wav")

plot(audio1)
plot(audio2)
plot(audio3)
plot(audio4)


■ 그래프 
 
 1. 기본 막대 그래프
 2. 기본 원형 그래프 
 3. 기본 라인 그래프 
 4. ggplot2 막대 그래프
 5. plotly  원형 그래프
 6. plotly  라인 그래프 
 7. 소리를 그래프로 시각화 하는 방법 

문제140. 치킨집의 년도별 창업 현황을  plotly 의 라인 그래프
         로 그리시오 !

 library(plotly)

 plot_ly( data = create_cnt,
            x = ~create_cnt[ ,"X"],
            y = ~create_cnt[ ,"치킨집"],
          type="scatter",
          mode = "dot" )  

문제141. 샤이니의 사이드 메뉴에 linechart 를 추가하고
         Linechart 를 클릭하면 업종을 물어보게해서 
         해당 업종의 년도별 창업현황이 출력되게 하시오 ! 

############## set this file location to working directory 
##########################
packages <- 'rstudioapi'
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
library('rstudioapi')
current_dir<-dirname(rstudioapi::getSourceEditorContext()$path)
setwd(current_dir)

package_in<-function(p_name,option=1){
  packages <- p_name
  if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
    install.packages(setdiff(packages, rownames(installed.packages())))
  }
  if (option==1){
    library(p_name,character.only = TRUE)
  }
}

###########################1. 패키지 
설치##########################################

package_in('shinydashboard')
package_in('shiny')
package_in('ggplot2')
package_in('plotly')

######################### 2. 화면 개발 
###########################################

sidebar <- dashboardSidebar(
  sidebarMenu(
    fileInput("file1", "Choose CSV File",
              multiple = FALSE,
              accept = c("text/csv",".xlsx",".txt",
                         "text/comma-separated-values,text/plain",
                         ".csv")),
    
    menuItem("Plot",
             menuSubItem('Barplot',tabName='barplot'),
             menuSubItem('Piechart',tabName='piechart'),
             menuSubItem('Linechart',tabName='linechart')
    )
    
    
  )
)


body <- dashboardBody(
  
  tabItems(
    
    ##### bar plot
    tabItem(tabName = "barplot",
            sidebarPanel(
              selectInput("in_sel_bar_xVar","x Variable:", choices = NULL),
              selectInput("in_sel_bar_yVar","y Variable:", choices = NULL)
            ),
            mainPanel(
              plotOutput('plot_bar')
            )
    ),
    ##### piechart
    tabItem(tabName = "piechart",
            sidebarPanel(
              selectInput("in_sel_pie_xVar","x Variable:", choices = NULL)
            ),
            mainPanel(
              plotlyOutput('plot_pie')
            )
    ),
    ##### linechart
    tabItem(tabName = "linechart",
            sidebarPanel(
              selectInput("in_sel_line_xVar","x Variable:", choices = NULL),
              selectInput("in_sel_line_yVar","Y Variable:", choices = NULL)
            ),
            mainPanel(
              plotlyOutput('plot_line')
            )
    )
  )
)



ui<-dashboardPage(
  dashboardHeader(title='my graph'),
  sidebar,
  body
  
)




######################3. 서버단 개발 ########################################


server <- function(input, output,session) {
  options(warn = -1)
  options(shiny.maxRequestSize = 30*1024^2)
  
  
  
  
  dataload<-reactive({
    req(input$file1)
    
    file1 = input$file1
    data1 = read.csv(file1$datapath)
    
    
    updateSelectInput(session, "in_sel_bar_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_bar_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_pie_xVar", choices = data1[,1])
    
    updateSelectInput(session, "in_sel_line_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_line_yVar", choices = colnames(data1))
    return(data1)
    
  })
  
  ####nomal_bar
  output$plot_bar <- renderPlot({
    table_in<-dataload()
    
    xdata<-as.factor(table_in[,input$in_sel_bar_xVar])
    ydata<-as.factor(table_in[,input$in_sel_bar_yVar])
    fdata=data.frame(x=xdata,y=ydata)
    
    
    ggplot(fdata) + 
      geom_bar(aes_string(x='x',y='y',fill='x'),stat = 
                 "identity",show.legend=F)
    
    
  })
  
  output$plot_pie <- renderPlotly({
    table_in<-dataload()
    
    plot_ly(table_in, labels = ~colnames(table_in)[-1], values=~as.factor( 
      table_in[table_in[,1] == input$in_sel_pie_xVar,-1] ),type='pie')
    
    
  })
  
  output$plot_line <- renderPlotly({
    
    table_in<-dataload()
    
    #라벨 명 지정
    x <- list(title = input$in_sel_line_xVar)
    y <- list(title = input$in_sel_line_yVar)
    
    plot_ly(data = table_in,
            x=~table_in[,input$in_sel_line_xVar],
            y=~table_in[,input$in_sel_line_yVar],
            type='scatter',mode='dot')%>%
      layout(xaxis = x, yaxis = y)
    
    
  })
  
}

######################### 4. 샤이니 실행 ###############################

shinyApp(ui = ui, server = server)


■ 그래프 
 
 1. 기본 막대 그래프
 2. 기본 원형 그래프 
 3. 기본 라인 그래프 
 4. ggplot2 막대 그래프
 5. plotly  원형 그래프
 6. plotly  라인 그래프 
 7. 소리를 그래프로 시각화 하는 방법
 8. 산포도 그래프와 상관관계 

■ 산포도 그래프와 상관관계

  "산포도 그래프를 데이터간의 상관 관계를 나타낼때 유용하다 "

 예제 :  나이와 소득간의 상관관계가 있는지 데이터를 시각화 하시오

 age_income <- read.csv("age_income.csv", header=T )

 age_income 
 
 plot(age_income$age, age_income$month_income, 
      xlab="나이", ylab="소득", col="red", pch=16)  
 

문제142. plotly 패키지를 이용해서 나이와 소득의 산포도 그래프를
         그리시오 ! 

 library(plotly)

 plot_ly( data = age_income,
            x = ~age_income[ ,"age"],
            y = ~age_income[ ,"month_income"],
          type="scatter",
          mode = "markers" ,
          color = I('#ed2d07') )  

문제143. 샤이니의 사이드 메뉴 아래쪽에 Plotchart 를 추가해서
         x 축과 y 축을 입력받아 산포도 그래프가 출력되게 하시오!

         그래프 아래쪽에 상관계수도 같이 출력되게 하시오 !
 

############## set this file location to working directory ##########################
packages <- 'rstudioapi'
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
library('rstudioapi')
current_dir<-dirname(rstudioapi::getSourceEditorContext()$path)
setwd(current_dir)

package_in<-function(p_name,option=1){
  packages <- p_name
  if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
    install.packages(setdiff(packages, rownames(installed.packages())))
  }
  if (option==1){
    library(p_name,character.only = TRUE)
  }
}

###########################1. 패키지 설치##########################################

package_in('shinydashboard')
package_in('shiny')
package_in('ggplot2')
package_in('plotly')
package_in('lattice')

######################### 2. 화면 개발 ###########################################

sidebar <- dashboardSidebar(
  sidebarMenu(
    fileInput("file1", "Choose CSV File",
              multiple = FALSE,
              accept = c("text/csv",".xlsx",".txt",
                         "text/comma-separated-values,text/plain",
                         ".csv")),
    
    menuItem("Plot",
             menuSubItem('Barplot',tabName='barplot'),
             menuSubItem('Piechart',tabName='piechart'),
             menuSubItem('Lineplot',tabName='lineplot'),
             menuSubItem('Scatterplot',tabName='scatterplot')
    )
    
    
  )
)


body <- dashboardBody(
  
  tabItems(
    
    ##### bar plot
    tabItem(tabName = "barplot",
            sidebarPanel(
              selectInput("in_sel_bar_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_bar_xVar","x Variable:", choices = NULL)
            ),
            mainPanel(
              plotOutput('plot_bar')
            )
    ),
    ##### piechart
    tabItem(tabName = "piechart",
            sidebarPanel(
              selectInput("in_sel_pie_xVar","x Variable:", choices = NULL)
            ),
            mainPanel(
              plotlyOutput('plot_pie')
            )
    ),
    ##### line plot
    tabItem(tabName = "lineplot",
            sidebarPanel(
              selectInput("in_sel_line_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_line_xVar","x Variable:", choices = NULL)
              
            ),
            mainPanel(
              plotlyOutput('plot_line')
            )
    ),
    ##### scatter plot
    tabItem(tabName = "scatterplot",
            sidebarPanel(
              selectInput("in_sel_scatter_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_scatter_xVar","x Variable:", choices = NULL)
              
            ),
            mainPanel(
              plotOutput('plot_scatter'),
              textOutput('text_scatter')
            )
    )
  )
)



ui<-dashboardPage(
  dashboardHeader(title='my graph'),
  sidebar,
  body
  
)




######################3. 서버단 개발 ########################################


server <- function(input, output,session) {
  options(warn = -1)
  options(shiny.maxRequestSize = 30*1024^2)
  
  
  
  
  dataload<-reactive({
    req(input$file1)
    
    file1 = input$file1
    data1 = read.csv(file1$datapath)
    
    
    updateSelectInput(session, "in_sel_bar_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_bar_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_pie_xVar", choices = data1[,1])
    
    updateSelectInput(session, "in_sel_line_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_line_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_scatter_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_scatter_yVar", choices = colnames(data1))
    
    return(data1)
    
  })
  
  ####nomal_bar
  output$plot_bar <- renderPlot({
    table_in<-dataload()
    
    xdata<-as.factor(table_in[,input$in_sel_bar_xVar])
    ydata<-as.factor(table_in[,input$in_sel_bar_yVar])
    fdata=data.frame(x=xdata,y=ydata)
    
    
    ggplot(fdata) + 
      geom_bar(aes_string(x='x',y='y',fill='x'),stat = "identity",show.legend=F)
    
    
  })
  
  output$plot_pie <- renderPlotly({
    table_in<-dataload()
    
    plot_ly(table_in, labels = ~colnames(table_in)[-1], values=~as.factor( table_in[table_in[,1] == input$in_sel_pie_xVar,-1] ),type='pie')
    
    
  })
  
  output$plot_line <- renderPlotly({
    table_in<-dataload()
    
    x <- list(title = input$in_sel_line_xVar)
    y <- list(title = input$in_sel_line_yVar)
    
    plot_ly(data = table_in,x=~table_in[,input$in_sel_line_xVar],y=~table_in[,input$in_sel_line_yVar],type='scatter',mode='dot')%>%
      layout(xaxis = x, yaxis = y)
    
    
  })
  
  output$plot_scatter <- renderPlot({
    table_in<-dataload()
    
    xyplot(table_in[,input$in_sel_scatter_yVar]~table_in[,input$in_sel_scatter_xVar], grid=T,type=c('p','smooth'),col.line='darkorange',lwd=2, xlab=input$in_sel_scatter_xVar,ylab=input$in_sel_scatter_yVar)
    
  })
  
  output$text_scatter <- renderText({
    table_in<-dataload()
    paste("The correlation between the two is: ", cor(table_in[,input$in_sel_scatter_yVar],table_in[,input$in_sel_scatter_xVar]))
  })
  
}

######################### 4. 샤이니 실행 ###############################

shinyApp(ui = ui, server = server)

■ 그래프 
 
 1. 기본 막대 그래프
 2. 기본 원형 그래프 
 3. 기본 라인 그래프 
 4. ggplot2 막대 그래프
 5. plotly  원형 그래프
 6. plotly  라인 그래프 
 7. 소리를 그래프로 시각화 하는 방법
 8. 산포도 그래프와 상관관계 
 9. 사분위수 그래프 

■ 사분위수 그래프 

  * 사분위수 그래프로 분석해야하는 데이터

  1. 데이터의 퍼짐정도가 매우 큰 경우 
  2. 이상치 있는 경우 
  3. 평균 하나로는 통계를 대표할 수 없다.

    중앙값, 최빈값, 최대값, 최소값, 평균값 

  4. 사분위수 그래프를 가로로 눕혀놓고 보면
     정규분포 모양을 확인할 수 있다.

예제:   install.packages("lattice")

        library(lattice)

        bwplot(emp$sal)

        summary(emp$sal)

   Min.    1st Qu.       Median    Mean      3rd Qu.       Max. 
    800     1250          1550      2073      2944        5000 
    ↑       ↑             ↑       ↑       ↑            ↑
 최소값   1번째 사분위수   중앙값  평균값    3번째 사분위수 최대값
           (Q1)             (Q2)             (Q3) 

문제144. 우리반 나이 데이터를 가지고 사분위수 그래프를 그리시오
          

■ 그래프 
 
 1. 기본 막대 그래프
 2. 기본 원형 그래프 
 3. 기본 라인 그래프 
 4. ggplot2 막대 그래프
 5. plotly  원형 그래프
 6. plotly  라인 그래프 
 7. 소리를 그래프로 시각화 하는 방법
 8. 산포도 그래프와 상관관계 
 9. 사분위수 그래프 
 10. 샤이니에 데이터 테이블 표시 하는 방법 
 11. 오라클 데이터베이스와 R 과 연동하여 샤이니에 구현 
 12. 워드 클라우드 
 13. 구글 지도 그래프 

■ 샤이니에 데이터 테이블 표시 하는 방법 


         그림 캡쳐  

* 기본 테이블 형태의 데이터 출력하는 샤이니 기본 코드


#install.packages("DT")

#library(DT)
library(shiny)
library(ggplot2)

emp <- read.csv("d:\\emp.csv",header=T)

# Define UI ----
ui <- fluidPage(
  titlePanel("EMP DataTable"),

  # Create a new Row in the UI for selectInputs
 
  # Create a new row for the table.
  DT::dataTableOutput("table")

)

# Define server logic ----
server <- function(input, output) {
  # Filter data based on selections
  output$table <- DT::renderDataTable(DT::datatable({
    data <- emp
  
  }))
  
}

# Run the app ----
shinyApp(ui = ui, server = server)

문제145. 우리 기존의 완성한 샤이니 코드에 csv 파일을 
         불러오면 아래와 같이 테이블 형태로 출력되도록
         사이드 텝을 만들고 구현하시오 !


############## set this file location to working directory ##########################
packages <- 'rstudioapi'
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
library('rstudioapi')
current_dir<-dirname(rstudioapi::getSourceEditorContext()$path)
setwd(current_dir)

package_in<-function(p_name,option=1){
  packages <- p_name
  if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
    install.packages(setdiff(packages, rownames(installed.packages())))
  }
  if (option==1){
    library(p_name,character.only = TRUE)
  }
}

###########################1. 패키지 설치##########################################

package_in('shinydashboard')
package_in('shiny')
package_in('ggplot2')
package_in('plotly')
package_in('lattice')

######################### 2. 화면 개발 ###########################################

sidebar <- dashboardSidebar(
  sidebarMenu(
    fileInput("file1", "Choose CSV File",
              multiple = FALSE,
              accept = c("text/csv",".xlsx",".txt",
                         "text/comma-separated-values,text/plain",
                         ".csv")),
    menuItem("Table",
             menuSubItem('Tableformat',tabName='Tableformat'),
             menuSubItem('DBconnect',tabName='DBconnect')),
    menuItem("Plot",
             menuSubItem('Barplot',tabName='barplot'),
             menuSubItem('Piechart',tabName='piechart'),
             menuSubItem('Lineplot',tabName='lineplot'),
             menuSubItem('Scatterplot',tabName='scatterplot')
    )
    
    
  )
)


body <- dashboardBody(
  
  tabItems(
    ####table_format
    tabItem(tabName = 'Tableformat',
            
            mainPanel(
              DT::dataTableOutput("table")
            )
            ),
    
    ##### bar plot
    tabItem(tabName = "barplot",
            sidebarPanel(
              selectInput("in_sel_bar_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_bar_xVar","x Variable:", choices = NULL)
            ),
            mainPanel(
              plotOutput('plot_bar')
            )
    ),
    ##### piechart
    tabItem(tabName = "piechart",
            sidebarPanel(
              selectInput("in_sel_pie_xVar","x Variable:", choices = NULL)
            ),
            mainPanel(
              plotlyOutput('plot_pie')
            )
    ),
    ##### line plot
    tabItem(tabName = "lineplot",
            sidebarPanel(
              selectInput("in_sel_line_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_line_xVar","x Variable:", choices = NULL)
              
            ),
            mainPanel(
              plotlyOutput('plot_line')
            )
    ),
    ##### scatter plot
    tabItem(tabName = "scatterplot",
            sidebarPanel(
              selectInput("in_sel_scatter_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_scatter_xVar","x Variable:", choices = NULL)
              
            ),
            mainPanel(
              plotOutput('plot_scatter'),
              textOutput('text_scatter')
            )
    )
  )
)



ui<-dashboardPage(
  dashboardHeader(title='my graph'),
  sidebar,
  body
  
)




######################3. 서버단 개발 ########################################


server <- function(input, output,session) {
  options(warn = -1)
  options(shiny.maxRequestSize = 30*1024^2)
  
  
  
  
  dataload<-reactive({
    req(input$file1)
    
    file1 = input$file1
    data1 = read.csv(file1$datapath)
    
    
    updateSelectInput(session, "in_sel_bar_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_bar_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_pie_xVar", choices = data1[,1])
    
    updateSelectInput(session, "in_sel_line_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_line_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_scatter_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_scatter_yVar", choices = colnames(data1))
    
  
    return(data1)
    
  })
  ####table_format
  output$table <- DT::renderDataTable(DT::datatable({
    req(input$file1)
    
    file1 = input$file1
    data1 = read.csv(file1$datapath)
    
    
  }))
  
  ####nomal_bar

  output$plot_bar <- renderPlot({
    table_in<-dataload()
    
    xdata<-as.factor(table_in[,input$in_sel_bar_xVar])
    ydata<-as.factor(table_in[,input$in_sel_bar_yVar])
    fdata=data.frame(x=xdata,y=ydata)
    
    
    ggplot(fdata) + 
      geom_bar(aes_string(x='x',y='y',fill='x'),stat = "identity",show.legend=F)
    
    
  })
  ####nomal_pie
  output$plot_pie <- renderPlotly({
    table_in<-dataload()
    
    plot_ly(table_in, labels = ~colnames(table_in)[-1], values=~as.factor( table_in[table_in[,1] == input$in_sel_pie_xVar,-1] ),type='pie')
    
    
  })
  ####nomal_line
  output$plot_line <- renderPlotly({
    table_in<-dataload()
    
    x <- list(title = input$in_sel_line_xVar)
    y <- list(title = input$in_sel_line_yVar)
    
    plot_ly(data = table_in,x=~table_in[,input$in_sel_line_xVar],y=~table_in[,input$in_sel_line_yVar],type='scatter',mode='dot')%>%
      layout(xaxis = x, yaxis = y)
    
    
  })
  ####nomal_scatter
  output$plot_scatter <- renderPlot({
    table_in<-dataload()
    
    xyplot(table_in[,input$in_sel_scatter_yVar]~table_in[,input$in_sel_scatter_xVar], grid=T,type=c('p','smooth'),col.line='darkorange',lwd=2, xlab=input$in_sel_scatter_xVar,ylab=input$in_sel_scatter_yVar)
    
  })
  ###상관관계
  output$text_scatter <- renderText({
    table_in<-dataload()
    paste("The correlation between the two is: ", cor(table_in[,input$in_sel_scatter_yVar],table_in[,input$in_sel_scatter_xVar]))
  })
  
}

######################### 4. 샤이니 실행 ###############################

shinyApp(ui = ui, server = server)

■ 오라클 database 와 R 과의 연동 

install.packages('DBI')
install.packages('RJDBC')

library(RJDBC)
library(DBI)


driver <- JDBC('oracle.jdbc.driver.OracleDriver', 'ojdbc6.jar')

oracle_db <- dbConnect(driver, 'jdbc:oracle:thin:@//127.0.0.1:1521/xe', 'scott', 'tiger')

emp_query <- 'select * from emp'

emp_data <- dbGetQuery(oracle_db, emp_query)

emp_data

■ Rjava 오류 해결

http://cafe.daum.net/oracleoracle/SRv4/13 <-- 참고 

1. 아래에서 java 64비트를 다운로드 받는다 

https://www.java.com/en/download/manual.jsp

2. 자바 설치시 대상 폴더 변경으로 설치 

3. 아래와 같이 환경설정을 한다.

Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_201')

4. 설치를 한다. 
install.packages("rJava")

library(rJava)

문제146.  부서번호가 10번인 사원들의 이름과 월급을 오라클 
          데이터베이스에서 불러와서 R 에서 출력하시오 !

emp_query <- 'select ename, sal from  emp where deptno = 10'

emp_data <- dbGetQuery(oracle_db, emp_query)

emp_data  

문제147. 이름과 부서위치를 오라클 데이터베이스에서 불러와서
         R 에서 출력하시오 ! ( 1999 ansi 문법) 

emp_query <- 'select e.ename, d.loc
               from  emp  e  join  dept  d
               on  (e.deptno = d.deptno ) ' 

emp_data <- dbGetQuery(oracle_db, emp_query)

emp_data  

문제148. scott 유져가 가지고 있는 테이블이 무엇이 있는지
         확인하시오 !


emp_query <- 'select table_name from  user_tables ' 

emp_data <- dbGetQuery(oracle_db, emp_query)

emp_data  

문제149. R 샤이니에서 오라클 데이터 베이스의 테이블의 데이터를
         쉽게 볼 수 있도록 구현하시오 !



문제150.(점심시간 문제) 오라클 데이터 베이스에서 선택한 
        테이블로 그래프가 그려지게 하시오 ! 


■ 그래프 
 
 1. 기본 막대 그래프
 2. 기본 원형 그래프 
 3. 기본 라인 그래프 
 4. ggplot2 막대 그래프
 5. plotly  원형 그래프
 6. plotly  라인 그래프 
 7. 소리를 그래프로 시각화 하는 방법
 8. 산포도 그래프와 상관관계 
 9. 사분위수 그래프 
 10. 샤이니에 데이터 테이블 표시 하는 방법 
 11. 오라클 데이터베이스와 R 과 연동하여 샤이니에 구현 
 12. 워드 클라우드 
 13. 구글 지도 그래프 

■ 지도 그래프 

문제151. maps 패키지를 설치하고 중국지도만 확대해서 
         출력하시오 !

install.packages("maps")
install.packages("mapproj")

library(maps)
library(mapproj)

map("world") 

map("world","china")

문제152. 우리나라 지도를 출력하시오 !

map("world","south korea")

문제153. 프랑스 지도를 출력하시오 !

map("world","france")

문제154. 구글 지도 그래프를 이용해서 서울 지역의 지하철
         2호선의 그래프를 시각화 하시오 ! 

install.packages("ggplot2")
install.packages("ggmap")

library(ggplot2)
library(ggmap)

loc <- read.csv("서울지하철2호선위경도정보.csv" , header=T)

loc

center <- c(mean(loc$LON), mean(loc$LAT) ) 
center 

kor <- get_map(center, zoom=11, maptype="roadmap")

에러: Google now requires an API key.
       See ?register_google for details.
> kor 

kor 

https://cloud.google.com/maps-platform/?hl=ko -> 시작하기 

-> Google Maps Platform 사용 설정 하기 다 체크 하고 
   계속해서 회원가입하고 


register_google(key="AIzaSyDrHR63mX00v-bRlSlS54PbUYeC7KOcIVM")

library(ggmap)
library(ggplot2)

center <- c(mean(loc$LON), mean(loc$LAT) ) 
center 

kor <- get_map(center, zoom=11, maptype="roadmap")

ggmap(kor)

전체 코드:

library(ggplot2)

library(ggmap)

loc <- read.csv("서울지하철2호선위경도정보.csv",header=T)

center <- c(mean(loc$LON),mean(loc$LAT)) 
kor <- get_map(center,zoom=11, maptype="roadmap")
kor.map <- ggmap(kor) + geom_point(data=loc,aes(x=LON,y=LAT),size=3, alpha=0.7) 
kor.map + geom_text(data=loc, aes(x=LON,y=LAT+0.005,label=역명),size=3) 




문제155. 지역별 인구수에 대한 지도 그래프를 그리시오


library(ggmap)
library(ggplot2)

register_google(key="AIzaSyDrHR63mX00v-bRlSlS54PbUYeC7KOcIVM")


population <- read.csv("d:/data/지역별인구.csv",header=T)

cen <- c(mean(population$LON),mean(population$LAT))

map <- get_googlemap(center=cen, maptyp="roadmap",zoom=7)
ggmap(map)

ggmap(map)+
  geom_point(data=population,aes(x=LON,y=LAT,colour=지역,size=총인구수))+
  xlab("longitude(경도)") +
  ylab("latitude(위도)")

ggmap(map)+
  geom_text(data=population,aes(x=LON,y=LAT,color=factor(지역),size=10,label=seq_along(지역)))

■  워드 클라우드

문제156. 겨울왕국 대본을 워드 클라우드로 그리시오 ! 

library(KoNLP)
library(wordcloud)
library(plyr)
 
useSejongDic()  # 세종사전에 있는 한글을 R 로 로드하는 명령어 

winter <- readLines('winter.txt')

# 겨울왕국 대본에서 단어만 추출 
nouns <- extractNoun(winter)   

nouns <- unlist(nouns)

#단어중에 2철자 이상인것만 추출  
nouns <- nouns[nchar(nouns)>=2]

# 단어별 건수 출력 
cnouns <- count(nouns)

you  181
You   53
YOU   44

# 색깔 추가 
pal <- brewer.pal(6,"Dark2")
pal <- pal[-(1)]
 
# 글씨체 추가 
windowsFonts(malgun=windowsFont("맑은 고딕"))

wordcloud(words=cnouns$x,  # 단어
          freq=cnouns$freq,  # 건수
          colors=pal,  $ 색깔 
          min.freq=3,  # 빈도수가 3단어 이상인것만 시각화
          random.order=F,  # F 로 하면 가장 많은것부터 
                           # 중앙에서 부터 퍼지게 한다. 
           family="malgun") # 맑음 글씨체로 시각화 

문제157. 영어 성경을 워드 클라우드로 시각화 하시오 ! 



library(KoNLP)
library(wordcloud)
library(plyr)
 
useSejongDic()

winter <- readLines('NIV.txt')

nouns <- extractNoun(winter)   

nouns <- unlist(nouns)
 
nouns <- nouns[nchar(nouns)>=2]

cnouns <- count(nouns)
 
 
pal <- brewer.pal(6,"Dark2")
pal <- pal[-(1)]
 
windowsFonts(malgun=windowsFont("맑은 고딕"))

data.table(words=cnouns$x, freq=cnouns$freq)
 
wordcloud(words=cnouns$x, freq=cnouns$freq, colors=pal, min.freq=3, 
          random.order=F, family="malgun")

문제158. 안철수 연설문을 워드 클라우드로 시각화 하시오 ! 
ahn <- "안녕하십니까 안철수입니다.
 
저는 지난 7월말에 말씀 드린 대로 국민들의 의견을 듣고자 많은 분들을 만났습니다.
 
그 동안 저는 재미있는 별명도 얻었고.
또 최근에는 저를 소재로 한 유머도 유행하더군요.
 
그동안 제 답을 기다려오신 여러 분들의 애정이라고 생각하고 
그 또한 무겁게 받아들이겠습니다.
 
기업인과 교수의 삶을 살아온 저로서는,
국가경영의 막중한 책임을 지는 결심에 이르기까지 
정말 많은 생각을 하지 않을 수 없었습니다.
 
저는 그동안 춘천에서 만난 어르신, 명예퇴직을 앞둔 중년의 가장, 
30대의 쌍둥이 엄마와 같은 많은 이웃들을 만나 뵈었고, 
각 분야에서 경륜과 전문성을 가진 분들도 만났습니다.
 
가능하면 조용하게 경청하고 귀를 기울였습니다.
 
어느 한분 힘들지 않은 분들이 없었습니다.
 
중산층이 무너지고 저소득층이 너무 고통 받고 있었습니다.
 
하지만 그렇게 힘들고 고단한 삶의 과정에서도 
그분들은 끊임없이 희망을 만들고 계셨습니다.
 
나 자신보다는 우리 아이들의 미래를 위해 
참고 견디고 희생하고 헌신할 준비가 되어 있습니다.
 
제가 희망을 드린 것이 아니라 제가 오히려 그분들께 힘과 용기를 얻었습니다.
모두 고맙습니다.
여러분이 제게는 스승입니다.
 
그 분들이 저를 한걸음 더 나아가게 했습니다.
 
그 분들이 제게 한결 같이 하신 말씀이 있습니다.
 
'정치가 이래서는 안 된다'는 겁니다.
'문제를 풀어야 할 정치가 문제를 만들고 있다'고 하셨습니다.
'국민들의 삶을 외면하고 국민을 분열시키고, 국민을 무시하고, 
서로 싸우기만 하는 정치에 실망하고 절망했다' 하셨습니다.
 
또 한 번도 정치에 발 딛지 않은 제가 '잘 할 수 있을까' 고민할 때 
많은 분들이 왜 제게 지지를 보내는지 설명해 주셨습니다.
 
'이제 좀 정치를 다르게 해보자, 새롭게 출발해보자'는 뜻이라는 겁니다.
 
하지만 저는 제 역량에 대해 고민했습니다.
국가의 리더라는 자리는 절대 한 개인이 영광으로 탐할 자리가 될 수도 없고, 
되어서도 안 된다고 생각합니다.
 
저에게는 당선 여부보다는 잘 해낼 수 있느냐가 중요했습니다.
 
그래서 스스로에게 거듭 질문을 던지고 대화를 통해 답을 찾고자 노력했습니다.
저는 이제 제 자신 스스로에게 질문했던 답을 내어놓으려 합니다.
 
지금까지 국민들은 저를 통해 정치쇄신에 대한 열망을 표현해주셨습니다.
 
저는 이제 
이번 18대 대통령 선거에 출마함으로써 
그 열망을 실천해내는 사람이 되려 합니다.
 
저에게 주어진 시대의 숙제를 감당하려고 합니다.
 
저는 먼저 정치개혁은 선거과정에서부터 시작해야한다고 생각합니다.
국민의 반을 적으로 돌리면서 통합을 외치는 것은 위선입니다.
선거과정에서 부당하고 저급한 흑색선전과 이전투구를 계속하면, 
서로를 증오하고 지지자들을 분열시키며, 나아가서는 국민을 분열시킵니다.
그렇게 선거가 끝나고 나면 선거에서 이겨도 국민의 절반 밖에 마음을 얻지 못합니다.
 
앞으로도 이런 일이 계속 된다면 다음 5년도 
분열과 증오의 시간을 보낼 수밖에 없을 겁니다.
누가 대통령이 되더라도 통합과 사회문제 해결은 요원한 일일 것입니다.
 
그래서 저는 저부터 선거과정에서의 쇄신을 약속드리겠습니다.
 
저는 선거과정에서 어떤 어려움과 유혹이 있더라도 
흑색선전과 같은 낡은 정치는 하지 않겠습니다.
 
그리고 어떤 결과가 나오더라도 
저를 지지하는 분들이 그 결과를 존중하고 같이 축하할수 있도록 노력하겠습니다.
 
박근혜 후보와 문재인 후보께 제안합니다.
 
모두 한자리에 모여, 
국민들을 증인으로 선의의 정책 경쟁을 할 것을 약속하면 어떻겠습니까?
 
그리고 선거후에도 승리한 사람은 다른 후보들의 이야기에 귀를 기울이며, 
패배한 사람은 깨끗이 결과에 승복하여 
더 나은 우리의 미래를 만들기 위해 협력할 것도 같이 약속하면 어떨까요? 
 
그래야 분열과 증오의 정치를 넘어서
우리의 미래를 위한 에너지로 바꿔 놓을 수 있을 겁니다.
 
누가 당선 되더라도 국민을 위해서라면 
서로 도울 수 있고 또 함께 할 수 있는 
통합의 시작점이 될 수 있습니다.
 
그러한 정책 대결 속에서 제가 만약 당선된다면 
다른 후보들의 더 나은 정책이 있다면 받아들이고 또 경청할 겁니다.
 
이것이 바로 국민들이 원하는 덧셈의 정치, 통합의 정치라고 저는 생각합니다.
 
많은 분들이 정치 경험도 없는데 
막상 대통령이 되면 어떻게 할 것이냐고 걱정을 하셨습니다.
정치라는 험한 곳에 들어가 괜히 만신창이가 되지 말라고도 하셨습니다.
지금 이 자리에도 그런 생각을 가진 분들이 계실 겁니다.
 
저는 정치경험뿐 아니라 조직도 없고, 세력도 없지만, 그만큼 빚진 것도 없습니다.
정치경험 대신 국민들께 들은 이야기를 소중하게 가지고 가겠습니다.
조직과 세력 대신 나라를 위해 애쓰시는 모든 분들과 함께 나아가겠습니다.
빚진 게 없는 대신, 공직을 전리품으로 배분하는 일만큼은 결코 하지 않을 것입니다.
 
사실 대통령 한 사람의 힘으로 5년 만에 모든 문제를 해결 할 수는 없습니다.
그렇지만 대한민국은 이미 현명한 국민들과 많은 전문가들이 
요소요소에서 각자가 역할을 하는 커다란 시스템을 이루고 있습니다.
 
그 속에 이미 답이 있습니다.
 
지금 대한민국은 낡은 체제와 미래가치가 충돌하고 있습니다.
이제 낡은 물줄기를 새로운 미래를 향해 바꿔야 합니다.
국민들의 민의를 반영하지 못하는 정치 시스템, 
빈부격차가 심해지고 일자리를 창출하지 못하는 경제 시스템,
계층 간의 이동이 차단된 사회시스템, 
공정한 기회가 부여되지 않는 기득권 과보호구조,
지식산업시대에 역행하는 옛날 방식의 의사결정구조, 
 
이와 같은 것들로는 미래를 열어갈 수 없습니다.
더 이상 이대로는 안 됩니다.
 
국민들은 이제 정치부터 바꿔야 한다고 이야기하십니다.
앞으로 5년은 누가 대통령이 되더라도 매우 힘든 상황이 전개될 것입니다.
 
국내의 가계부채와 부동산 문제가 정말 심각합니다. 세계적인 장기불황까지 겹쳐 한꺼번에 
위기적 상황이 닥쳐올 가능성이 많습니다.
 
이러한 상황 하에서 제가 혼자서 모든 문제를 해결하고 
세상을 바꿀 수 있다고 생각하지 않습니다.
 
저도 열심히 살려고 노력했지만 부족하고 실수도 하고 결점이 많은 사람이기 때문입니다.
 
하지만, 현명한 국민들과 전문가들 속에서 답을 구하고, 지혜를 모으면 
그래도 최소한 물줄기는 돌려놓을 수 있을 거라고 생각합니다.
 
위기의 시대에 힘을 합쳐 함께 어려움을 헤쳐나갈 수 있다고 생각합니다.
 
정치가 바뀌어야 우리 삶이 바뀔 수 있습니다.
새로운 정치가 들어서야 민생경제 중심 경제가 들어섭니다.
 
대한민국은 새로운 경제모델이 필요합니다.
지금 논의되고 있는 경제민주화와 복지는 성장동력과 결합하는 경제혁신을 만들어야 합니다.
 
평화체제는 역시 안보와 균형을 맞출 때 실현가능합니다.
 
제 정책비전과 구상의 구체적 내용은 앞으로 선거과정에서 말씀드리겠습니다.
 
저는 이번 선거 과정부터 
국민의 생각이 하나로 모아지는 첫걸음을 시작했으면 좋겠습니다.
 
이번 선거를 통해 새로운 변화를 원하는 국민의 마음이 하나로 모아지면 좋겠습니다.
 
저는 세상을 움직이는 것은 진심이라고 생각합니다.
진심의 정치를 하겠습니다.
 
그 과정에서 저를 향한 공격이나 비난은 두렵지 않습니다. 극복하겠습니다.
더 나은 미래를 만들기 위해 싸워야 한다면 정정당당하게 싸울 것입니다.
사람의 선의가 가장 강력한 힘이 될 수 있다는 것을 
국민여러분과 함께 증명하려고 합니다.
 
저에게 많은 이야기를 들려주신 
그리고 많은 지지를 보내주신 국민여러분 
저와 함께 해주십시오.
 
그래야 정치가 바뀌고 정치가 바뀌어야 우리의 삶이 바뀝니다.
변화의 열쇠는 바로 국민 여러분께 있습니다.
국민이 선택하는 새로운 변화가 시작됩니다.
 
마지막으로 
제가 좋아하는 작가, 윌리엄 깁슨의 말을 하나 소개하고 싶습니다.
 
'미래는 이미 와 있다. 단지 널리 퍼져있지 않을 뿐이다'
그렇습니다. 미래는 지금 우리 앞에 있습니다.
 
고맙습니다."
 
 
library(KoNLP)
library(wordcloud)
library(plyr)
 
useSejongDic()
 
Backup was just finished!
370957 words dictionary was built.


mergeUserDic(data.frame(c('안철수', '박근혜', '문제인'), c('nqpc')))  #  추가 시키고 싶은
 
nouns <- extractNoun(ahn)   
 
nouns <- nouns[nchar(nouns)>=2]
 
cnouns <- count(nouns)
 
 
pal <- brewer.pal(6,"Dark2")
pal <- pal[-(1)]
 
windowsFonts(malgun=windowsFont("맑은 고딕"))

data.table(words=cnouns$x, freq=cnouns$freq)
 
wordcloud(words=cnouns$x, freq=cnouns$freq, colors=pal, min.freq=3, 
          random.order=F, family="malgun")

문제159. 샤이니에서 wordcloud 를 보는 코드를 실행해보시오 !


install.packages("tm")

library(shiny)

library(tm)
library(wordcloud)
library(memoise)

# The list of valid books
books <<- list("A Mid Summer Night's Dream" = "summer",
               "The Merchant of Venice" = "merchant",
               "Romeo and Juliet" = "romeo")

# Using "memoise" to automatically cache the results
getTermMatrix <- memoise(function(book) {
  # Careful not to let just any name slip in here; a
  # malicious user could manipulate this value.
  if (!(book %in% books))
    stop("Unknown book")
  
  text <- readLines(sprintf("d://%s.txt.gz", book),
                    encoding="UTF-8")
  
  myCorpus = Corpus(VectorSource(text))
  myCorpus = tm_map(myCorpus, content_transformer(tolower))
  myCorpus = tm_map(myCorpus, removePunctuation)
  myCorpus = tm_map(myCorpus, removeNumbers)
  myCorpus = tm_map(myCorpus, removeWords,
                    c(stopwords("SMART"), "thy", "thou", "thee", "the", "and", "but"))
  
  myDTM = TermDocumentMatrix(myCorpus,
                             control = list(minWordLength = 1))
  
  m = as.matrix(myDTM)
  
  sort(rowSums(m), decreasing = TRUE)
})

# Define UI ----
ui <- fluidPage(
  # Application title
  titlePanel("Word Cloud"),
  
  sidebarLayout(
    # Sidebar with a slider and selection inputs
    sidebarPanel(
      selectInput("selection", "Choose a book:",
                  choices = books),
      actionButton("update", "Change"),
      hr(),
      sliderInput("freq",
                  "Minimum Frequency:",
                  min = 1,  max = 50, value = 15),
      sliderInput("max",
                  "Maximum Number of Words:",
                  min = 1,  max = 300,  value = 100)
    ),
    
    # Show Word Cloud
    mainPanel(
      plotOutput("plot")
    )
  )
)

# Define server logic ----
server <- function(input, output, session) {
  # Define a reactive expression for the document term matrix
  terms <- reactive({
    # Change when the "update" button is pressed...
    input$update
    # ...but not for anything else
    isolate({
      withProgress({
        setProgress(message = "Processing corpus...")
        getTermMatrix(input$selection)
      })
    })
  })
  
  # Make the wordcloud drawing predictable during a session
  wordcloud_rep <- repeatable(wordcloud)
  
  output$plot <- renderPlot({
    v <- terms()
    wordcloud_rep(names(v), v, scale=c(4,0.5),
                  min.freq = input$freq, max.words=input$max,
                  colors=brewer.pal(8, "Dark2"))
  })
}

# Run the app ----
shinyApp(ui = ui, server = server)






■ 그래프 
 
 1. 기본 막대 그래프
 2. 기본 원형 그래프 
 3. 기본 라인 그래프 
 4. ggplot2 막대 그래프
 5. plotly  원형 그래프
 6. plotly  라인 그래프 
 7. 소리를 그래프로 시각화 하는 방법
 8. 산포도 그래프와 상관관계 
 9. 사분위수 그래프 
 10. 샤이니에 데이터 테이블 표시 하는 방법 
 11. 오라클 데이터베이스와 R 과 연동하여 샤이니에 구현 
 12. 워드 클라우드 
 13. 구글 지도 그래프 

■ 사분위수 그래프를 샤이니 코드에 추가 

library(lattice)

bwplot(emp$sal)

문제160. 사원 테이블에 대하여 박스 그래프와 산포도 그래프를
         겹쳐서 보이게 출력하시오 

graphics.off()
plot( emp$sal, col='blue')
par(new=T)
boxplot(emp$sal) 



문제161. 사분위수 그래프를 샤이니에 추가하시오 ! 


############## set this file location to working directory ##########################
packages <- 'rstudioapi'
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
library('rstudioapi')
current_dir<-dirname(rstudioapi::getSourceEditorContext()$path)
setwd(current_dir)

package_in<-function(p_name,option=1){
  packages <- p_name
  if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
    install.packages(setdiff(packages, rownames(installed.packages())))
  }
  if (option==1){
    library(p_name,character.only = TRUE)
  }
}

###########################1. 패키지 설치##########################################

package_in('shinydashboard')
package_in('shiny')
package_in('ggplot2')
package_in('plotly')
package_in('lattice')

######################### 2. 화면 개발 ###########################################

sidebar <- dashboardSidebar(
  sidebarMenu(
    fileInput("file1", "Choose CSV File",
              multiple = FALSE,
              accept = c("text/csv",".xlsx",".txt",
                         "text/comma-separated-values,text/plain",
                         ".csv")),
    menuItem("테이블",
             menuSubItem('Tableformat',tabName='tableformat') ),
    
    menuItem("그래프",
             menuSubItem('Barplot',tabName='barplot'),
             menuSubItem('Piechart',tabName='piechart'),
             menuSubItem('Lineplot',tabName='lineplot'),
             menuSubItem('Scatterplot',tabName='scatterplot'),
             menuSubItem('boxplot',tabName='boxplot')
    )
    
    
  )
)


body <- dashboardBody(
  
  
  
  tabItems(
    
    ##### table_format
    tabItem(tabName = "tableformat",
            
            mainPanel(
              DT::dataTableOutput("table")
            )
    ),
    
    ##### bar plot
    tabItem(tabName = "barplot",
            sidebarPanel(
              selectInput("in_sel_bar_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_bar_xVar","x Variable:", choices = NULL)
            ),
            mainPanel(
              plotOutput('plot_bar')
            )
    ),
    ##### piechart
    tabItem(tabName = "piechart",
            sidebarPanel(
              selectInput("in_sel_pie_xVar","x Variable:", choices = NULL)
            ),
            mainPanel(
              plotlyOutput('plot_pie')
            )
    ),
    ##### line plot
    tabItem(tabName = "lineplot",
            sidebarPanel(
              selectInput("in_sel_line_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_line_xVar","x Variable:", choices = NULL)
              
            ),
            mainPanel(
              plotlyOutput('plot_line')
            )
    ),
    ##### scatter plot
    tabItem(tabName = "scatterplot",
            sidebarPanel(
              selectInput("in_sel_scatter_yVar","y Variable:", choices = NULL),
              selectInput("in_sel_scatter_xVar","x Variable:", choices = NULL)
              
            ),
            mainPanel(
              plotOutput('plot_scatter'),
              textOutput('text_scatter')
            )
    ),
    ##### scatter plot
    tabItem(tabName = "boxplot",
            sidebarPanel(
                 selectInput("in_sel_box_xVar","x Variable:", choices = NULL)
              
            ),
            mainPanel(
              plotOutput('plot_box')
            )
    )
  )
)


ui<-dashboardPage(
  dashboardHeader(title='my graph'),
  sidebar,
  body
)




######################3. 서버단 개발 ########################################


server <- function(input, output,session) {
  options(warn = -1)
  options(shiny.maxRequestSize = 30*1024^2)
  
  
  
  
  dataload<-reactive({
    req(input$file1)
    
    file1 = input$file1
    data1 = read.csv(file1$datapath)
    
    
    updateSelectInput(session, "in_sel_bar_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_bar_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_pie_xVar", choices = data1[,1])
    
    updateSelectInput(session, "in_sel_line_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_line_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_scatter_xVar", choices = colnames(data1))
    updateSelectInput(session, "in_sel_scatter_yVar", choices = colnames(data1))
    
    updateSelectInput(session, "in_sel_box_xVar", choices = colnames(data1))
    
    return(data1)
    
  })
  
  ####table_format
  output$table <- DT::renderDataTable(DT::datatable({
    req(input$file1)
    
    file1 = input$file1
    data1 = read.csv(file1$datapath)
    
    
  }))
  
  
  ####nomal_bar
  output$plot_bar <- renderPlot({
    table_in<-dataload()
    
    xdata<-as.factor(table_in[,input$in_sel_bar_xVar])
    ydata<-as.factor(table_in[,input$in_sel_bar_yVar])
    fdata=data.frame(x=xdata,y=ydata)
    
    
    ggplot(fdata) + 
      geom_bar(aes_string(x='x',y='y',fill='x'),stat = "identity",show.legend=F)
    
    
  })
  
  output$plot_pie <- renderPlotly({
    table_in<-dataload()
    
    plot_ly(table_in, labels = ~colnames(table_in)[-1], values=~as.factor( table_in[table_in[,1] == input$in_sel_pie_xVar,-1] ),type='pie')
    
    
  })
  
  output$plot_line <- renderPlotly({
    table_in<-dataload()
    
    x <- list(title = input$in_sel_line_xVar)
    y <- list(title = input$in_sel_line_yVar)
    
    plot_ly(data = table_in,x=~table_in[,input$in_sel_line_xVar],y=~table_in[,input$in_sel_line_yVar],type='scatter',mode='dot')%>%
      layout(xaxis = x, yaxis = y)
    
    
  })
  
  output$plot_scatter <- renderPlot({
    table_in<-dataload()
    
    xyplot(table_in[,input$in_sel_scatter_yVar]~table_in[,input$in_sel_scatter_xVar], grid=T,type=c('p','smooth'),col.line='darkorange',lwd=2, xlab=input$in_sel_scatter_xVar,ylab=input$in_sel_scatter_yVar)
    
  })
  
  output$text_scatter <- renderText({
    table_in<-dataload()
    paste("The correlation between the two is: ", cor(table_in[,input$in_sel_scatter_yVar],table_in[,input$in_sel_scatter_xVar]))
  })
  
  output$plot_box <- renderPlot({
    table_in<-dataload()
    
    bwplot(~table_in[,input$in_sel_box_xVar], data=table_in,xlab=input$in_sel_box_xVar)
    
  })
  
  
}

######################### 4. 샤이니 실행 ###############################

shinyApp(ui = ui, server = server)

■ R 을 활용한 머신러닝 

- 통계수업 (오전 통계 수업, 오후 포트폴리오 제작)
         - 데이터의 종류 
         - 데이터 수치 요약 
         - 확률과 확률변수 
         - 확률과 표본분포
         - 통계적 추정 
         - 통계적 가설검정 

 통계학 개론 ---------> 머신러닝 

 ■ 머신러닝책 수업 목차 

          머신러닝              <--------------------------- 통계학 개론 

    1. 머신러닝이 무엇인지 ?  (1장)   

    2. 머신러닝을 배우기 위해     <---------------- 1장. 데이터의 종류
       기본적으로 알야하는 내용(2장)                2장. 데이터 수치 요약 

    3. 지도학습 
 
             분류 :  knn           <-------------- 산포도 그래프
                    의사결정트리  <-------------- 엔트로피 개념 
                    서포트 백터머신 <------------ 오즈비율 --> 시그모이드 함수
                                                    ---> 로지스틱 회귀 
                    나이브베이즈  <---------------- 확률 (3장. 확률과 확률변수)
              
                    신경망   <--------------------  딥러닝 

             회귀 :  선형회귀 <--------------------  상관계수, 결정계수 
                                                     (5장. 통계적 추정) 

    4. 비지도 학습  :   k-means  <--------------  산포도 그래프 + 거리계산 


    5. 기타 머신러닝 기법 :  부스팅, 배깅  <--------- Cohne's kappa 
                                                       ( 5장. 통계적 추정)

■ 1장. 머신러닝이란 ? 


   ppt 참고 

■ 2장. 머신러닝을 배우기 위해 기본적으로 알고 있어야하는 내용 

 - 통계학 개론 

 1. 데이터의 종류 
 2. 데이터 수치 요약 (평균값,중앙값,최빈값,사분위수,분산,표준편차)
 3. 범주형 데이터 
 4. 그래프 
       - 산포도 그래프
       - 이원 교차표 

 ■1. 데이터의 종류 

  1. 범주형 데이터

      - 명목형(norminal) 데이터 : 몇개의 범주로 나누어진 자료
          예:  high, low, middle

      - 순서형(ordinal)  데이터 : 명목형 데이터 + 순서 
          예:  low,  middle, high 

  2. 수치형 데이터 

     - 이산형 데이터 (discreat : 뚜렷이 구별되다)
            
               주사위처럼 1 ~9 까지의 숫자 

        예:  2016년 음주운전 적발건수 22만 6599건 
             
              계수 ( 헤아려 얻는것) 

     - 연속형 데이터 : 연속적인 값을 데이터 

        예:  신장, 체중( 82.321)    

             계량 ( 측정해서 얻는것 )

    이산형 데이터 보다는 연속형 데이터가 얻을수 있는 정보가 많다 

 * 연속형 데이터에 대한 기술적인 통계를 이용한 자료 요약 3가지 

   1. 데이터의 중심화 경향 : 중앙값, 평균값, 최빈값

   2. 데이터의 퍼짐 정도 :  분산(데이터의 퍼짐정도), 
                            표준편차(평균에 대한 오차) , 
                            범위 

   3. 데이터의 분포와 대칭 정도 : 왜도(좌우로 기울어짐의 정도),
                                  첨도(위아래 뾰족한 정도)

 딥러닝 -->  분류
 통계학 -->  분석 

  두가지 데이터 분석 질문을 던지고 해결하는 방법으로 위의
  통계기법들을 사용해볼것임. 

   1. 헬스클럽에 오는 특정 사람의 나이를 가지고 
      가장 적합한 운동 교실을 선택하기 위한 분석 
                 ↓
   사용할 통계기법 ?  평균값, 중앙값, 최빈값

   2. 농구선수 3명의 점수를 가지고 3명중에 가장 적합한 1명을
      선택하기 위한 분석 
                ↓
   사용할 통계기법 ?  분산, 표준편차, 사분위수 그래프 


   ○
  -↑-
  ↙↘


   1. 헬스클럽에 오는 특정 사람의 나이를 가지고 
      가장 적합한 운동 교실을 선택하기 위한 분석 
                 ↓

     사용할 통계기법 ?  평균값, 중앙값, 최빈값

■ 평균값 

            헬스클럽에  찾아와서 자신과 비슷한 나이대에 
            사람들이 있는 운동교실을 알려달라고 했다 

   ○                 
  -↑-                <----- 추천해준 교실의 사람들 나이대
  ↙↘                                ↓
 50대 남자           나이  19   20   21   145   147 
                     도수   3   6    3     1     1 

문제162. 위의 표를 R 로 구현하고 나이의 평균값을 출력하시오 !

class1 <- c( rep(19,3),rep(20,6), rep(21,3), 145, 147)
class1 

table(class1) 

mean(class1) 

  38                            쿵푸 교실 

                     나이  19   20   21   145   147 
                     도수   3   6    3     1     1 
                                          ↑    ↑
                                         고수   고수 
  ※ 이상치때문에 평균이 높아졌다
      
  ※ 이상치 ?   다른 데이터에 비해 눈에 뜨일 정도로 지나치가 
                높거나 낮은값 

  ※ 편향이란 ?   이상치에 의해서 평균값이 상승 되었다.
                  이런 현상을 보이면 데이터가 편향되었다고 한다. 

문제163. 위의 class1 의 이상치를 출력하시오 ! 

 install.packages("outliers")

 library(outliers)

 outlier(class1)

  147

 x2 <-  boxplot(class1)
 x2  

 $out
[1] 145 147

문제164.  데이터 분석을 잘못해서 이런일이 벌어졌다.
          이 문제를 해결하기 위해 알아야하는 값이 무엇인가 ?

   답 : 중앙값(median) 을 구하는것이다. 

        median(class1)
        summary(class1)
                                       
> summary(class1)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
     19      20      20      38      21     147 

   19   19   20   20   20    21   21   100  102   (홀수)
                       ↑ 
                     중앙값

 
     19   19   20   20    21   21   100  102  (짝수) 
                       ↑      
                     가운데 평균값 

 평균값이 갖는 위험은 실제로 데이터 집합에 존재하지 않는 수라는
 것이다. 

 그런데 중앙값은 그 반의 임의 학생을 선택한것이라 다른 학생들도
 20대일 가능성이 높다. 
■ 편향  

 만약 데이터가 오른쪽으로 편향되면
 평균값은 중앙값의 오른쪽(높음)에 위치한다

                                 만약 데이터가 왼쪽으로 편향되면
                                 평균값은 중앙값의 왼쪽(낮음) 에
                                 위치한다. 


문제165. 위의 두개의 그래프를 R 로 직접 그려서 편향 여부를
         확인하시오

class1

 값       1    2   3   4   5   6   7   8      평균값?
 도수     4    6   4   4   3   2   1   1      중앙값?

class2

 값       1    4   6   8  9  10   11   12     평균값?
 도수     1    1   2   3  4   4    5    5     중앙값? 

 class1 <- c(rep(1,4),rep(2,6),rep(3,4),rep(4,4), rep(5,3),
             rep(6,2), rep(7,1), rep(8,1) )

 class2 <- c(rep(1,1), rep(4,1), rep(6,2), rep(8,3), rep(9,4),
             rep(10,4), rep(11,5), rep(12,5) ) 


 summary(class1) 평균값 , 중앙값  ---> 데이터가 오른쪽으로 편향
                  3.44   >  3.00

 summary(class2) 평균값 , 중앙값 ---> 데이터가 왼쪽으로 편향 
                   9.28  <  10

 plot( class1, dnorm(class1, mean=mean(class1), sd=sd(class1)),
       type='l', main="정규분포 그래프")

 plot( class2, dnorm(class1, mean=mean(class2), sd=sd(class2)),
       type='l', main="정규분포 그래프")

■ 왜도와 첨도 (p 99)

 - 왜도 : 데이터의 좌우로 기울어짐의 정도(skeness)

        왜도값 > 0  :  오른쪽 꼬리가 길다
        왜도값 < 0  :  왼쪽 꼬리가 길다 

 - 첨도 :  위아래 뾰족한 정도 (kurtosis)

        첨도값이 3에 가까울 수록 정규분포에 해당하고
                 3보다 작은 경우는 완만한 곡선
                 3보다 크면 뾰족한 곡선 

문제166. class1 과 class2 의 왜도값을 확인하시오 !

 install.packages("fBasics")
 library(fBasics)
 skewness(class1) 

  0.5805801   <---   왜도값이 0보다 크다는 것은 
                    오른쪽으로 꼬리가 긴 그래프이므로 
                    대부분의 데이터들이 작은쪽에 있다.

                     "오른쪽 편향 "
	
         높은 이상치가 평균값을 왜곡할 수 있을텐데 
 
               평균값 >  중앙값 

         이상치가 어디에 있을 가능성이 높은가?  높은쪽에 있다.

  skewness(class2) 

  -1.326201    <--- 왜도값이 0보다 작다는 것은
                    왼쪽으로 꼬리가 긴 그래프이므로 
                    대부분의 데이터들이 큰쪽에 있다.  
                   
                     " 왼쪽 편향" 
 
                 평균값 < 중앙값  

           이상치가 어디에 있을 가능성이 높은가 ? 낮은쪽에 있다

문제167. 우리반 나이 데이터를 가지고 정규분포 그래프를 그리고
          왜도값이 양수인지 음수인지 확인하시오 !

setwd("d://data")
emp8 <- read.csv("emp8.csv", header=T)


plot(emp8$age , dnorm(emp8$age , mean = mean(emp8$age) , sd = sd(emp8$age)), main = '정규분포 그래프')

skewness(emp8$age)

※ 설명 :  이상적인 경우는 데이터가 좌우대칭을 형성하는 것이다.
           데이터가 좌우대칭이면 평균값은 가운데에 위치한다. 
           평균값을 한쪽 방향으로 잡아끄는 이상치가 없으며,
           좌우에 형성되는 차트의 모양이 중앙을 중심으로 했을때
           동일하다.

문제168. (점심시간 문제) 위에서 말하는 좌우 대칭의 정규분포를
         그리시오 !

 첨도 확인하는 방법 ?

  library(fBasics)
  kurtosis(class$age) # 첨도 확인 
  skewness(class$age) # 왜도 확인 

오전에 배운 내용 ?

   평균값, 중앙값,  이상치 , 편차, 왜도, 첨도 

  운동 클래스에 들어오고 싶은 특정 사람의 나이로 
  적합한 교실을 골라주는 데이터 분석 ?

오후에 배울 내용 ?

   최빈값, 범위, 사분위수 범위

■ 최빈값
        
       운동센터에서 

      " 저랑 나이대가 비슷한 10대들이 있는 수영교실에 
        저를 등록시켜주세요 ~ " 
  
   ○           swim_class1  나이   1   2   3   31   32   33
  -↑-                       도수   3   4   2   2    4     3 
  ↙↘
 10대 여학생    swim_class1 을 생성하고 평균값과 중앙값을 구하시오

 swim_class1 <- c(rep(1,3), rep(2,4), rep(3,2), rep(31,2),
                  rep(32,4), rep(33,3) )

 swim_class1

 mean(swim_class1)   #  17
 median(swim_class1) #  17 

 이 여학생이 등록을 해서 가봤더니 수영교실 이름이 
  
  " 엄마와 아기가 함께하는 수영교실 "  이었다.

 평균값과 중앙값에 뭔가 문제가 있다는 것이다.

  1  1  1  2  2  2  2  3  3  □  31 31 32 32 32 32 33 33 33
                             ↑
                          평균값 17
                          중앙값 17

       이 교실의 평균값과 중앙값은 교실세 17세인 사람이 
       없음에도 불구하고 모두 17이다.  

    학생수가 짝수여서 존재하지 않는값인 17이 나온게 문제다 

    그렇다면 홀수가 되면 중앙값이 어떻게 되는가 ?  

  1  1  1  2  2  2  2  3  3  3   31 31 32 32 32 32 33 33 33


 
                       중앙값 

  
  1  1  1  2  2  2  2  3  3   31  31 31 32 32 32 32 33 33 33

 그래서 필요한  값이  "최빈값" 이다.

■ 최빈값(mode)  이란 ?  평균과 중앙값 이외의 세번째 종류의 
                         평균이 존재하는데 그게 바로 최빈값이다.

       최빈값이란 가장 흔하게 나타나는 값을 말한다. 

       아래의 나온 2개의 데이터가 이 데이터를 대표할 수 있는 값
       이다. 
       
   ○           swim_class1  나이   1   2   3   31   32   33
  -↑-                       도수   3   4   2   2    4     3 
                                        ↑           ↑

문제169. swim_class1 데이터에서 최빈값 출력하시오 !

답 :    table(swim_class1)

        which.max( table(swim_class1) ) 
          2
          2 
            
        names(table(swim_class1))[2]

    "최빈값은 반드시 데이터 안에 존재하는 값이다" 

     평균값도 그렇고 중앙값도 그렇게 데이터 안에 반드시 있는 값이
     나오는게 아니다.

■ 범위

 "평균값과 중앙값과 최빈값만으로는 데이터 분석을 하기 부족한
  경우가 있다. 이런 평균 데이터는 중심이 어디즘인지는 알려주지만
  데이터가 어떤식으로 변화하는지에 대해서는 알려주지 않는다.
  특정 데이터가 평균을 중심으로 어떻게 분포되어있는지 알려면
  범위를 알아야 한다 "

 질문 :  어느 농구단의 감독이 아래의 3명의 농구선수중 한명을
         선택해야 할려고 데이터 분석을 하려고 한다.
         아래의 3명의 선수의 게임장 점수를 가지고 
         평균값, 중앙값, 최빈값을 각각 구하시오 !  

 
x1 <-     c(7	,8	,9	,9	,10	,10	,11	,11	,12	,13)

x2 <- c (7	,9	,9	,10	,10	,10	,10	,11	,11	,13 )

x3 <- c(3	,3	,6	,7	,7	,10	,10	,10	,11	,13	,30 )

 총 10번의 시합에서 올린 점수 

x1
 7  8  9 10 11 12 13     <-- 각각의 점수를 받았던 게임수 
 1  1  2  2  2  1  1     
      ↑       ↑
 x1 선수는 2번의 시합에서 9점을 올렸고, 1번의 시합에 12점을 올렸다

x2
 7  9 10 11 13 
 1  2  4  2  1 

x3
 3  6  7 10 11 13 30 
 2  1  2  3  1  1  1 

mean(x1)   #10
mean(x2)   #10
mean(x3)   #10

median(x1)  # 10
median(x2)  # 10
median(x3)  # 10 

which.max(table(x1)) # 2번의 시합에서 10점
which.max(table(x2)) # 4번의 시합에서 10점
which.max(table(x3)) # 3번의 시합에서 10점

 중앙값,평균값, 최빈값이 다 동일해서 특정 선수를 선택하기가
 어렵다. 그래서 감독이 각 선수의 점수가 어떻게 분포가 되어
 있는지 분포 방식을 측정할 수 있으면 결정을 내리는데
 도움을 줄 수 있을것이다.

range(x1) # 7  13
range(x2) # 7  13

range(x3) # 3  30 

 범위는 그 자체로는 데이터의 폭만을 설명할뿐 그 안에서
 데이타가 분포되는 방식은 설명해주 않는다. 

 특히 이상치에 민감하다. 
  
 3번째 선수 같은 경우 만약 어쩌다 한번 잘한 게임(30점)인
 이상치 때문에 범위가 넓어져 버리게 되면 분석하기 어렵다.

 그래서 이상치로 부터 멀어질 필요가 있다.



  사분위수 범위로 해결할 수 있다.


 



            사분범위 = 상한 사분위수 - 하한 사분위수 

            이 값들 사이에 존재하는 범위가 미니범위 


문제170. 아래의 데이터에서 하한 사분위수, 중앙값, 상한 사분위수
         가 무엇인지 선택하시오 !

     3   3    6    7    7   10   10   10   11   13    30  
 
 x <- c( 3,3,6,7,7,10,10,10,11,13,30 )
 
 x2 <- boxplot(x)
 
 x2

     [,1]
[1,]  3.0  <-- 하한값
[2,]  6.5  <-- 하한 사분위수 
[3,] 10.0  <-- 중앙값
[4,] 10.5  <-- 상한 사분위수 
[5,] 13.0  <-- 상한값

문제171. 다음 데이터에서 하한값, 하한 사분위수, 중앙값, 
         상한 사분위수, 상한값을 출력하시오 !  

  1 1 1 2 2 □  2 2 3 3 3 □ 3 3 4 4 4 □ 4 5 5 5 10
 ↑       ↑            ↑           ↑        ↑ 
 하한  하한사분위수  중앙값        상한       상한 
                                 사분위수 

a <- c( 1, 1 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4 ,4 ,4,4, 5, 5, 5, 10)

▦ 이런 값들을 구하는 이유가 무엇인가 ?

   답 :  이상치 때문이다. 이상치를 제거하고 가운데 50% 의
         데이터에만 집중함으로써 문제를 우회할 수 있는것이다. 

문제172. 아래의 3명의 농구선수들의 점수를 가지고 사분위수 그래프를
         그리는데 2번 선수와 3번 선수 두명의 그래프를 하나의
         그래프로 출력하시오 !

x1 <-     c(7	,8	,9	,9	,10	,10	,11	,11	,12	,13)

x2 <- c (7	,9	,9	,10	,10	,10	,10	,11	,11	,13,13 )

x3 <- c(3	,3	,6	,7	,7	,10	,10	,10	,11	,13	,30 )

cbind(x2,x3)
 [1,]  7  3
 [2,]  9  3
 [3,]  9  6
 [4,] 10  7
 [5,] 10  7
 [6,] 10 10
 [7,] 10 10
 [8,] 11 10
 [9,] 11 11
[10,] 13 13
[11,] 13 30

 zz <- cbind(x3,x2)

 boxplot(zz, horizontal = TRUE) 



※ 설명 : 2번 선수가 3번 선수보다 상대적으로 좁은 범위를 가지고
          있다.

          3번 선수는 넓은 범위를 가지고 있고 이 선수는 2번 선수에
          비해 훨씬 높은 점수로 득점을 했지만 다른 경우에는
          훨씬 낮은 점수를 기록 했다.

          2번선수가 더 일관성이 있고, 대부분의 경우에 3번 선수
          보다 더 높은 점수를 기록을 했다.

          따라서 2번 선수를 고를 것이다. 

■ 분산과 표준편차  

  사분위수 범위(미니범위) 가 유용한것 처럼 보이지만 
  때때로 정말 낮은 점수를 올리는 선수가 있다면 어떻게 할까 ?
  
  어떤 선수가 시합날 분위기를 망치면 우리는 리그를 포기해야하나?

  범위 혹은 사분범위가 가장 일관성 있는 선수가 누구인지를
   확실히 측정할 수 있을까 ? 

  저 정확하게 측정할 수는 없을까 ?  

  다시 말해 선수가 올린 점수의 변이를 측정하고 싶은것이다.

  그 방법중에 하나가 각각의 값들이 평균값으로 부터 
  얼마나 떨어져 있는지를 확인하는 것이다. 

  만약 값들이 평균값들로 부터 많이 떨어져 있다면 시합당일에
  어떤 모습을 보여줄지 예상하기가 어렵다.

  그러나 값들이 평균값에 가까우면 시합날 어떤 모습일지
  예측할 수 있다.

  그게 무엇인가  ?         "분산"  이다. 


  그런데 우리가 정말 필요한것은 평균값으로 부터의 거리를 제곱한
  것이 아니라 그냥 거리 자체가 분포되 있는 양상을 알려주는 수이다.

  그게 무엇인가  ?       "표준편차"  이다.  

   표준편차(σ) =  √분산 

   표준편차(σ)^2 = 분산 

 즉 정리하면 평균은 데이터의 중심에 어느 값이 있는지 설명해주지만
 그것만으로는 부족하다. 그래서 표준편차는 데이터가 어떻게 변동
  하는지 말해준다. 

선수1 







선수2

※ 표준점수 혹은 z점수란 ?

   평균값과 표준편차가 서로 다른 데이터 집합을 비교하는 방법이다.
   값 x 에 대한 표준점수를 구하려면 아래의 공식을 사용한다.


          Z = -----------
                  


문제173. x2 선수와 x3 선수의 슈팅 성공율이 둘다 70% 라고 할때
         Z score(표준점수) 를 구하시오 !

x2 <- c (7	,9	,9	,10	,10	,10	,10	,11	,11	,13,13 )

x3 <- c(3	,3	,6	,7	,7	,10	,10	,10	,11	,13	,30 )

답:
> (70-mean(x2))/sd(x2)
[1] 34.37953

> (70-mean(x3))/sd(x3)
[1] 8.149887

  어느날 연습시합을 가졌는데 첫번째 선수는 슛의 75%를 네트에
  넣었고, 두번재 선수는 55% 를 성공시켰다. 
   
  그들의 평상시 기록에 비추어 보았을때 누가 평소보다 더 잘한
   것일까 ?

설명 : 70이 라는것은 %(퍼센트) 가 아니라 점수로 봐야 맞다. 
       기존에 가지고 있는 x2 와 x3 가 점수로 구성 되어 있기 
        때문이다. 

■ 범주형 데이터 살펴 보기 (p 103) 

  범주형 데이터를 살펴보는 방법 2가지 

  1. table 함수
  2. prop.table 함수 

예제:  사원 테이블의 부서번호, 부서번호별 인원수를 출력하시오 !

   table(emp$deptno) 

  10  20   30
   3    5   6 

  prop.table( table(emp$deptno) )   * 100 

     10       20       30 
21.42857 35.71429 42.85714 

 30번 부서번호인 사원들의 42% 로 가장 많은 비율을 차지하고 있구나

문제174. 중고차의 색깔과 색깔별 비율이 어떻게 되는지 출력하시오 !
           ( usedcars.csv)   p 104

 usedcars <- read.csv("usedcars.csv", header=T)

 color_pct <- table( usedcars$color )

 color_pct <- prop.table(color_pct) * 100

 round(color_pct, digit=1)

 Black   Blue   Gold   Gray  Green    Red Silver  White Yellow 
  23.3   11.3    0.7   10.7    3.3   16.7   21.3   10.7    2.0  

설명 :  검정색이 전체 차중에 23%로 가장 많은 비율을 차지하는구나
        그리고 대부분 보수적인 색깔이 주를 이루고 있구나 

■ 산포도 그래프 ( p 106) 

문제175. 커미션을 받는 사원들의 월급의 분포도가 어떻게 되는지
         산포도 그래프로 확인하시오 !

 샤이니로 확인하시오 !

문제176. (마지막 문제)  중고차의 주행거리가 높으면 중고차의 가격이
         낮아진다는것을 plot 그래프로 확인 하시오 !


■ 이원 교차표 p108

문제177.  직업(가로), 부서번호(세로), 직업별 부서번호별
          인원수를 출력하시오 !   (tapply 함수 사용)

     ANALYST CLERK MANAGER PRESIDENT SALESMAN
10       0     1       1         1        0
20       2     2       1         0        0
30       0     1       1         0        4

tapply( emp$empno, list(emp$deptno, emp$job), length, default=0)

* 이원 교차표란 ?  

  두 명목 변수간의 관계를 관찰하기 위해 이원 교차표를 사용한다

문제178. 위의 결과를 이원 교차표를 출력하는 CrossTable 함수를
         이용해서 위의 결과를 출력하시오 ! 

install.packages("gmodels")
library(gmodels)
CrossTable(x=emp$deptno, y=emp$job) 

문제179. 직업별로 월급의 차이가 존재하는지 이원교차표로 
         확인하시오

         월급 2500 을 기준으로 직업별로 각각 월급이 
         2500 이상인 사원과 2500 보다 적은 사원들의
         어떻게 분포가 되어있는지 확인하시오 !


     emp$job |     FALSE |      TRUE  | Row Total | 
-------------|-----------|-----------|-----------|
     ANALYST |         0 |         2 |         2 | 
             |     1.286 |     2.314 |           | 
             |     0.000 |     1.000 |     0.143 | 
             |     0.000 |     0.400 |           | 
             |     0.000 |     0.143 |           | 
-------------|-----------|-----------|-----------|
       CLERK |         4 |         0 |         4 | 
             |     0.794 |     1.429 |           | 
             |     1.000 |     0.000 |     0.286 | 
             |     0.444 |     0.000 |           | 
             |     0.286 |     0.000 |           | 
-------------|-----------|-----------|-----------|
     MANAGER |         1 |         2 |         3 | 
             |     0.447 |     0.805 |           | 
             |     0.333 |     0.667 |     0.214 | 
             |     0.111 |     0.400 |           | 
             |     0.071 |     0.143 |           | 
-------------|-----------|-----------|-----------|
   PRESIDENT |         0 |         1 |         1 | 
             |     0.643 |     1.157 |           | 
             |     0.000 |     1.000 |     0.071 | 
             |     0.000 |     0.200 |           | 
             |     0.000 |     0.071 |           | 
-------------|-----------|-----------|-----------|
    SALESMAN |         4 |         0 |         4 | 
             |     0.794 |     1.429 |           | 
             |     1.000 |     0.000 |     0.286 | 
             |     0.444 |     0.000 |           | 
             |     0.286 |     0.000 |           | 
-------------|-----------|-----------|-----------|
Column Total |         9 |         5 |        14 | 
             |     0.643 |     0.357 |           | 
-------------|-----------|-----------|-----------|

답:  library(data.table)
     data.table(emp$sal, emp$sal >= 2500) 

      V1    V2
 1: 5000  TRUE
 2: 2850  TRUE
 3: 2450 FALSE
 4: 2975  TRUE
 5: 1250 FALSE
 6: 1600 FALSE
 7: 1500 FALSE
 8:  950 FALSE
 9: 1250 FALSE
10: 3000  TRUE
11:  800 FALSE
12: 3000  TRUE
13: 1100 FALSE
14: 1300 FALSE

  CrossTable( emp$job, emp$sal>= 2500)  

■ 머신러닝에서 이원교차표를 어떻게 활용하는가 ?

   학습시킨 모델의 정확도를 확인하고 분석하기 위해서 사용한다.


 P -> 암     ,  T -> 맞춘것이고
 N -> 정상   ,  F -> 틀린것이다.

■ R 에서의 함수 생성 방법

 * 함수생성하는 문법

 함수명 <-  function(인수 또는 입력값) {
                 
                    계산 처리1
                    계산 처리2
                    ..............

               return  (계산 결과 반환)

                                       } 
예제 :  normalize <-  function(x) {

          return   ((x - min(x) ) / ( max(x) - min(x) ))

                                   }
    키, 몸무게, ......, 체질량 지수
   177    78              비만
                          복비
                          정상 

> normalize( c(55,64,77,81,90) )  # 몸무게

[1] 0.00 0.25 0.50 0.75 1.00

> normalize( c(165,172,177,181,186) )  # 키

[1] 0.00 0.25 0.50 0.75 1.00

문제180. 통계 표준화  함수 생성를 생성하시오 !

                        x   -  μ   
      표준화 (Z) = ---------------------
                         표준편차

 평균이 0 이고 표준편차가 1인 정규분포의 데이터로 변환 

> z_standard( c(55,64,77,81,90) )  # 몸무게

> scale(c(55,64,77,81,90))

> z_standard( c(165,172,177,181,186) )  # 키

문제181. knn 알고리즘 이해하기 위해 아래의 거리를 구하는
         함수를 생성하시오 !

  knn 알고리즘이 data 와 data 사이의 거리를 구하는 알고리즘 
  거리를 구해서 가장 인접한 데이터가 나와 유사한 데이터이다.

 distance(a,b)  


a=c(0,3,2)
b=c(2,0,0)


distance<-function(a,b){
 
           return  ( sqrt(sum((a-b)^2)) )
 
                      }

distance(a,b)
 
■ R 에서의 for  loop 문 

 문법 예제 :

          for  ( 루프변수  in  리스트 )  {
 
                         반복할 문장 
 
                                         }

 예제 :     aaa <-  function(x) {
 
                        for  ( i  in  1:x) {
 
                              print(i)

                                           }
                                }

            aaa(10) 

문제182. 아래의 파이썬 코드를 R 로 코드로 변환하시오 !

import  csv

file = open("d:\\emp2.csv",'r')
emp_csv = csv.reader(file)

a = []

for  emp_list in emp_csv:
    a.append(emp_list[5])

print(a) 
  
[5000, 2850, 2450, 2975, 1250, 1600, 1500, ...] 

답:

> x <- c() 

> for (i in 1:length(emp$sal))
   x[i] <- emp$sal[i]

> x   

문제183. 아래의 data frame 을 생성하시오 !

  그림 


a <- data.frame('데이터'=c('A','B','C','D','E','F'),
                'x좌표'=c(1,2,4,5,6,7),
                'y좌표'=c(5,6,5,2,3,1),
                '그룹'=c(rep('A',3),rep('P',3)))
a

문제184. 아래의 그림에서 N 과 가장 거리가 가까운 과일은
         무엇인가 ?

   점심시간 문제 카페에 R 수업 게시판에 올리세요

-- data frame 에서 한행씩 가져오기 

for(i in 1:nrow(dataFrame)) {
    row <- dataFrame[i,]
    # do stuff with row
}

답 :

fruits <- data.frame('데이터'=c('A','B','C','D','E','F'),
 'x좌표'=c(1,2,4,5,6,7),
 'y좌표'=c(5,6,5,2,3,1),
 '그룹'=c(rep('A',3),rep('P',3)), stringsAsFactors = FALSE)

distance<-function(a,b){
 return ( sqrt( sum( (a-b)^2) ) ) 
}

temp <- c()
for ( i in 1 : length(fruits$x좌표) ) {
temp <- append( temp, distance( c ( fruits$x좌표[i],fruits$y좌표[i] ) , c ( 4, 4 ) ) )
 }
fruits$그룹[which(temp==min(temp))]


■ R 에서의 if 문

if 문 예제 :

   if  (조건식) {

             조건식이 true 일때 실행되는 식 

                }
      else  if  (조건식)  {

             조건식이 true 일때 실행되는 식 

                           }
      else      {

             위에 조건식들에 만족하지 않는 경우 실행되는 식 
                 }
               
문제185. N 과 거리가 가까운 과일을 3개를 출력하시오 ! 

     순위 1위, 2위,3위   

 
fruits <- data.frame('데이터'=c('A','B','C','D','E','F'),
 'x좌표'=c(1,2,4,5,6,7),
 'y좌표'=c(5,6,5,2,3,1),
 '그룹'=c(rep('A',3),rep('P',3)), stringsAsFactors = FALSE)

distance<-function(a,b){
 return ( sqrt( sum( (a-b)^2) ) ) 
}

temp <- c()
for ( i in 1 : length(fruits$x좌표) ) {
temp <- append( temp, distance( c ( fruits$x좌표[i],fruits$y좌표[i] ) , c ( 4, 4 ) ) )
 }

fruits$그룹[which(rank(temp) <= 3)]

 "A" "P" "P"

문제186. 위에 요소 A P P 중에 가장 많은 요소인 P 를 출력하시오 !

힌트 :

result <- x[x$순위<=3,c('데이터')]

xx <- table(result)
names(xx[max(xx)])



 A A P   --->  A
 A P P   --->  P 

답:

fruits <- data.frame('데이터'=c('A','B','C','D','E','F'),
 'x좌표'=c(1,2,4,5,6,7),
 'y좌표'=c(5,6,5,2,3,1),
 '그룹'=c(rep('A',3),rep('P',3)), stringsAsFactors = FALSE)

distance<-function(a,b){
 return ( sqrt( sum( (a-b)^2) ) ) 
}

temp <- c()
for ( i in 1 : length(fruits$x좌표) ) {
temp <- append( temp, distance( c ( fruits$x좌표[i],fruits$y좌표[i] ) , c ( 4, 4 ) ) )
 }

result <-fruits$그룹[which(rank(temp) <= 3)]

xx <- table(result)
names(xx[max(xx)])

■ 3장. knn 

 knn 알고리즘은 무엇인가 ?

  "k nearest neighbor  의 약자로 머신러닝의 지도학습에
   분류에 해당하는 알고리즘이다. "

  새로 들어온 데이터가 기존 데이터의 그룹에 어느 그룹에
  속하는지를 찾을때 거리가 가까운 데이터의 그룹을 자기 그룹으로
  선택하는 아주 간단한 알고리즘이다.  

■ knn 알고리즘의 장단점 

 - 장점 :  단순하고 효율적이다.
           훈련 단계가 빠르다.

 - 단점 :  모델을 생성하지 않아 특징과 클래스간의
           관계를 이해하는 능력이 제약된다.

           적절한 k 값을 사용자가 직접 알아내야 한다.

   예: wbcd_test_pred <- knn( train= wbcd_train, test=wbcd_test,
                              c1=wbcd_train_labels, k = 21 )

             k값         테스트 데이터의 정확도 
             1                 75%
             2                 79%
             :                  :


           명목형 데이터와 결측치에 대한 추가 처리작업이 
           필요하다.  (더미코딩이 필요할 수 있다) 

 ■  knn 으로 분류하기전에 전처리해야하는 사항

 1. 명목형 데이터는 더미 코딩해서 숫자 0 또는 1로 변환한다.

 2. 변수간에 서로 단위가 다른 데이터이므로 

     min/max 정규화 또는 Z 표준화를 해야한다.

     - min/max 함수는 데이터를 0~1사이의 숫자로 변환(정규화)

     - z 표준화는 평균 0 이고 표준편차가 1인 데이터의 분포로
       구성하는 작업 (표준화)

 3. 데이터를 3가지로 나눈다 

  전체 data 가 10000 개

         1. train data  (6000개)

         2. vaildation  data(2000개)  --> 훈련 데이터의 일부를 
                                     모델을 학습시키기 위해 쓰는
                                     데이터 

         3. test  data (2000개)  -->  실전 데이터 

  4. 라벨의 class 가  양성과 악성 두가지면 두개의 비율이
     비슷해야한다. 즉 양성 50%, 악성 50% 로 데이터의 갯수를
     맞춰주고 학습 시켜야한다.  (SMOTE 를 활용한다) 

 ※ knn 은 거리를 계산하기 때문에 반드시 정규화 또는 표준화 작업
    을 수행해야 한다. 

문제187. 유방암 데이터의 건수와 컬럼의 갯수를 확인하시오 !

 wbcd <- read.csdv("wisc_bc_data.csv", header=T)

 nrow(wbcd)  # 전체건수 확인
 ncol(wbcd)  # 컬럼 갯수 확인 
 
문제188. 유방암 데이터의 양성(B) 과 악성(M) 의 건수가
         각각 어떻게되는지 확인하시오 ! 

 wbcd <- read.csdv("wisc_bc_data.csv", header=T)

 table(wbcd$diagnosis)

  B   M 
357 212 

문제189. 유방암 데이터의 양성(B)  과 악성(M) 의 건수의 
         비율이 어떻게 되는지 확인하시오 ! 

prop.table( table(wbcd$diagnosis) )  * 100 


    유방암 데이터 : wisc_bc_data.csv  


■ 유방암 환자 데이터로 Knn 모델 테스트 하기 


■ Knn 의 분류 실습 ( 유방암 데이터 악성과 양성 분류)

1.데이터 게시판에서 유방암 데이터를 내려받는다

wisc_bc_data.cv 

wbcd <- read.csv("wisc_bc_data.csv", 
                   stringsAsFactors=FALSE)

str(wbcd)

라벨 : B --> 양성,  M --> 악성

table(wbcd$diagnosis)
  B   M 
357 212 

 총 : 569 명중 1/3 이 악성이고 2/3 가 양성이다.

2. 라벨 컬럼을 팩터로 변환 

wbcd$diagnosis <- factor(wbcd$diagnosis, 
                      levels = c("B","M"),
                  labels=c("Benign","Maliganant") )

str(wbcd)

3. 양성과 악성의 비율이 어떻게 되는지 확인한다.

round( prop.table( table(wbcd$diagnosis) ), 1 ) * 100

4. 정규화 작업 (normalize 함수로 작업)

normalize <- function(x) {
      return ( (x-min(x)) / (max(x) - min(x))  )
                          }

wbcd_n <- as.data.frame(lapply(wbcd[,3:32], normalize) ) 
wbcd_n


* 정규화가 잘 되었는지 확인 ( 0 ~ 1 사이에 있는지)

summary(wbcd_n)

5. 훈련 데이터와 테스트(실험) 데이터를 나누는 작업 
   ( 4/5 )        ( 1/5 )

  " 훈련 데이터로 기계학습 시켜서 모델을 생성한후
    실험 데이터로 검증하는 작업 "

nrow(wbcd_n)
569

wbcd_train <- wbcd_n[1:469, ]
wbcd_test  <- wbcd_n[470:569, ]

wbcd_train_label <- wbcd[1:469,2]
wbcd_test_label  <- wbcd[470:569,2]

wbcd_test_label

6. knn 알고리즘으로 기계학습 시켜서 모델을 생성한다.

install.packages("class") 

library(class)

훈련데이터의 갯수 469 의 제곱근  21 

result1 <- knn(train=wbcd_train, test=wbcd_test,
               cl= wbcd_train_label, k = 21 )

result1

data.frame(wbcd[470:569,2], result1)

7. 이 모델의 정확도가 몇 % 인지 알아내시오 ! 

prop.table( table(ifelse(wbcd[470:569,2]==result1,"o","x" )))

   o    x 
0.98 0.02 

8. 이원 교차표를 이용해서 모델을 분석하시오 !

library(gmodels)

CrossTable(x= wbcd[470:569,2]  , y = result1,
           prop.chisq=FALSE )
	
Total Observations in Table:  100 

 
                 | result1 
wbcd[470:569, 2] |     Benign | Maliganant |  Row Total | 
-----------------|------------|------------|------------|
          Benign |         61 |          0 |         61 | 
                 |      1.000 |      0.000 |      0.610 | 
                 |      0.968 |      0.000 |            | 
                 |      0.610 |      0.000 |            | 
-----------------|------------|------------|------------|
      Maliganant |          2 |         37 |         39 | 
                 |      0.051 |      0.949 |      0.390 | 
                 |      0.032 |      1.000 |            | 
                 |      0.020 |      0.370 |            | 
-----------------|------------|------------|------------|
    Column Total |         63 |         37 |        100 | 
                 |      0.630 |      0.370 |            | 
-----------------|------------|------------|------------|


k = 1  , 정확도 : 94%  
 









 


■ 요번주 일정 

    월                 화               수         목        
  3장 .knn
  4장.나이브베이즈     5장.결정트리  6장.회귀    7장.신경망


  8장 ~12장 -> 다음주 일정 


■ knn  복습  

 - knn 이란 ?      k 개의 근접한 이웃이 내 이웃이다

              라는 개념으로 거리계산으로 가장 가까운 거리에
              있는 이웃을 나의 이웃으로 분류하는 머신러닝의 종류 

 - knn 의 특징 ?
                  1. 훈련 단계가 없다. 바로 예측이다. 

                  2. 거리를 계산하기때문에 반드시 
                     변수들의 데이터를 min/max 정규화를 해야한다.

                  3. 적절한 k 값을 사용자 직접 알아내야한다.

set.seed(2)  

knn 함수 안에 동점처리가 되었을때 랜덤으로 둘중에 하나를 
선택하는 코드가 있었을것 이다. 

■ 유방암 데이터로 knn 알고리즘 돌려서 악성인지 양성인지 테스트

  
   정확도 : 98%   

  데이터를 표준화 또는 정규화는 작업

    1. min/max 정규화 ---> 정확도 98%

P -->  Target 군 (관심범주)  ---> 암 
N -->  대조군    (관심범주가 아닌군) --> 정상 

                 | result1 
wbcd[470:569, 2] |     Benign | Maliganant |  Row Total | 
-----------------|------------|------------|------------|
          Benign |         61 |          0 |         61 | 
                 |      1.000 |      0.000 |      0.610 | 
                 |      0.968 |      0.000 |            | 
                 |      0.610 |      0.000 |            | 
-----------------|------------|------------|------------|
      Maliganant |          2 |         37 |         39 | 
                 |      0.051 |      0.949 |      0.390 | 
                 |      0.032 |      1.000 |            | 
                 |      0.020 |      0.370 |            | 
-----------------|------------|------------|------------|
    Column Total |         63 |         37 |        100 | 
                 |      0.630 |      0.370 |            | 
-----------------|------------|------------|------------|



    2. Z - 표준화 ( 평균 0, 표준편차 1) --> 정확도 ?          

문제190. 문제189번에서는 min/max 정규화를 해서 정확도를 확인했
         는데 이번에는 z-표준화를 해서 정확도를 확인하시오!
         ( 이원 교차표를 반드시 확인하시오 )


■ Knn 의 분류 실습 ( 유방암 데이터 악성과 양성 분류)

1.데이터 게시판에서 유방암 데이터를 내려받는다

wisc_bc_data.cv 

wbcd <- read.csv("wisc_bc_data.csv", 
                   stringsAsFactors=FALSE)

str(wbcd)

라벨 : B --> 양성,  M --> 악성

table(wbcd$diagnosis)
  B   M 
357 212 

 총 : 569 명중 1/3 이 악성이고 2/3 가 양성이다.

2. 라벨 컬럼을 팩터로 변환 

wbcd$diagnosis <- factor(wbcd$diagnosis, 
                      levels = c("B","M"),
                  labels=c("Benign","Maliganant") )

str(wbcd)

3. 양성과 악성의 비율이 어떻게 되는지 확인한다.

round( prop.table( table(wbcd$diagnosis) ), 1 ) * 100

4. 정규화 작업 (scale 함수로 작업)

wbcd <- read.csv("wisc_bc_data.csv", 
                   stringsAsFactors=FALSE)

wbcd_n <- wbcd[,-1]  # 환자번호(id) 를 제외시킨다.

str(wbcd_n)

# diagnosis 라벨을 제외하고 scale 한다.
wbcd_z <- as.data.frame( scale(wbcd_n[-1]) )  

summary(wbcd_z)


* 정규화가 잘 되었는지 확인 ( 0 ~ 1 사이에 있는지)

summary(wbcd_z)

5. 훈련 데이터와 테스트(실험) 데이터를 나누는 작업 
   ( 4/5 )        ( 1/5 )

  " 훈련 데이터로 기계학습 시켜서 모델을 생성한후
    실험 데이터로 검증하는 작업 "

nrow(wbcd_z)
569

wbcd_train <- wbcd_z[1:469, ]
wbcd_test  <- wbcd_z[470:569, ]

wbcd_train_label <- wbcd_n[1:469,1]
wbcd_test_label  <- wbcd_n[470:569,1]

wbcd_test_label

6. knn 알고리즘으로 기계학습 시켜서 모델을 생성한다.

install.packages("class") 

library(class)

훈련데이터의 갯수 469 의 제곱근  21 

result1 <- knn(train=wbcd_train, test=wbcd_test,
               cl= wbcd_train_label, k = 21 )

result1

data.frame(wbcd[470:569,2], result1)

7. 이 모델의 정확도가 몇 % 인지 알아내시오 ! 

prop.table( table(ifelse(wbcd[470:569,2]==result1,"o","x" )))

   o    x 
0.98 0.02 

8. 이원 교차표를 이용해서 모델을 분석하시오 !

library(gmodels)

CrossTable(x= wbcd[470:569,2]  , y = result1,
           prop.chisq=FALSE )

 
                 | result1 
wbcd[470:569, 2] |         B |         M | Row Total | 
-----------------|-----------|-----------|-----------|
               B |        61 |         0 |        61 | 
                 |     1.000 |     0.000 |     0.610 | 
                 |     0.924 |     0.000 |           | 
                 |     0.610 |     0.000 |           | 
-----------------|-----------|-----------|-----------|
               M |         5 |        34 |        39 | 
                 |     0.128 |     0.872 |     0.390 | 
                 |     0.076 |     1.000 |           | 
                 |     0.050 |     0.340 |           | 
-----------------|-----------|-----------|-----------|
    Column Total |        66 |        34 |       100 | 
                 |     0.660 |     0.340 |           | 
-----------------|-----------|-----------|-----------|

■  현업에서 데이터 분석할때 필요한 사항

  1. 오라클 데이터베이스에 있는 데이터를 csv 로 내리지 않고
     바로 오라클과 R 을 연동한다든지 오라클과 파이썬을 연동해서
     바로 분석할 수 있게 하는게 중요하다.

  2. 현업 담당자들이 바로 분석할 수 있도록 UI 를 만들어
     UI 를 배포 ( 샤이니 기술 요구 )

문제191. knn 알고리즘을 R 샤이니에 구현하시오 !

 1. 지난시간까지 완성했던 샤이니 코드 : 그래프 + 테이블 포멧


샤이니 그래프와 테이블까지 완성한 코드_20190225.txt

 2. 머신러닝 텝을 샤이니에 붙이기 위한 기본 골격 코드 

0. knn 패키지 추가

#Knn
package_in("class")

1. 사이드 메뉴에 아래의 내용 추가 

sidebar <-    dashboardSidebar(


    menuItem("머신러닝",
             menuSubItem('Knn',tabName = 'knn')

                                                )


2. 바디에 아래의 내용 추가 


body <- dashboardBody(
  
    tabItem(tabName = "knn",
            sidebarPanel(
              uiOutput("dependents_delcol_knn"),    컬럼 삭제하는 화면
              uiOutput("dependents_selcol_knn"),    라벨 컬럼 선택하는 화면
              uiOutput("dependents_button_knn")     knn 알고리즘 수행하는
              #uiOutput("check_view_plot_knn"),    버튼 화면 
              
            ),                                 컬럼삭제하면 나머지 컬럼들 표시
            mainPanel(verbatimTextOutput("submit_input_sample_knn")),
            mainPanel(verbatimTextOutput("TestTableRender_knn"),
                      style = "color:red;  font-size:12px;  font-style:italic;
                      overflow-y:scroll;  max-height: 400px;  background: ghostwhite;")
            )
                                이원 교차표 표시  
    )

3. 서버에 아래의 내용 추가 


server <- function(input, output,session) {


  ## knn UI input 

  output$dependents_delcol_knn <- renderUI({
    data <- dataload()
    if (is.null(data)) return(NULL)
    checkboxGroupInput(inputId  = 'in_che_delcol_knn',
                       label    = "delete colmun:",       필요없는 컬럼을
                       choices  = colnames(data),         삭제하는 코드 
                       selected = 'null',
                       inline   = FALSE
    )
  })
  output$dependents_button_knn <- renderUI({
    data <- dataload()                                  knn 모델을 돌리겠금
    if (is.null(data)) return(NULL)                  action 버튼을 누르는 코드 
    actionButton("in_btn_submit_knn","Submit") 
  })
  output$dependents_selcol_knn <- renderUI({
    data <- dataload()                                   라벨이 어떤 컬럼인지
    if (is.null(data)) return(NULL)                      선택하는 코드
    selectInput("in_sel_label_knn","Submit",choices = colnames(data))
  })



#4. 서버에 UI output 에 아래의 내용추가 


  normalize <- function(x) {
    return (( x - min(x)) / (max(x) -min(x))) 
  }	
  
  
  ###############knn show, reactive
  subinput_table_knn <- eventReactive(input$in_btn_submit_knn, {
    req(input$file1)
    file1 = input$file1
    data = read.csv(file1$datapath,stringsAsFactors =FALSE)
    
    data1<-as.data.frame(lapply(data[,-which(colnames(data)==input$in_sel_label_knn)], normalize))
    
    train_index = as.integer(trunc(nrow(data1) *0.8))

    train <- data1[1:as.integer(train_index), ]
    
    test <- data1[as.integer(train_index+1):as.integer(nrow(data1)), ]
    
    train_label <-data[1:as.integer(train_index),which(colnames(data)==input$in_sel_label_knn)] 

    test_label <- data[as.integer(train_index+1):as.integer(nrow(data1)),which(colnames(data)==input$in_sel_label_knn) ]
    
    train_label <- factor(train_label )
    
    #test_label <- factor(test_label)
   
    result <-  knn(train=train , test=test , cl=train_label, k= 21 )
  
    cross_table <- CrossTable(test_label , result, prop.chisq=FALSE )
    
    return(cross_table)
    
  })
  output$TestTableRender_knn <- renderPrint({
    subinput_table_knn()
  })
  output$submit_input_sample_knn <- renderPrint({
    req(input$file1)
    file1 = input$file1
    data <- read.csv(file1$datapath)
    data1 <- data[,!(colnames(data) %in% input$in_che_delcol_knn )]
    return(head(data1,5))
  })
  
  
}

######################### 4. 샤이니 실행 ###############################

shinyApp(ui = ui, server = server)

문제192.  위에 샤이니 코드에는 k 값을 21을 직접 셋팅했지만
          다른 데이터를 넣으면 21이 아니라 달라져야 하므로
          k 값이 책에 나온데로 입력 데이터 전체 건수의 제곱근이
          들어가겠금 코드를 수정하시오 !  (책 121페이지)


문제193. 우리가 지금 만든 knn 샤이니 화면에
         붓꽃 데이터로 knn 분류를 잘 하는지 확인하시오 !


생각해야할 문제 :  K값 출력하기 


■ 4장. 나이브 베이즈 분류  

* 4장 목차 

 1. 확률에 대한 기본적인 이해 
 2. 나이브 베이즈 알고리즘 
 3. 나이브 베이즈 실습 
           - 독버섯과 정상버석의 분류 
           - 영화 장르 선호도 분류
           - 스팸메일과 햄메일의 분류(책 실습) --> text mining 실습 


■1. 확률에 대한 기본적인 이해 

     pdf 로 설명 

■ 2. 나이브 베이즈 알고리즘  (p 152 )


       비아그라(w1)      돈(w2)        식료품(w3)    주소삭제(w4)
 우도    Yes    No      Yes    No        Yes    No    Yes    No   
 스팸    4/20  16/20   10/20  10/20   0/20  20/20   12/20  8/20     20 
 햄      1/80  79/80   14/80  66/80   8/80  71/80   23/80  57/80    80  
 총합  5/100  95/100  24/100 76/100  8/100  9/100  35/100  65/100  100 

* 스팸 메일과 햄 메일을 정확하게 분류하기 위해서는 ?

  비아그라 단어 하나만 가지고 스팸 메일인지를 분류하면 
  정확하게 분류가 안될 수 있으니 다른 단어들도 같이 포함시켜서
  확률을 구해야한다.

예:  비아그라, 돈, 식료품, 주소삭제 

용어 :  ㄱ : 존재하지 않는다(부정)
        ㅋ : 존재한다. (긍정)

예제: 비아그라와 주소삭제라는 단어는 포함되어있는 메일인데
      돈 과 식료품은 포함하지 않는 메일은 스팸일 확률이 어떻게 되는가 ?

  P( 스팸 |  비아그라 ∩ ㄱ돈 ∩ ㄱ식료품 ∩ 주소삭제 )  =   ?  

               P(B|A) * P(A)
  P(A|B) = ---------------------------
                    P(B)
  
      P(비아그라 ∩ ㄱ돈 ∩ ㄱ식료품 ∩ 주소삭제 |스팸) * P(스팸) 
 =  -------------------------------------------------------------
     P( 비아그라 ∩ ㄱ돈 ∩ ㄱ식료품 ∩ 주소삭제)


  P(비아그라|스팸) * P(ㄱ돈|스팸) * P(ㄱ식료품|스팸) * P(주소삭제|스팸) * P(스팸) 
 =    --------------------------------------------------------------------------------
               P( 비아그라 ∩ ㄱ돈 ∩ ㄱ식료품 ∩ 주소삭제)


       비아그라(w1)      돈(w2)        식료품(w3)    주소삭제(w4)
 우도    Yes    No      Yes    No        Yes    No    Yes    No   
 스팸    4/20  16/20   10/20  10/20   0/20  20/20   12/20  8/20     20 
 햄      1/80  79/80   14/80  66/80   8/80  71/80   23/80  57/80    80  
 총합  5/100  95/100  24/100 76/100  8/100  9/100  35/100  65/100  100

* 스팸일 확률 ? 스팸일 우도 ?

 P(비아그라|스팸) * P(ㄱ돈|스팸) * P(ㄱ식료품|스팸) * P(주소삭제|스팸) * P(스팸)
       4/20            10/20              20/20             12/20          20/100

 = 0.012 

* 햄일 확률 ? 햄일 우도 ? 


 P(비아그라|햄) * P(ㄱ돈|햄) * P(ㄱ식료품|햄) * P(주소삭제|햄) * P(햄 )
     1/80            66/80         71/80            23/80         80/100
 
 = 0.002    


                        0.012
 스팸일 우도 ?  ------------------------- = 0.85
                    0.012 + 0.002   
 
                     0.002  
 햄일 우도  ?   ------------------------- = 0.15
                   0.012 + 0.002   
 


왜  분모가   P( 비아그라 ∩ ㄱ돈 ∩ ㄱ식료품 ∩ 주소삭제) 가 
            스팸의 우도 + 햄의 우도와 같냐면 ?  

 증명하면 ?

   P( 비아그라 ∩ ㄱ돈 ∩ ㄱ식료품 ∩ 주소삭제) 를 그냥
   
   비아그라 하나면 보고 

   P(비아그라) = P(스팸 ∩ 비아그라) + P(햄 ∩ 비아그라)
               = P(비아 | 스팸) x P(스팸) + P(비아|햄) x P(햄)
                   스팸의 우도               햄의 우도 
     

 정리하면 비아그라, 주소삭제가 포함되어져 있고 돈과 식료품이 포함되지 
 않은 메세지가 스팸일 확률은 85% 가 된다. 

■ 나이브 베이즈 복습 


  1. knn 과 나이브 베이즈의 차이 ?

      knn 은 데이터간의 거리를 계산해서 가장 가까운 거리에 있는 
      데이터가 나의 이웃이라고 분류하는 분류 방법이고

      나이브 베이즈는 확률을 이용하여 분류하는 분류 기법 

  2. 언제 knn 을 사용하고 언제 나이브베이즈를 사용해서 분류해야하는가 ? 

      -  knn -->  분석하려는 데이터가 수치형 데이터일때 
 
          예:  유방암 데이터       

      -  naive bayes -> 분석하려는 데이터가 명목형 데이터 일때 

          예:  영화 선호도 데이터 

  3. knn 과 나이브 베이즈로 분석하려는 질문 리스트 ?

    - knn 의 질문 ?

         1. 종양의 크기와 모양만 보고 악성 종양인지 양성 종양인지를
            분류할 수 있을까 ?
       
         2. 붓꽃의 모양만 보고 붓꽃의 종류를 알아맞힐수 있을까 ?  

    - naive bayes 의 질문 ?

        1. 직업과 나이, 성별, 직업유무를 가지고 어느 영화를 더 
           선호할지 선호도를 알아 맞힐수 있을까 ?

           예: movie.csv 

        2. 버섯의 모양, 색깔, 향기 등의 정보를 가지고 독버섯인지
           일반 버섯인지 알아 맞힐 수 있을까 ?  

           예: mushrooms.csv 

  4. 나이브 베이즈의 원리  복습  

 
        비아그라 
        예    아니오   총계
 스팸   4       16     20
 햄     1       79     80
        5        95   100


  P(스팸 | 비아그라 ∩ 돈 ∩ 식료품 ∩ 주소삭제)  =   확률 ?

            스팸의 우도 
   -----------------------------
     햄의 우도 + 스팸의 우도 

  P(공포 | '30대' ∩ '여자' ∩'IT' ∩ '미혼') =  확률 ?  

        공포의 우도 
  -------------------------------------------------
  공포 + 로멘틱 + 코믹 + 무협 + 스릴러 + 액션 + SF 

문제194. 나이, 성별, 직업, 결혼여부, 이성친구의 여부에 따라서
         선호하는 영화 장르가 어떻게 되는지 예측하는 모델을 생성하시오!

install.packages("e1071")

library(e1071)

movie <- read.csv("movie.csv", header=T)

model <- naiveBayes(movie[ ,1:5], movie$장르, laplace=0)
                       ↑           ↑
                   훈련 데이터   훈련 데이터 라벨 
model 

test_data <- read.csv("n_test1.csv", header=TRUE) 

result <- predict(model, test_data[1:5])


result 

로맨틱 

문제195. 나이가 20대이고 성별이 남자이고 직업이 학생이고 
         결혼 아직 안했고 이성친구가 없는 재혁이가 선호하는 영화는
         무엇이겠는지 나이브 베이즈로 예측 하시오 !



문제196. 독버섯과 정상 버섯을 예측하는 나이브 베이즈 모델을 생성하시오

1. 버섯 데이터를 R 로 로드한다. 

 mushroom <- read.csv("mushrooms.csv", header=T, stringsAsFactors=TRUE)

 View(mushroom)

2. 8124 독버섯 데이터만 따로 빼서 mush_test.csv 로 저장한다. 

 mush_test <- mushroom[8123, ]
 
 mush_test 

 write.csv( mush_test, "mush_test.csv",row.names=FALSE )

3. 8124 독버섯 데이터를 훈련 데이터에서 제외 시키시오 !

 nrow(mushroom)
 mushrooms <- mushroom[ -8123,  ] 
 nrow(mushrooms)

4. mushrooms 데이터를 훈련 데이터와 테스트 데이터로 나눈다 
    ( 훈련 데이터는 75%,  테스트 데이터는 25% )

set.seed(1)
dim(mushrooms)

train_cnt <- round( 0.75*dim(mushrooms)[1] )
train_cnt 

train_index <- sample( 1:dim(mushrooms)[1], train_cnt, replace=F)

mushrooms_train <- mushrooms[ train_index,  ]
mushrooms_test <- mushrooms[- train_index,  ] 

nrow(mushrooms_train)  #  6092
nrow(mushrooms_test)    #  2031 

str(mushrooms_train)

5. 나이브 베이즈 알고리즘으로 독버섯과 일반 버섯을 분류하는 모델을 
   생성한다.

library(e1071)         모든 컬럼들
                          ↓
model1 <- naiveBayes(type~ . ,  data=mushrooms_train)
                      ↑
                   라벨 컬럼명 

model1

6. 위에서 만든 모델과 테스트 데이터를 가지고 독버섯과 일반버섯을 
   잘 분류하는지 예측해 본다.

result1 <- predict( model1, mushrooms_test[  , -1] )

result1 

7. 이원 교차표를 그려서 최종 분류 결과를 확인한다. 

library(gmodels)

CrossTable( mushrooms_test[  ,1], result1) 
                   ↑                ↑
                  실제              예측 


                    | result1 
mushrooms_test[, 1] |    edible | poisonous | Row Total | 
--------------------|-----------|-----------|-----------|
             edible |      1049 |         7 |      1056 | 
                    |   347.908 |   447.814 |           | 
                    |     0.993 |     0.007 |     0.520 | 
                    |     0.918 |     0.008 |           | 
                    |     0.516 |     0.003 |           | 
--------------------|-----------|-----------|-----------|
          poisonous |        94 |       881 |       975 | 
                    |   376.811 |   485.017 |           | 
                    |     0.096 |     0.904 |     0.480 | 
                    |     0.082 |     0.992 |           | 
                    |     0.046 |     0.434 |           | 
--------------------|-----------|-----------|-----------|
       Column Total |      1143 |       888 |      2031 | 
                    |     0.563 |     0.437 |           | 
--------------------|-----------|-----------|-----------|

8. 위의 모델의 성능을 올리시오 !

model2 <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0004)

result2 <- predict( model2, mushrooms_test[ , -1] )

CrossTable( mushrooms_test[ ,1], result2) 

mushrooms_test[, 1] |    edible | poisonous | Row Total | 
--------------------|-----------|-----------|-----------|
             edible |      1050 |         6 |      1056 | 
                    |   459.814 |   496.053 |           | 
                    |     0.994 |     0.006 |     0.520 | 
                    |     0.996 |     0.006 |           | 
                    |     0.517 |     0.003 |           | 
--------------------|-----------|-----------|-----------|
          poisonous |         4 |       971 |       975 | 
                    |   498.014 |   537.264 |           | 
                    |     0.004 |     0.996 |     0.480 | 
                    |     0.004 |     0.994 |           | 
                    |     0.002 |     0.478 |           | 
--------------------|-----------|-----------|-----------|
       Column Total |      1054 |       977 |      2031 | 
                    |     0.519 |     0.481 |           | 
--------------------|-----------|-----------|-----------|

문제197.  위의 모델에  별도로 구분해 놓은 테스트 데이터 한개(독버섯)
          8123 번 데이터를 넣어서 독버섯인지 정상인지 확인하시오 ! 

result3 <- predict( model2, mush_test )


문제198. (점심시간 문제)  set.seed(1) 을 정확히 다시 설정하고
         laplace 값을 0.0001 ~  0.0017 까지 주고 FN 값을 확인하시오 !

   laplace     FN
    0.0001      0
     :         :
     :         :

■ 라플라스 추정기 (P 155 페이지) 


       비아그라(w1)      돈(w2)        식료품(w3)    주소삭제(w4)
 우도    Yes    No      Yes    No        Yes    No    Yes    No   
 스팸    4/20  16/20   10/20  10/20   0/20  20/20   12/20  8/20     20 
 햄      1/80  79/80   14/80  66/80   8/80  71/80   23/80  57/80    80  
 총합  5/100  95/100  24/100 76/100  8/100  9/100  35/100  65/100  100 
    
스팸의 우도 ?  

  P(비아그라|스팸) * P(돈|스팸) * P(식료품|스팸) * P(주소삭제|스팸) * P(스팸)

      4/20             10/20         0/20 
      
 ※ 설명: 식료품으로 인해서 다른 증거들이 다 무효가 되어벼렸다.
          이를 해결하기 위해서 프랑스의 수학자 피에르 시몬 라플라스가
          확률이 0 이 되지 않기 위해서 빈도표의 각 값에 작은 수를 
          추가를 했다.  
     
    4/20       x     10/20    x     0/20    x   12/20  x    20/100 

                               ↓

    5/24       x     11/24    x     1/24    x   13/24  x    20/100 


햄의 우도 ? 

  P(비아그라|햄) * P(돈|햄) * P(식료품|햄) * P(주소삭제|햄) * P(햄)

                       스팸의 우도 
   스팸일 확률 = ----------------------
                  스팸의 우도 + 햄의 우도


문제199. 나이브 베이즈의 알고리즘을 R 샤이니로 구현하시오 !



문제200. knn 과 나이브 베이즈 두개를 R 샤이니에 추가 시키시오 !  

그래프_테이블_KNN까지 구현한 샤이니 코드_수정본_20190225.txt 
      ↑
 위의 코드에 나이브 베이즈 코드를 추가한다.  
  

■ 5장. 의사결정 트리와 규칙 기반 분류 

 의사결정트리란 ?

  의학에서 질병에 대한 진행 과정을 바탕으로 올바른 처방을
  위해 결정해야하는 경우나 은행에서 대출을 해줄때 대출을 
  해줄지 말지의 여부를 기업 데이터를 보고 결정해야 하는 경우등에
  사용하는 지도학습 머신러닝 알고리즘  

■ 엔트로피와 정보 획득량 

 결정트리를 만들때 가장 먼저 해야할 일이 무엇인가 ?
    
               ↓

    중요한 컬럼(변수) 를 찾는 것이다.

               ↓

    정보 획득량이 높은 변수 

               ↓

     엔트로피 함수를 사용 

■ 엔트로피(entropy) 함수

  " 데이터의 불확실성이 얼마나 되는가 ? "  

  엔트로피 지수가 높다는것은 불확실성이 높다는 것 

      그림 


  정보획득량 = 분할전 엔트로피 - 분할후 엔트로피 

문제201. 구매여부 데이터의 변수들 중에 정보획득량이 가장 높은게 
         무엇인지 알아내시오 ! (buy2.csv) 


buy <- read.csv("buy2.csv", header=T)

install.packages("FSelector")
library(FSelector)

weights <-  information.gain(buy_yn~. , buy)

print(weights)

cust_name          0.50040242
card_yn            0.50040242
review_yn          0.22314355
before_buy_yn      0.05053431

문제202. 백화점 화장품 고객 데이터(skin.csv) 를 R 로 로드하고
         skin 데이터셋 변수들의 정보 획득량을 구하시오 !

skin <- read.csv("skin.csv", header=T)
skin <- skin[, -1] 

x <- information.gain( cupon_react ~ ., skin)

x 

gender     0.080610238
age        0.000000000
job        0.013737789
marry      0.224337222
car        0.006023806

문제203. skin 데이터를 의사 결정 트리로 시각화 하시오 !

install.pacakges("rpart")
library(rpart)

tree1 <- rpart( cupon_react ~ ., data=skin[ , -1], 
                 control=rpart.control(minsplit=2) )  

plot( tree1, compress=T, uniform=T, margin=0.1)
text( tree1, use.n = T, col="blue")

문제204. 지방간 환자들 데이터의 정보획득량을 구해서 
         지방간을 일으키는데 가장 영향력이 큰 변수가 무엇인지 알아내시오 !

fatliver <- read.csv("fatliver2.csv", header=T)

library(FSelector)

x <-  information.gain(FATLIVER~. , fatliver )

print(x)

      attr_importance
AGE         0.022358781
GENDER      0.019859086
DRINK       0.008449112
SMOKING     0.006801213

문제205. 심장질환 데이터를 내려받고 심장질환에 있어 가장 영향력이 큰 
         변수가 무엇인지 정보 획득량을 구해서 알아내시오 !


문제206. (오늘의 마지막 문제)  기업이 부도가 나는데 가장 크게 영향을
         미치는 요소가 무엇인지 부도 예측 데이터의 정보 획득량을 구해서
         알아내시오 !  ( 카페에 뎃글로 올리세요 ~)

bankrupt <- read.csv("부도예측데이터3.csv", header=F)

x <- information.gain(V1~., bankrupt) 

 0 : 부도
 1 : 건전 

colnames(bankrupt) <- c("class","매출액","자기자본","총자본투자효율","부가가치율","매출액증가율","재고자산증가율","총자산증가율","금융비용대매출액비율","대출효율성계수","매출액순이익률","매출원가율","손익분기점율","순금융비용대매출액비율","이자보상배율","자기자본순이익률","총자본경상이익률","총자본순이익률","고정장기적합율의역","단기부채대총차입금","당좌비율","매출채권대매입채무","순운전자본비율","유동비율","유동부채대총자본","유보액대총자산비율","자기자본비율","차입금의존도","총차입금대매출액","금융비용부담율증가분","매입채무회전율","순운전자본대매출액","운전자금대매출액","재고자산회전율","총자본회전율","현금흐름지표(1)","현금흐름지표(2)","현금흐름지표(3)","현금흐름지표(4)","현금흐름지표(5)","현금흐름지표(6)","현금흐름지표(7)","현금흐름지표(8)","현금흐름지표(9)" )

■ 의사결정트리 복습 

  머신러닝의 종류  3가지 

             1. 지도 학습 -> 라벨이 있다.

                   - knn  : 데이터간의 거리를 이용해서 분류 
                   - naive bayes : 확률을 이용해서 분류 
                   - decision tree : 정보 획득량으로 분류 

                                         ↓

                            공식? 분할전 엔트로피 - 분할후 엔트로피 

                  정보획득량으로 가장 먼저 질문해야할 변수들을 알아내고
                  분류를 하는것이다.  

             2. 비지도 학습 -> 라벨이 없다.

             3. 강화 학습 -> 환경을 스스로 agent 가 학습해 나가는 방법 

■ C5.0 패키지를 이용해서 분류 모델 생성 

  " 백화점 화장품 고객중에 구매가 예상이 되는 고객이 누구인가 ? "

  " 은행 대출 채무를 불이행할 것 같은 고객이 누구인가 ?  "

             ↓ 

   의사 결정 트리로 분류를 할 것이다. 

■ 백화점 화장품 고객중에 구매가 예상이 되는 고객이 누구인가 ?

 1. 의사결정 패키지인 C50 패키지를 설치한다.

  install.packages("C50")
  library(C50)

 2. 백화점 화장품 고객 데이터를 로드하고 shuffle 한다.

  skin <- read.csv("skin.csv", header=T )
  nrow(skin)
 
  skin_real_test_cust <- skin[30,  ] 
 
  skin2 <-  skin[ 1:29, ] 
 
 nrow(skin2) 

 skin_real_test_cust

cust_no gender age job marry car cupon_react
30      30 female  40 YES   YES  NO         YES

 skin2 <- skin2[ , -1] # 고객번호를 제외시킨다. 

 set.seed(11)

 skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴 

3. 화장품 고객 데이터를 7대 3로 train 과 test 로 나눈다.

 train_num <-  round(0.7 * nrow(skin2_shuffle), 0) 

 skin2_train <- skin2_shuffle[1:train_num,  ]  

 skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ] 

 nrow(skin2_train)  # 20
 nrow(skin2_test)   #  9 

4. C50 패키지를 이용해서 분류 모델을 생성한다. 

 library(C50)
 skin_model <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react )  
                        ↑                        ↑
               라벨을 뺀 train 전체 data    train 데이터의 라벨 

5. 위에서 만든 skin_model 를 이용해서 테스테 데이터의 라벨을 예측하시오!

 skin2_result  <- predict( skin_model , skin2_test[  , -6])
                                              ↑
                                   라벨을 뺀 테스트 데이터 전체 

6. 이원 교차표로 결과를 확인하시오 !

 library(gmodels)
 CrossTable( skin2_test[  , 6],  skin2_result ) 


                | skin2_result 
skin2_test[, 6] |        NO |       YES | Row Total | 
----------------|-----------|-----------|-----------|
             NO |         6 |         0 |         6 | 
                |     0.381 |     1.333 |           | 
                |     1.000 |     0.000 |     0.667 | 
                |     0.857 |     0.000 |           | 
                |     0.667 |     0.000 |           | 
----------------|-----------|-----------|-----------|
            YES |         1 |         2 |         3 | 
                |     0.762 |     2.667 |           | 
                |     0.333 |     0.667 |     0.333 | 
                |     0.143 |     1.000 |           | 
                |     0.111 |     0.222 |           | 
----------------|-----------|-----------|-----------|
   Column Total |         7 |         2 |         9 | 
                |     0.778 |     0.222 |           | 
----------------|-----------|-----------|-----------|

문제207. 위의 의사결정트리의 성능을 높이시오 !

 library(C50)

 skin_model2 <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react,
                     trials = 6  )

 
 skin2_result2  <- predict( skin_model2 , skin2_test[  , -6])
                              

 library(gmodels)

 CrossTable( skin2_test[  , 6],  skin2_result2 ) 
    

                | skin2_result2 
skin2_test[, 6] |        NO |       YES | Row Total | 
----------------|-----------|-----------|-----------|
             NO |         6 |         0 |         6 | 
                |     1.000 |     2.000 |           | 
                |     1.000 |     0.000 |     0.667 | 
                |     1.000 |     0.000 |           | 
                |     0.667 |     0.000 |           | 
----------------|-----------|-----------|-----------|
            YES |         0 |         3 |         3 | 
                |     2.000 |     4.000 |           | 
                |     0.000 |     1.000 |     0.333 | 
                |     0.000 |     1.000 |           | 
                |     0.000 |     0.333 |           | 
----------------|-----------|-----------|-----------|
   Column Total |         6 |         3 |         9 | 
                |     0.667 |     0.333 |           | 
----------------|-----------|-----------|-----------|

 
■ C5.0 패키지에서 trials 가 무엇인가 ?



문제208. 아까 위에서 한건 뺀 고객 데이터 (skin_real_test_cust) 이
         쿠폰 반응이 있는 고객인지 아닌지 잘 맞추는지 확인하시오 !


■ " 은행 대출 채무를 불이행할 것 같은 고객이 누구인가 ?  "

   데이터 :  credit.csv  (독일 은행의 고객 데이터) 


1. 데이터를 로드한다.

credit <- read.csv("credit.csv")
str(credit) 

2. 데이터에 각 컬럼들을 이해한다. 

 라벨 컬럼 :  default  --->  yes : 대출금 상환 안함 
                             no  : 대출금 상환 

 prop.table( table(credit$default)  )

 no   yes 
0.7   0.3 

 -   계좌 소개 :  checking_balance --> 예금계좌
                  saving_balance   --> 적금계좌 

 -  amount    :  대출 금액 250마크르 ~ 18424 마르크 
                 ( 100 마르크가 우리나라돈 6~7만원 )

 
> summary( credit$amount)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    250    1366    2320    3271    3972   18424 

 은행의 목표 :  과거의 데이터를 분석해보니 대출금 상환 불이행자가
                30% 나 되어서 앞으로는 30% 이내로 떨어뜨리겠금 하는게
                은행의 목표라서 거기 맞는 model 을 생성해야한다.

3. 데이터가 명목형 데이터인지 확인해본다.

 str(credit) 

4. 데이터를 shuffle 시킨다.

 set.seed(31)

 credit_shuffle <-  credit[ sample( nrow(credit) ),  ]

5. 데이터를 9 대 1로 나눈다.

 train_num <- round( 0.9 * nrow(credit_shuffle), 0) 

 credit_train <- credit_shuffle[1:train_num ,  ]
 credit_test  <- credit_shuffle[(train_num+1) : nrow(credit_shuffle),  ]

6. C5.0 패키지와 훈련 데이터를 이용해서 모델을 생성한다.

 library(C50)

 credit_model <- C5.0( credit_train[ ,-17] , credit_train[  , 17] )

7. 위에서 만든 모델을 이용해서 테스트 데이터의 라벨을 예측한다.

 credit_result <-  predict( credit_model, credit_test[  , -17] )

8. 이원 교차표로 결과를 확인한다.

 library(gmodels)
 CrossTable(  credit_test[   , 17], credit_result )
                   ↑                     ↑
                  실제                   예측

        | credit_result 
credit_test[, 17] |        no |       yes | Row Total | 
------------------|-----------|-----------|-----------|
               no |        64 |        11 |        75 | 
                  |     0.267 |     1.067 |           | 
                  |     0.853 |     0.147 |     0.750 | 
                  |     0.800 |     0.550 |           | 
                  |     0.640 |     0.110 |           | 
------------------|-----------|-----------|-----------|
              yes |        16 |         9 |        25 | 
                  |     0.800 |     3.200 |           | 
                  |     0.640 |     0.360 |     0.250 | 
                  |     0.200 |     0.450 |           | 
                  |     0.160 |     0.090 |           | 
------------------|-----------|-----------|-----------|
     Column Total |        80 |        20 |       100 | 
                  |     0.800 |     0.200 |           | 
------------------|-----------|-----------|-----------|

 정확도 : 73% 

  trials :  46,  FN: 16 + FT: 11  = 27 

문제209. 부스팅 기법을 이용해서 위의 의사결정트리 모델의 정확도를
         올리시오 !


문제210. 의사 결정 트리 알고리즘을 R 샤이니에 붙이시오 ! 


문제211.  근무하고 있는 사원중에 퇴사가 예상되는 사원이 누구인지
          알아내는 의사결정트리 모델을 생성하시오 ! 
         ( hr.csv 데이터를 활용) 


  만족도,
  성과평가,
  프로젝트수_년,
  근무시간_월,
  근속년수,
  사고_2년내,
  퇴사여부,  <------------  라벨 
  승진_5년내,
  부서,
  급여


■ 규칙기반 알고리즘

  1. oneR 알고리즘  :  p 223 

     "하나의 사실만 가지고 간단하게 분류하는 알고리즘"

     간단하긴 하지만 오류가 많아진다.

  예:  가슴통증의 유무에 따라 심장질환이 있는지 분류 

       가슴통증 하나만 보고 심장질환이 있다고 분류하기에는
       오류가 많이진다. 왜냐하면 식도염, 폐질환도 가슴통증이
       있기 때문이다.  

  2. Riper 알고리즘 :  p 226  

     " 복수개의 사실(조건) 을 가지고 분류하는 알고리즘 "

  예:  하늘을 날고 털이 있다면 그것은 포유류이다.
       땅을 걷고 털이 있다면 그것은 포유류이다. 

■ 규칙기반 분류 알고리즘 (oneR 실습) 

   " 독버섯 데이터 "   

 나이브 베이즈와 비교해보기 위해서 나이브 베이즈로 독버섯과
 일반 버섯을 분류한 이원교차표를 아래에 출력하시오 ! 

 mushrooms.csv  , laplace = 0.0001

1. 버섯 데티터를 R 로 로드한다.

mushroom <- read.csv("mushrooms.csv", stringsAsFactors=T)

2. mushroom 데이터를 훈련 데이터와 테스트 데이터로 나눈다 
   ( 훈련 데이터 75%,  테스트 데이터 25% ) 

set.seed(11)

dim(mushroom) 

train_cnt <- round( 0.75 * dim(mushroom)[1])
train_index <- sample(1:dim(mushroom)[1], train_cnt, replace=F)

mushroom_train <- mushroom[train_index,  ]
mushroom_test  <- mushroom[-train_index, ]

3. 규칙기반 알고리즘인 oneR 을 이용해서 독버섯과 일반버섯을 
   분류하는 모델을 생성한다.

install.packages("OneR")
library(OneR)

model1 <- OneR(type~. ,  data=mushroom_train)

model1
summary(model1)

4. 위에서 생성한 모델을 가지고 테스트 데이터로 결과를 확인한다.

result1 <- predict( model1, mushroom_test[   , -1] )

library(gmodels)

CrossTable( mushroom_test[ , 1],  result1)  



■ 규칙기반 분류 알고리즘 (JRip 실습) 

install.packages("RWeka")
library(RWeka)

model2 <-  JRip(type~ ., data=mushroom_train)
model2

summary(model2)  

 작은 이원교차표가 하나 보임 

result2 <- predict( model2, mushroom_test[   , -1] )

library(gmodels)

CrossTable( mushroom_test[ , 1],  result2)  


                   | result2 
mushroom_test[, 1] |    edible | poisonous | Row Total | 
-------------------|-----------|-----------|-----------|
            edible |      1028 |         0 |      1028 | 
                   |   495.327 |   507.673 |           | 
                   |     1.000 |     0.000 |     0.506 | 
                   |     1.000 |     0.000 |           | 
                   |     0.506 |     0.000 |           | 
-------------------|-----------|-----------|-----------|
         poisonous |         0 |      1003 |      1003 | 
                   |   507.673 |   520.327 |           | 
                   |     0.000 |     1.000 |     0.494 | 
                   |     0.000 |     1.000 |           | 
                   |     0.000 |     0.494 |           | 
-------------------|-----------|-----------|-----------|
      Column Total |      1028 |      1003 |      2031 | 
                   |     0.506 |     0.494 |           | 
-------------------|-----------|-----------|-----------|


문제212.  Jrip 알고리즘을 R 샤이니에 새로운 텝으로 추가하시오 !


■ 6장. 회귀분석   

 * 6장 목차 

 1. 단순 선형 회귀 분석 이론
 2. 단순 선형 회귀 실습1 (탄닌 함유량과 애벌래 성장간의 관계)
 3. 단순 선형 회귀 실습2 (우주 왕복선 챌린저호 폭발 원인 분석)
 4. 단순 선형 회귀 실습3 (코스피 지수 수익율과 삼성, 현대 자동차
                          주식 수익율의 상관관계 분석)
 5. 다중 선형 회귀 이론 
 6. 다중 선형 회귀 실습1 (스마트폰 만족에 미치는 영향도 분석)
 7. 다중 선형 회귀 실습2 (미국 대학 입학에 가장 영향이 높은
                          과목 분석)
 8. 다중 선형 회귀 실습3 (보험회사의 보험료 선정에 미치는 요소
                          분석) 
 9. 다중 선형 회귀 실습4 (날씨와 전력 사용량과의 상관관계)

■ 1. 단순 선형 회귀 분석 이론

  * 머신러닝의 종류 3가지 ?

        1. 지도학습
               분류:  knn, naivebayes, decesion tree, 규칙기반 
               회귀:  

        2. 비지도 학습
        3. 강화학습 

■ 회귀분석이란 ? 

  회귀분석은 하나의 변수가 나머지 다른 변수들과의
  선형적 관계를 갖는가의 여부를 분석하는 방법으로
  하나의 종속변수(예측하고자 하는 값) 와 독립변수 사이의
  관계를 명시하는것 

  예: 집값에 가장 영향을 주는 요소가 무엇인가 ?  

  - 독립변수 :  종속변수에 영향을 주는 변수(평수,역세권,학군,..)

  - 종속변수 :  서로 관계를 가지고 있는 변수들 중에서 
                다른 변수에 영향을 받는 변수 (집값) 

   회귀식 :  y =  ax + b 
            ↓     ↓
           집값    평수 

 예:  1986년 1월 28일 미국의 스페이서 셔틀 챌린져호의 승무원 
      7명이 모두 사망했다. 우주 왕복선이 발사 도중에 폭파하는
      바람에 사망을 했다. 원인 분석을 했는데 그 원인이 ?

     "발사 온도에 대한 o 형링의 파손이 원인 "

           y =  ax + b

    y 가 o형링의 파손수,  x 가 발사 온도 

   여기서 회귀모수인 a,b 를 기계가 구하겠금 해야한다. 

   일단 a 가 -0.057 이고 b 가 4.3 이라고 기계학습 결과로 알아냈다
   면 식은  

            y = -0.057 * x  + 4.3  

■ 최소제곱 추정법(p 253)

  최적의 a(기울기) 와  b(절편) 을 결정하기 위해서 
  정규 최소젭곱으로 알려진 추정기법을 사용한다.

                     ∧
  실제값 y 와  예측값 y  사이의 수직 직선이 오차(잔차)를
  제곱해서 구한 총합을 알아야 한다.

               ∧
     ∑ ( yi - yi )^2  
          ↑   ↑
        실제값 예상값 

                ∧
      ∑ ( yi - yi )^2  
  a = --------------------
                ∧
     ∑ ( xi - xi )^2   

  이를 다시 아래의 식으로 나타내면  ?

                  
        ∑ ( x값 - x값의 평균) ( y값 - y값의 평균)  
  a = --------------------------------------------
        ∑ ( x값 - x값의 평균)^2  

         x 와 y 의 공분산 
  a = ---------------------------
              분산 
         
           cov(x,y)
   = -------------------  = 기울기 
            var(x) 

   y = ax + b 
       ↓
     기울기 

문제213. 챌린져호의 폭파원인 데이터를 R 로 로드하고 
         x 축을 온도로 하고 y 축을 o 형링 파손수로 해서
         R 샤이니에 plot 그래프를 그리시오 ! 


문제214. 챌린져호의 폭파원인을 분석하기 위한 회귀직선의
         기울기를 R 로 알아내시오 ! 

          x축 : temprature ,  y축 :  distress_ct 

           cov(x,y)
   = -------------------  = 기울기 
            var(x) 

  기울기 :  -0.04754

 공분산의 뜻 ?  서로 다른 변수들 사이에 얼마나 의존하는지를
                수치적으로 표현한것 

       y = ax + b  

       y = -0.04754 x  +  b  

# 수동으로 a (기울기) 를 추정 
setwd("d://data") 
launch <- read.csv("launch.csv", header=T)
a <-- cov( launch$temprature, launch$distress_ct)
                            / var(launch$temprature) 

# 수동으로 b (절편) 을  추정 

y = ax + b  
   ↓
b = y - ax 

b <- mean( launch$distress_ct) - a * mean(lauch$temprature) 

b = 3.698414  

  y = -0.04754 x  + 3.698414   <-- 회귀직선 식 

문제215. 위의 손으로 구한 기울기와 절편을 R 에서 제공하는
         회귀함수인 lm 을 이용해서 구해보시오 !
 setwd("d:\\data")
 launch <- read.csv("challenger.csv")

 attach(launch)

 lm( distress_ct ~ temperature,  launch )
       ↑              ↑
       y축            x축 

Call:
lm(formula = distress_ct ~ temperature, data = launch)

Coefficients:
(Intercept)  temperature  
    3.69841     -0.04754  
      ↑           ↑
    절편         기울기 

문제216. 책 251 페이지에 나오는  plot 그래프를 그리시오 !

    x축 :  온도
   y축  :  o형링 파손수 
 
공식 :   plot(y축~x축, data=데이터셋 이름) 

답 :  plot( distress_ct ~ temperature,  data=launch, col='blue')

문제217. 위의 plot 그래프의 data 에 맞는 회귀직선을 그리시오 !

m <- lm( distress_ct ~ temperature, launch)

abline( m ,  col="red")  

문제218. 위의 그래프에 제목에 회귀직선의 방정식이 출력되게 
         하시오 ! 

    y = -0.04754 x + 3.69841 

attach(launch)

names(launch)

plot(distress_ct ~ temperature, data=launch)
m <- lm(distress_ct  ~  temperature, launch)
abline(m, col='red')

title(expression?(italic(파손수== -0.04754 %*%온도 + 3.69841 )))

title(paste('파손수=', round(m$coefficients[1], 4), "* 온도 + ", round(m$coefficients[2], 4)))

문제219. 애벌래의 성장 추이와 탄닌과의 관계가 어떻게 되는지
         탄닌 포함량이 많을수록 애벌래의 성장이 증가하는지
         감소하는지를 나타내는 회귀 방정식을 구하고 시각화 하시오
         ( 데이터 :  regression.txt ) 

 reg <- read.table("regression.txt", header=T) 

growth	tannin
12	0
10	1
8	2
11	3
6	4
7	5
2	6
3	7
3	8

답 : 

 reg <- read.table("regression.txt", header=T) 

 attach(reg)
 
 plot(growth~tannin, data = reg, pch=21, col='blue', bg='red')

 m <- lm( growth ~ tannin, data=reg)

 abline(m, col='red')

title(paste( 성장률=', round(m$coefficients[1], 4), "* 탄닌 + ", round(m$coefficients[2], 4)))

문제220. 위의 그래프에 잔차를 그리시오 !
 reg <- read.table("regression.txt", header=T) 
 attach(reg)
 plot(growth~tannin, data = reg, pch=21, col='blue', bg='red')
 m <- lm( growth ~ tannin, data=reg)
 abline(m, col='red')

 join <- function(i)

 lines( c(tannin[i], tannin[i]), c( growth[i],yhat[i]),
        col="green")
  
  sapply(1:9, join) 


■ 상관관계 (p 256)

 상관분석은 두 변수가 서로 어떠한 관계인지를 파악하는 분석

 기울기에 따라 양의 상관관계, 음의 상관관계로 나눌수 있다.



                    그림 


  점들이 흩어져 있는 모습을 보고 두 변수의 관계를 파악하는데
  밀도의 차이에 따라서 상관계수를 나타낸다.

  상관계수의 기호는 "r" 을 사용한다.  

  상관계수의 수치가 0 에 가까울수록 상관관계가 약하다는 뜻이고
  +1 에 가까우면 양의 상관관계가 강하다.
  -1 에 가까우면 음의 상관관계가 강하다. 

예:  온도와 0 형링 파손수 두 변수간의 상관관계가 강한지
     약한지 알아내시오 !

  attach(lauch)
  cor( temperature, distress_ct) 

 -0.5111264

상관 관계 공식 :  

예제:

install.packages('corrplot')
library(corrplot)

z <- c(1,5,10,15)
x <- c(3,9,21,29)
y <- c(1,11,19,31)
d <- data.frame(z,x,y)
d
# 일반 상관계수
cor(d)

part1 <- lm(y~x, data=d)
summary(part1)

cov(x,y) / ( sqrt(var(x))*sqrt(var(y)) )  #0.9853035

cor(d$x, d$y) #0.9853035

문제221. 코스피 지수 수익율과 삼성전자와 현대자동차 수익율을
         가지고 plot 그래프를 그리시오 !



k_index <- read.csv("K_index.csv", header=T, 
                                       stringsAsFactors=F)

s_stock <- read.csv("S_stock.csv", header=T, 
                                       stringsAsFactors=F)

h_stock <- read.csv("H_stock.csv", header=T, 
                                       stringsAsFactors=F)

all_data <- merge(merge(k_index,s_stock), h_stock)

head(all_data)

attach(all_data)

             y축(삼성 수익율 등락 비율)
               ↓
plot(k_rate, s_rate, col="blue")
      ↑
   x축(코스피 등락 비율) 

plot(k_rate, h_rate, col="blue")

문제222. 코스피 등락 비율과 삼성 수익율 등락 비율을 plot 그래프로
        그리고 그 그래프에 회귀직선을 그으시오 !  
        
plot(k_rate, s_rate, col="blue")

model_s <- lm( s_rate ~ k_rate, data=all_data)

abline( model_s,  col="red") 

문제223. 현대자동차도 마찬가지로 회귀 그래프를 만드시오 ! 

plot(k_rate, h_rate, col="blue")

model_h <- lm( h_rate ~ k_rate, data=all_data)

abline( model_h,  col="red") 

문제224. 현대 자동차와 삼성전자 그래프를 하나의 화면으로 
         출력되게 하시오 !  

graphics.off()
par(mfrow=c(1,2), new=T)
par(mar=c(2,2,2,2) )

plot(k_rate, s_rate, col="blue")
model_s <- lm( s_rate ~ k_rate, data=all_data)
abline( model_s,  col="red") 

plot(k_rate, h_rate, col="blue")
model_h <- lm( h_rate ~ k_rate, data=all_data)
abline( model_h,  col="red") 

model_s  (삼성)

Coefficients:
(Intercept)       k_rate  
     -0.035        1.001  

model_h  (현대)  

Coefficients:
(Intercept)       k_rate  
     0.1263       0.6348  

회귀모수중 기울기가 1보다 크면 공격적 주식이고 1보다 작으면
방어적 주식이다. 

문제225. 아래의 두개의 그래프에 회귀 기울기가 각각 제목으로
         출력되게하시오 ! 

 삼성 : y = 1.001 * x - 0.035 , 현대 : y = 0.6348 * x + 0.1263

답:
k_index <- read.csv("K_index.csv", header=T, stringsAsFactors=F)
s_stock <- read.csv("S_stock.csv", header=T, stringsAsFactors=F)
h_stock <- read.csv("H_stock.csv", header=T, stringsAsFactors=F)
all_data <- merge(merge(k_index,s_stock), h_stock)
attach(all_data)

graphics.off()
par(mfrow=c(1,2), new=T)
par(mar=c(2,2,2,2) )

m_h <- lm( h_rate ~ k_rate , data = all_data )
plot(k_rate, h_rate)
abline( m_h , col = 'red')
title(paste('현대 : y=',m_h$coefficients[2] , '*x+',m_h$coefficients[1]))

m_s <- lm( s_rate ~ k_rate , data = all_data )
plot(k_rate, s_rate)
abline( m_s , col = 'blue')
title(paste('삼성 : y=',m_s$coefficients[2] , '*x',m_s$coefficients[1]))

문제226. 삼성전자와 현대전자의 삼성 수익율 등락 비율이
         각각 코스피 등락율과 상관관계가 어떻게 되는지 출력하시오

         즉 둘중에 코스피 등락율과 더 상관관계가 높은 주식이
         어떤건지 알아내시오 !

 cor( na.omit(k_rate), na.omit(h_rate) ) #  0.3262777

 cor( na.omit(k_rate), na.omit(s_rate) ) # 0.5142455

 주식 시장에서의 상관계수는 시장과 해당 종목이 얼마나 비슷하게
 움직이고 있느냐를 찾는것이다. 
 현업에서는 0.65 ~ 0.70 위부터 가치가 있다고 판단하고 투자분석을
 한다.

■ 다중 선형 회귀 분석 (p 258)  

 단순 선형 회귀분석의 목적이 하나의 독립변수만을 가지고
 종속변수를 예측하기 위한 회귀모형을 만들기 위한 것이었다면
 다중 회귀분석의 목적은 여러개의 독립변수들을 가지고 종속변수를
 예측하기 위한 회귀모형을 만드는것이다. 

 예를 들면 집갑에 영향을 미치는 요소가 단순히 평수만 있는게 
 아니다. 
 
 집값 <------ 평수, 교통, 학군, 신도시, 범죄율, 층수 , 방갯수 ..
  ↑              ↑
 종속변수      독립변수들 

  집값 =  b(절편) + a1 * x1 + a2 * x2 + a3 * x3 
                    
 독립변수들이 여러개이므로 


 단순회귀와는 다르게 행렬을 이용해서 회귀모수를 구해야한다. 


■ 다중 회귀식의 배타값 구하는 함수 생성하는 방법 

문제227. 아래의 행렬을 R 로 만드시오 !

a  
    1   2   3
    4   5   6
    7   8   9 

 a <- matrix( c(1:9) , nrow=3, ncol=3, byrow=T)

 a 

문제228. 아래의 행렬을 만드시오 !

b
  1   4   7
  2   5   8
  3   6   9 

 b <- matrix( c(1:9) , nrow=3, ncol=3, byrow=F)

문제229. a 행렬과 b 행렬의 곱을 출력하시오 !  

  a%*%b 

문제230. a 행렬의 전치행렬을 구하시오 !

  a
 t(a) 

문제231. 아래의 c 행렬을 만들고 c 행렬의 역행렬을 구하시오 !

  c <- matrix( c(1:4), nrow=2, ncol=2, byrow=T)

  c 

   1  2
   3  4  

 solve(c)  

     y =  ax +  b 

     y1          x1    1  
     y2          x2    1
     :           :     :    a
     :        =  :     :  * b    
     :           :     :
     :           :     :
     yn          xn    1


   y1 = ax1 + b
   y2 = ax2 + b   
   y3 = ax3 + b
       :
       :
   yn = axn + b

문제232. 위의 회귀모수인 a(알파) 와 b (베타) 를 구하는
         함수를 생성하시오 !  ( 책 260 페이지의 reg 라는 함수)

 reg <-  function (y, x) {
            x <- as.maxtrix(x)
            x <- cbind(intercept=1,x) 
            b <- solve(t(x)%*%x) %*% t(x) %*% y
            colnames(b) <- "estimate" 
            print(b) 

                          }

 reg( y= launch$distress_ct, x= launch$temprature) 

문제233. 챌린져호 o형링 파손의 영향을 주는 요소로 온도 말고 
         다른 요소도 있는지 launch 전체 데이터를 보고 
         온도외에 다른 독립변수들의 회귀모수 기울기를 
         reg 함수를 이용해서 출력하시오 !

   distress_ct temperature field_check_pressure flight_num
1            0          66                   50          1
2            1          70                   50          2
3            0          69                   50          3
4            0          68                   50          4

reg ( y = launch$distress_ct,  launch[2:4] )

                         estimate
Intercept             3.527093383   <--- 절편
temperature          -0.051385940 
field_check_pressure  0.001757009
flight_num            0.014292843

■ 다중 선형 회귀를 이용한 의료비 예측 실습 

 "보험회사에서 보험료를 산정하기위해 미국 국민의 의료비가
  얼마나 드는지를 예측하는 회귀모델을 생성하는게 목표 " 

 독립변수들                             종속변수 

  나이
  성별
  bmi(체질량지수)     ------------------>  의료비 
  부양가족수
  흡연유무
  미국의 어느지역에 사는지 


1. 미국 국민의 의료비 데이터를 로드 한다.

 setwd("d:\\data")
 insurance <- read.csv("insurance.csv")

 head(insurance)
 
 age    sex  bmi children smoker    region expenses
1  19 female 27.9        0    yes southwest 16884.92
2  18   male 33.8        1     no southeast  1725.55
3  28   male 33.0        3     no southeast  4449.46
4  33   male 22.7        0     no northwest 21984.47
5  32   male 28.9        0     no northwest  3866.86
6  31 female 25.7        0     no southeast  3756.62

 종속변수 :  expenses (의료비)
 독립변수 :  age, sex, bmi, children, skmoker, region

2. 의료비의 분포가 어떻게 되는지 확인한다. 

  summary(insurance$expenses)

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1122    4740    9382   13270   16640   63770 

3. 의료비의 분포를 확인하기 위해 히스토그램 그래프를 그린다.

  hist(insurance$expenses)


4. 독립변수와 종속변수간의 상관관계 분석
   그리고 독립변수들간의 상관관계를 분석 

 cor(insurance[ c("age","bmi","children","expenses")] )

               age        bmi   children   expenses
age      1.0000000 0.10934101 0.04246900 0.29900819
bmi      0.1093410 1.00000000 0.01264471 0.19857626
children 0.0424690 0.01264471 1.00000000 0.06799823
expenses 0.2990082 0.19857626 0.06799823 1.00000000

5. paris 함수를 이용해서 독립변수들간의 상관관계를 확인하시오

install.packages("psych")
libary(psych)

pairs.panels(insurance[c('age','bmi','children','expenses')])


※ 다중 공선성 

 회귀분석에서 사용된 모형의 일부 설명변수(독립변수)가 
 다른 독립변수와의 상관정도가 높아 데이터 분석시
 부정적인 영향을 미치는 현상을 말한다. 

 두 독립변수들끼리 서로에게 영향을 주고 있다면
 둘 중 하나의 영향력을 검증할때 다른 하나의 영향력을 
 완전히 통제할 수 없게 된다. 

 예를 들면  음주가 학업 성취도에 미치는 영향을 알아보기 위해서
 회귀분석을 한다고 가정해보면

 학업성취도를 종속변수로 y 로 보고 

 이를 설명해주는 독립변수 x1 는 일평균 음주량, 또 다른 독립변수 
 x2 는 혈중 알코올 농도라고 하면 

 일평균 음주량과 혈중 알코올 농도는 서로 아주 강한 상관정도를 
 보인다. 

 이 상태에서 회귀분석을 하면 분명히 x1 와 x2 둘중에 하나는
 유의한 변수로 드러나게 된다. 

 실제로 x1 과 x2 의 값이 증가또는 감소 할수록 y 값이 증가 또는
 감소할것인데 이중 하나는 굉장히 불안정한 계수값을 보이게 된다. 

예제:  car 데이터로 회귀분석전에 다중 공선성을 보이는 변수를
       찾아내시오 !

install.packages("car")
library("car")

data(Boston, package = "MASS")

summary(Boston)

str(Boston)

data <- subset(Boston, select = -c(chas, rad)) 

# integer type 인 chas, rad 변수 독립변수에서 제외

lmfit <- lm(medv ~ . , data = data)

summary(lmfit)

vif(lmfit)

vif(lmfit) > 10
 
 crim      zn   indus     nox      rm     age     dis     tax 
  FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   FALSE 
ptratio   black   lstat 
  FALSE   FALSE   FALSE 

설명 :  이 데이터에서는 다중 공선성을 보이는 변수는 없는걸로 
        확인이 되었다.

 x1 과 x2  두개가 서로 강한 상관정도가 있어서 둘다 종속변수에
 대한 영향력이 높다면 예를 들어 x2가 설명할 부분을 x1 이 
 가져가 버리기 때문에 x2의 설명력은 분명 작아지게 된다.

 그럼 쓸모 없어진 x2 의 p-value 가 커서 유의수준을 넘어버리게 
 된다.

 vif(lmfit) > 10  

 팽창계수는 보통 10 보다 큰것으로 골라내고 
 엄격하게 하려면 5 보다 큰것으로 골라낸다. 
 느슨한게 하려면 15 또는 20으로 주고 골라낸다.


■  요번주 일정 

  월                        화             수          목 
 회귀분석                 신경망    서포트벡터머신   연관규칙
 회귀분석 ncs 시험                                   k-means 
 회귀분석 샤이니화 
 회귀트리 

 다음주 
 10장. 11장. 12장 

 * 3월 26일 최종 포트폴리오 발표 계획서 오늘까지 제출 


문제234. 의료비 데이터(insurance.csv) 에서 다중공선성을 보이는
         변수가 있는지 조사하시오 ! (팽창계수가 10보다 큰 변수가
         있는지 확인하시오 !)

library("car")

data <- subset(insurance) 

lmfit <- lm(expenses ~ . , data = data)

summary(lmfit)

vif(lmfit)

vif(lmfit) > 10
          GVIF    Df GVIF^(1/(2*Df))
age      FALSE FALSE           FALSE
sex      FALSE FALSE           FALSE
bmi      FALSE FALSE           FALSE
children FALSE FALSE           FALSE
smoker   FALSE FALSE           FALSE
region   FALSE FALSE           FALSE

설명 : 의료비 데이터에서는 다중공선성을 보이는 변수는 없는것으로
       확인이 되었다.

6. 회귀함수인 lm 을 이용해서 위의 독립변수들의 회귀모수를 
   확인한다 ( 기울기 )

ins_model <- lm(expenses~age+children + bmi + sex + region, data=insurance)
 
  또는 

ins_model <- lm(expenses ~ . ,  data= insurance )

ins_model 

Coefficients:
    (Intercept)              age          sexmale  
       -11941.6            256.8           -131.4  
            bmi         children        smokeryes  
          339.3            475.7          23847.5  
regionnorthwest  regionsoutheast  regionsouthwest  
         -352.8          -1035.6           -959.3  

 북동      북서            남동              남서      
(기준)     -353          - 1036             -959 

 북동지역의 평균비용이 가장 높은 경향이 있음을 의미한다. 

※ 설명:

 1. 모델 수식에는 특징을 여섯개만 명시했지만 보고된 계수는
    절편 이외에 여덟개를 보고 하고있다.
    이렇게 된 이유는 lm() 함수가 더미코딩 기법을 모델의
    팩터 타입변수에 자동으로 적용했기 때문이다. 

 2. 나이가 일년씩 더해질 때마다 평균적으로 의료비가 256.80 달러
    정도 높아질것으로 예상한다.

 3. 부양가족수가 한명 늘어날때 마다 의료비가 평균적으로 475.7 
    달러 정도 추가되는것을 예상하고 있다.

 4. BMI 단위가 증가할때 마다 연간 의료비가 평균 339.30 달러 
    증가될것으로 예상되하고 있다.

 5. 흡연자는 비흡연자보다 매년 평균 23,847.50 달러의 비용이
    더 들것으로 예상하고 있다.

 6. 남성은 여성에 비해 매년 의료비가 131.40 달러를 적게 들것으로
    예상하고 있다.



문제235. 비만인 사람이 흡연까지 했을때 좀더 높은 패널티를
         부여하려면 즉 보험료을 인상시킬려면 어떻게 모델을
         만들어야하는가 ? (P 278)


insurance$bmi30 <- ifelse( insurance$bmi >= 30, 1, 0)   

ins_model2 <- lm(expenses ~ age + children + bmi + sex +
                   smoker+ bmi30*smoker + region , data = insurance) 

ins_model2   

   (Intercept)              age         children  
        -4740.7            263.2            520.5  
            bmi          sexmale            bmi30  
          114.8           -491.1           -863.3  
      smokeryes  regionnorthwest  regionsoutheast  
        13402.3           -266.8           -824.6  
regionsouthwest  bmi30:smokeryes  
        -1223.9          19794.3  

※ 설명 :   smokeryes 는 13,402 달러로 예상하는데 
            bmi30:smokeryes 는 19,794 달러로 예상하고 있어서 
            비만이면서 흡연을 하는 사람이 더 많은 의료비가 
            예상이 되고 있다. 
            따라서 흡연이 비만관 관련되 질병을 더 악화시킨다는
            것을 말한다. 


insurance$bmi30 <- ifelse( insurance$bmi >= 30, 1, 0)   

ins_model2 <- lm(expenses ~ age + children  + sex +
                   smoker+ bmi30*smoker + region , data = insurance) 

ins_model2   

문제236. 스마트폰 만족감(종속변수)에 영향을 미치는 
         요소들(독립변수) 중에서 가장 영향도가 높은것이 
         무엇인지 알아내시오 !

multi_hg <- read.csv("multi_hg.csv", header=T)

head(multi_hg)

      외관   편의성   유용성 만족감
1   0.92028  2.26322  2.49969   5.00
2   0.35472  0.11779  1.02933   3.00
3  -0.51868  0.20008 -0.28899   3.00
4   1.27550  0.16089  0.72871   4.33
5   1.25506 -1.83269  0.31254   3.33


 reg <-  function (y, x) {
            x <- as.matrix(x)
            x <- cbind(intercept=1,x) 
            b <- solve(t(x)%*%x) %*% t(x) %*% y
            colnames(b) <- "estimate" 
            print(b) 

                          }


reg(y=multi_hg$만족감, x=multi_hg[1:3] )

           estimate
intercept 3.5136006
외관      0.2694261
편의성    0.2105249
유용성    0.1623154
> 

또는 

attach(multi_hg)

lm(만족감 ~ 외관 + 편의성 + 유용성 , data= multi_hg)


Call:
lm(formula = 만족감 ~ 외관 + 편의성 + 유용성, data = multi_hg)

Coefficients:
(Intercept)         외관       편의성       유용성  
     3.5136       0.2694       0.2105       0.1623  

※ 설명 :  스마트폰 만족감에는 외관에 가장 영향력이 높은 변수
           라는것을 예측할 수 있다. 


문제237. 미국 대학 입학에 가장 영향을 크게 미치는 요소가
         academic 점수인지, music 점수인지 sports 점수인지
         알아내시오 ! 

         " acceptance 가 종속변수가 된다. " 

school  <-  read.csv("sports.csv", stringsAsFactors=T)

head(school)

attach(school)

model4 <-  lm( acceptance ~ ., data= school[2:5] )


model4 Call:
lm(formula = acceptance ~ ., data = school[2:5])

Coefficients:
(Intercept)     academic       sports        music  
    11.4903       0.1558       0.5727       0.1046  

> 
summary(model4) 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 11.490280   1.052578  10.916  < 2e-16 ***
academic     0.155774   0.005796  26.877  < 2e-16 ***
sports       0.572686   0.039688  14.430  < 2e-16 ***
music        0.104601   0.023427   4.465 1.35e-05 ***
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.948 on 196 degrees of freedom
Multiple R-squared:  0.9067,	Adjusted R-squared:  0.9053 
F-statistic: 634.8 on 3 and 196 DF,  p-value: < 2.2e-16

> 
Residual standard error: 5.961 on 195 degrees of freedom
Multiple R-squared:  0.9068,	Adjusted R-squared:  0.9048 
F-statistic:   474 on 4 and 195 DF,  p-value: < 2.2e-16


■ Multiple R-squared 와 Adjusted R-squared 의 차이 ?

 결정계수란 회귀모형의 데이터에 대한 설명력을 나타내는 척도이다.

 좋은 회귀 모형에는 두가지 조건이 있다

  1. 데이터를 잘 설명한다.
  2. 간단하다 

 독립변수(설명변수)가 많은 회귀모형의 경우에는 첫번째 조건을 
 만족한다.  그러나 두번째 조건에서는 탈락이라고 볼수 잇다.

 아무리 설명력이 좋아도 복잡하다면 그다지 좋은 모형이 아니다.

 그래서 결정계수가 높다고 좋은 모형이라며 쉽사리 결론을 지으면
 안된다.

 결정계수가 모형의 설명력을 측정하기 좋은 척도라는것은 사실이
 지만 위에서 애기 했던것과 같은 단점을 보완하기 위해
 보완된 척도가 등장하게 되었는게 그게 바로 

  "조정된 결정계수 (Adjusted R-squared)"  

  이 척도는 다행스럽게도 독립변수의 숫자가 증가한다고 해서
  무작정 커지지 않는다.

2. p-value 값 확인 

 
 귀무가설에서 얻은 검정 통계량의 값 이상으로 
 대립가설에서 유리한 데이터를 얻을수 있는 확률 

 P값 > 유의 수준 -> 귀무가설 수용
 P값 < 유의 수준 -> 대립가설을 채택 

 일부 P 값에는 별이 있는데, 추정치로 충족되는 유의수준
 을 나타내는 각주에 해당이 된다. 
 
 유의 수준보다 낮는 P값은 통계적으로 유의한것으로 간주된다.

문제238. (점심시간 문제) 의료비 데이터 회귀 모델의 성능(R제곱값)
         이 75% 의 설명력을 높이기 위해 책 278페이지에 나오는
        파생변수인 age2 가 이 회귀모델의 설명력을 높이는지 
        확인하시오 !
 
- 결정계수 75% 모델 :

ins_model <- lm(expenses~age+children + bmi + sex + smoker + region, data=insurance)  
 
- 여기에 age2 를 추가한 모델의 결정계수를 출력 

insurance$age2 <- insurance$age^2  

ins_model2 <- lm(expenses ~ age + age2 +children + bmi + sex + smoker + region, data=insurance)   


■ 다중 회귀 세번째 ppt 설명 

문제1. 

 답 :

 회귀분석의 회귀란 평균으로 되돌아간다는 의미이다. 

문제2. 

 명목 서열 척도 :    빙과류가 가장 많이 팔리는 영향을 주는
                    독립변수중에 계절이 있다면 ?

                    
              계절변수의 데이터:   봄, 여름, 가을 ,겨울 


문제3.

  편차 ?  관측치가 평균으로 부터의 떨어져 있는 정도 
          즉, 평균과의 차이 ( yi - yi의 평균) 

  잔차 ?  관측치와 회귀식의 예측치와의 차이 
          즉, 잔차는 편차의 일부분이다. ( yi - y 의 예측치)

  오차 ?  편차와 달리 예측하기 위하여 추정된 값과 
          실제값과의 차이 ( yi - y 의 예측치) 

          오차는 평균을 생각할 필요 없이 단순히
          예측값과 실제값의 차이를 나타낸다.
          예측값이 정확하지 못한 정도를 나타내는것 

   총 편차 ?  회귀선으로 설명할 수 없는 편차 + 
              회귀선으로 설명할 수 있는 편차 

  SST ?  Sum  of  Squered  Total
  SSE ?  Sum  of  Squered  Error
  SSR ?  Sum  of  Squered  regression 
 
 결정계수 ?  추정된 회귀선이 실제값과 평균 사이의 편차를 
             얼마나 줄여주는가를 나타내는 지수 


문제4.   


문제5. 

■ 정규화(표준화) 를 하는 경우와 안하는 경우 

 1. 정규화를 하는 경우 

  보험비용에 가장 영향을 크게 미치는 변수가 무엇인지 확인할때 

  예:  몸무게와 키는 단위가 다르므로 단위를 생각해서 
       가장 영향이 큰 변수를 확인할때 사용 
 
 2. 정규화를 안하는 경우  

  나이가 한살 더 늘어날때 또는 부양가족이 한명 더 늘어날때의
  보험료가 얼마나 인상되어야하는지를 예측할때 

 예: 부양가족이 한명 더 늘어날수록 의료비가 475달러 더 증가한다.
     비만지수 1증가는 매년 의료비를 339달러를 만든다. 
     나이의 1살 증가는 매년 의료비를 256달러 만든다.

문제5. 

답:
     만족도 = 1.873 + 0.195*통화품질 + 0.250*이미지 


문제6. 답 :         비표준화          표준화  
        통화품질     0.195             0.453      
        이미지       0.250             0.494 

    비표준화 했을때 표준화 했을때 둘다 같이 비교를 해보았을때
    이미지가 더 만족도에 더 큰 영향을 끼친다.

문제7. 비표준화 계수와 표준화 계수의 차이점에 대하여 설명하시오

   비표준화 계수는 변수의 단위를 생각하지 않는 추정치이고
   표준화 계수는 단위를 생각해서 만든 추정치이다. 

문제239. 의료비 데이터를 정규화 한후에 회귀 계수를 
         정규화 하지 않았을때의 회귀 계수와 비교하시오 ! 

- 정규화 안했을때 

insurance <- read.csv("insurance.csv")

ins_model <- lm(expenses ~ . ,  data= insurance )

ins_model 

Coefficients:
    (Intercept)              age          sexmale  
       -11941.6            256.8           -131.4  
            bmi         children        smokeryes  
          339.3            475.7          23847.5  
regionnorthwest  regionsoutheast  regionsouthwest  
         -352.8          -1035.6           -959.3  

summary(ins_model)

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     -11941.6      987.8 -12.089  < 2e-16 ***
age                256.8       11.9  21.586  < 2e-16 ***
sexmale           -131.3      332.9  -0.395 0.693255    
bmi                339.3       28.6  11.864  < 2e-16 ***
children           475.7      137.8   3.452 0.000574 ***
smokeryes        23847.5      413.1  57.723  < 2e-16 ***
regionnorthwest   -352.8      476.3  -0.741 0.458976    
regionsoutheast  -1035.6      478.7  -2.163 0.030685 *  
regionsouthwest   -959.3      477.9  -2.007 0.044921 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6062 on 1329 degrees of freedom
Multiple R-squared:  0.7509,	Adjusted R-squared:  0.7494 
F-statistic: 500.9 on 8 and 1329 DF,  p-value: < 2.2e-16

-- 정규화 했을때

normalize <- function(x) {
       return  ( ( x-min(x) )  / (max(x) - min(x) )  )
                         }

insurance <- read.csv("insurance.csv")

insurance_n <- as.data.frame( lapply( insurance[  , c(1,3,4,7) ]
                               , normalize )  ) 

model_n <- lm( expenses ~ . , data=insurance_n) 

model_n
Coefficients:
(Intercept)          age          bmi     children  
    0.02536      0.17619      0.19692      0.04334  

summary(model_n)

Call:
lm(formula = expenses ~ ., data = insurance_n)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.22161 -0.11173 -0.08144  0.11387  0.77600 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.02536    0.01492   1.699   0.0896 .  
age          0.17619    0.01637  10.766  < 2e-16 ***
bmi          0.19692    0.03038   6.481 1.28e-10 ***
children     0.04334    0.02061   2.103   0.0357 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1815 on 1334 degrees of freedom
Multiple R-squared:  0.1202,	Adjusted R-squared:  0.1182 
F-statistic: 60.74 on 3 and 1334 DF,  p-value: < 2.2e-16

> 

문제8. 

 답 : 4개의 수준을 갖는 계절변수를 봄을 기준으로 하여 3개의 
      더미변수로 아래와 같이 코딩한다. 

  계절     더미변수1     더비변수2    더미변수3
  봄            0            0           0
  여름          1            0           0
  가을          0            1           0
  겨울          0            0           1 

■ 6장의 내용

 1. 단순회귀
 2. 다중회귀

 3. 회귀트리
 4. 모델트리 

■ 회귀트리 p 282 

1. 회귀트리란 ?   수치를 예측하는 트리 -->  분류  

                                          ↙     ↘
                                      구매      비구매 

 - 의사결정트리의 결과 그림 


 - 회귀트리 결과 그림 



2. 특정 숫자를 예측하는데 다중 회귀분석을 이용하지 않고
   왜 회귀트리를 사용하는가 ?

  예측해야하는 특정숫자(집값) <--- 평수, 학군, 지하철과의 거리,.. 
  
  와인의 등급(숫자) <---- 휘발성, 알코올함량, 자유 이산화황...

 수치 예측 작업을 할때 일반적으로 전통적인 회귀방법을
 가장 먼저 선택하지만, 경우에 따라 수치 의사결정트리가 
 분명한 이점을 제공하기도 한다. 

 예를 들어 의사결정트리의 장점을 수치 예측에 활용할 수 있다.

 의사결정트리는 작업이 특징이 많거나 특징과 결과간에 매우
 복잡하고 비선형적인 관계를 가질때 잘 맞는 반면
 회귀는 이럴때 어려움이 있다. 

 회귀의 경우는 독립변수의 갯수 많으면

  1. 다중공선성도 고려를 해야했고
  2. 결정계수를 높이기위해 파생변수를 추가해야하는 모델의 
    성능을 높이기 위한 작업들이 필요했다.

3. 회귀트리의 원리 ?


  회귀트리와 모델트리의 이해 ppt 


4. 회귀트리의 나눔의 기준인 SDR 테스트 

 P284  표  

 속성 A 와 속성 B 중에 어떤게 더 균일하게 나누었는지 SDR 을 
 확인한다.

# 원본 데이터 

 tee <- c(1,1,1,2,2,3,4,5,5,6,6,7,7,7,7)

# 원본 데이터를 A 속성으로 나누었을때의 데이터

 at1 <- c(1,1,1,2,2,3,4,5,5)
 at2 <- c(6,6,7,7,7,7)

# 원본 데이터를 B 속성으로 나누었을때의 데이터

 bt1 <- c(1,1,1,2,2,3,4)
 bt2 <- c(5,5,6,6,7,7,7,7) 

# A 속성과 B 속성의 SDR 을 구한다.

 sdr_a <- sd(tee) - ( length(at1) / length(tee) * sd(at1) +
                      length(at2) / length(tee) * sd(at2) )  

 sdr_b <- sd(tee) - ( length(bt1) / length(tee) * sd(bt1) +
                      length(bt2) / length(tee) * sd(bt2) )  

 sdr_a
 sdr_b 

> sdr_a
[1] 1.202815

> sdr_b
[1] 1.392751   <---- b 속성이 더 높다 

# sdr 이 더 높은 b 속성으로 나눔을 결정하고 각각 bt1 과 bt2의
  평균값을 구해서 등급을 예측하자 !

 mean(bt1)   # 2로 예측한다.
 mean(bt2)   # 6.25 로 예측한다. 


  트리 다이어그램 

■ 와인 품질 평가 예측 모델 만들기 (p286)


■ 회귀 트리 (Regression Tree)  

 1. 와인 데이터에 대한 소개 

■ 와인 데이터로 의사결정 트리 시각화 

 
#fixed.acidity       : 고정 산도
#volatile.acidity    : 휘발성 산도
#citric.acid         : 시트르산
#residual.sugar      : 잔류 설탕
#chlorides           : 염화물
#free.sulfur.dioxide : 자유 이산화황
#total.sulfur.dioxide: 총 이산화황
#density             : 밀도
#pH                  : pH
#sulphates           : 황산염
#alcohol             : 알코올
#quality             : 품질

setwd("d:\\data")

wine <- read.csv("whitewines.csv")

2. 와인의 quality 데이터가 정규분포에 속하는 안정적인 
   데이터 인지 확인

hist(wine$quality)

※ 설명 : 와인 품질값이 6 근처를 중심으로 매우 정규적인 벨 모양의
          분포를 따르는것처럼 보인다.
          대부분의 와인이 평균 품질이기 때문에 직관적으로 이해가
          된다. 

 이상치가 있는지 확인해본다.

 summary(wine$quality) 

  Min. 1st Qu.  Median    Mean 
  3.000   5.000   6.000   5.878 

3. wine 데이터를 train 데이터와 test 데이터로 나눈다.

wine_train <- wine[1:3750,  ]
wine_test  <- wine[3751:4898, ]

4. train 데이터를 가지고 model 을 생성한다. 

library(rpart)

model <-  rpart( quality ~ . , data=wine_train)

model 

n= 3750 

node), split, n, deviance, yval
      * denotes terminal node

 1) root 3750 2945.53200 5.870933  
   2) alcohol< 10.85 2372 1418.86100 5.604975  
     4) volatile.acidity>=0.2275 1611  821.30730 5.432030  
       8) volatile.acidity>=0.3025 688  278.97670 5.255814 *
       9) volatile.acidity< 0.3025 923  505.04230 5.563380 *
     5) volatile.acidity< 0.2275 761  447.36400 5.971091 *
   3) alcohol>=10.85 1378 1070.08200 6.328737  
     6) free.sulfur.dioxide< 10.5 84   95.55952 5.369048 *
     7) free.sulfur.dioxide>=10.5 1294  892.13600 6.391036  
      14) alcohol< 11.76667 629  430.11130 6.173291  
        28) volatile.acidity>=0.465 11   10.72727 4.545455 *
        29) volatile.acidity< 0.465 618  389.71680 6.202265 *
      15) alcohol>=11.76667 665  403.99400 6.596992 *
> 

설명:  * 표시가 있는 노드는 잎노드로, 노드에서 예측이 이뤄진다는
       것을 의미한다. 와인 데이터이면 예측 등급이다.  

      5.97 이라는 등급으로 예를들면  alcohol < 10.85 이고 
      volatile.acidity < 0.2275 이면 모든 와인 샘풀 품질값이
      5.97 로 예측된다는 의미이다. 

5. 위에서 나온 모델로 트리를 시각화 하시오 !

library(rpart.plot)
rpart.plot( model, digits=3)

rpart.plot(model, digits=3, fallen.leaves=T, type=3, extra=101)

6. 위에서 만든 모델로 테스트 데이터의 라벨을 예측하시오 !

result <- predict(model, wine_test) 

7. 테스트 데이터의 실제 라벨(품질) 과 예측결과(품질) 을 비교한다

cbind( round(result), wine_test$quality)

8. 테스트 데이터의 라벨과 예측 결과와 상관관계가 어떻게 되는지
   확인한다.

cor(result, wine_test$quality)

설명 : 0.53은 두 데이터간의 연관 강도만 측정하는것이다.

      그래서 두 데이터간의 오차율이 어떻게 되는지 확인해서
      이 오차율을 줄여나가겠금 모델을 튜닝을 한다.

9. 두 데이터간의 오차율을 확인  (책 297의 평균절대오차로 측정)

MAE <-  function( actual, predicted) {
             mean(  abs( actual - predicted) ) 
                                     }

MAE( result, wine_test$quality) 

0.58  <---   이 모델의 경우 다른 모델인 서포트 벡터 머신에서의
             오차는 0.45 인데 0.58이면 상대적으로 좀 큰 오차이
             므로 개선의 여지가 필요하다.


개선방법이 회귀트리 ----> 모델트리로 변경해서 개선을 한다.

■ 와인 데이터의 품질을 예측하는 모델을 모델트리를 이용해서 생성


   그림 


 분할한 후에 평균값 대신 회귀식을 이용해서 수치를 예측한다. 

1. 회귀트리 모델 생성하는 작업의 1번 ~ 3번까지 다시 반복 

2. 모델트리를 구현하기 위한 패키지 설치

  library(RWeka)

3. 와인의 품질을 예측하는 모델을 생성한다. 
	
4. 만든 모델과 테스트 데이터로 예측을 한다.

  p.m5p <- predict( m.m5p, wine_test) 

5. 예측값(p.m5p) 과 테스트 데이터의 라벨간의 상관관계를 확인한다

  cor( p.m5p , wine_test$quality )

  0.62729

5. 예측값(p.m5p) 과 테스트 데이터의 라벨간의 평균절대오차를 확인
   한다.

  MAE( wine_test$quality, p.m5p)  

  0.5463


3?? 05, 2019 11:35:00 ???? com.github.fommil.netlib.BLAS <clinit>
????: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
3?? 05, 2019 11:35:00 ???? com.github.fommil.netlib.BLAS <clinit>
????: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
3?? 05, 2019 11:35:00 ???? com.github.fommil.netlib.LAPACK <clinit>
????: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
3?? 05, 2019 11:35:00 ???? com.github.fommil.netlib.LAPACK <clinit>
????: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK

문제240. 보스턴 하우징 데이터( 보스턴 지역의 집값)
         을 이용해서 회귀트리 모델을 생성하시오 !

         범죄율, 방의 갯수, 지역의 학교의 교사의 숫자,
         강과 인접한 거리등의 데이터를 확인해서 회귀트리 생성
         (라벨 :  MEDV(집값),
                 cat.MEDV (주택가격이 3만달러가 넘는지 안넘는지
                           에 대한 라벨)

답:

# 데이터를 로드한다. 
boston <- read.csv("D:/data/boston.csv")

# 본래 데이터의 최소값, 최대값 비교
summary(boston$MEDV)


# 훈련과 테스트 데이터 생성
boston_train <- boston[1:495, ]
boston_test <- boston[496:506, ]

str(boston_train)

# 회귀트리 모델을 생성한다.

model <-  rpart( MEDV ~ . , data=boston_train)

model 

# 생성된 모델과 테스트 데이터로 예측한다.

result <- predict(model, boston_test) 

# 결과와 실제 테스트 라벨과의 상관정도를 확인한다.

cor(result, boston_test$MEDV)

0.2808209

# 결과와 실제 테스트 라벨과의 평균절대오차를 확인한다. 

MAE <-  function( actual, predicted) {
  mean(  abs( actual - predicted) ) 
}

MAE( result, boston_test$MEDV) 

 3.484211

# 이번에는 보스톤 하우징 데이터를 모델트리로 구현해서 성능을 
  높여본다. 

library(RWeka)

m.m5p <- M5P(MEDV ~ . , data=boston_train)

p.m5p <- predict( m.m5p, boston_test) 

cor( p.m5p , boston_test$MEDV )

0.506185

MAE( boston_test$MEDV, p.m5p) 

2.869352

문제241. (점심시간 문제)  R 샤이니 최종 코드에 다중회귀 메뉴를
         추가하시오 ! 

 왓슨 메인 화면 -->  의사결정트리, 다중 회귀 


■ 7장. 신경망과 서포트 벡터 머신 

  * 목차  

   1. 활성화 함수 소개
   2. 신경망 실습 1 ( 콘크리트 데이터)
   3. 신경망 실습 2 ( 필기체 데이터 )
   4. 신경망 실습 3 ( 전력 생산량 데이터) 

■ 1. 활성화 함수 소개 

  활성화 함수란 ?   입력신호의 총합이 활성화를 일으킬지를 
                    정하는 역활을 하는 함수 

   k =  x0*w0 + x1*w1 + x2*w2

   y = f(k)

   1  :  신호가 흐른다.
   0  :  신호가 안흐른다.

 * 활성화 함수의 종류 

    1. 계단함수
    2. 시그모이드 함수
    3. 렐루 함수 

문제242. R 로 relu 함수를 만들고 relu 함수 그래프를 그리시오 !

파이썬 코드 :

import  numpy  as  np

def  relu(x):
    return  np.maximum(0,x)  # 0 과 x 값중에 큰값을 출력해라 !

print (relu(-2))  # 0 을 출력
print (relu(0.3)) # 0.3 을 출력 

답:  relu <- function(x) {

           ifelse( x>0 , x, 0)  
           
                         }

     relu(-2) 

   x <-  seq(-10, 10, 0.01)
   
   plot( x, relu(x), col="red") 

문제243. 계단함수를 R 로 구현하고 계단함수 그래프를 그리시오 !

 * 계단함수 ?   f(0.3) =  1  ,  f(-0.2) = 0  
 
                0 를 임계치로 해서 임계치 이상이면 1
                아니면 0 을 출력하는 함수로 생성하세요 

step <- function(x) {

         ifelse( x>=0, 1, 0 )

                    }

x <- seq(-5, 5, 0.01)

plot( x, step(x), col='blue', type='l')  

문제244. R 로 시그모이드 함수를 생성하고 그래프로 시각화 하시오 !

파이썬 : 

x = np.arange(-5,5,0.1)
print (x)

def  sigmoid(x):
    return 1 / (1 + np.exp(-x) )

y = sigmoid(x)

답:   sigmoid <- function(x) {

               1  / ( 1  +  exp(-x)  )

                             }   

   x <- seq(-5, 5, 0.01)

   plot( x, sigmoid(x), col='blue') 


■ 신경망 실습1 (콘크리트 데이터) 

 " 콘크리트의 강도를 예측하는 신경망을 만드는 실습 "

   자갈, 모래, 시멘트등을 몇대 몇 비율로 섞었을때
   어느정도 강도가 나오는지 예측하는 신경망 

 예: 6기 대주학생이 lg 전자에서 하는일 ?

   태양열 전지의 재료의 조합을 어떻게 했을때 가장 큰 전력 효율이
   발생하는지 예측하는 신경망을 딥러닝으로 생성 


1.  콘크리트 데이터 소개 

* 콘크리트 데이터 

 1. mount of cement: 콘크리트의 총량
 2. slag  :  시멘트 
 3. ash   :  분 (시멘트)
 4. water :  물
 5. superplasticizer :고성능 감수재(콘크리트 강도를 높이는 첨가제)
 6. coarse aggregate :  굵은 자갈
 7. fine  aggregate :  잔 자갈
 8. aging time  :  숙성시간 
 
2.  콘크리트 데이터를 R 로 로드한다.

 -  머신러닝 데이터 116번 

concrete <- read.csv("concrete.csv")
str(concrete)

3.  정규화 함수로 데이터를 정규화 작업

normalize <- function(x) {
    return ( (x-min(x)) / (max(x) - min(x) ) )
                         }

concrete_norm <- as.data.frame(lapply(concrete,normalize) ) 

4.  0~1사이로 데이터가 잘 변경되었는지 확인 

summary( concrete_norm$strength)

# 본래 데이터의 최소값, 최대값과 비교 

summary( concrete$strength)

5.  훈련 데이터,테스트 데이터를 나눈다 (8:2)

concrete_train <- concrete_norm[1:773, ]
concrete_test  <- concrete_norm[774:1030, ]

6.  neuralnet 패키지를 설치한다.

install.packages("neuralnet")
library(neuralnet) 

7.  neuralnet 패키지에 콘크리트 훈련 데이터를 넣어서
    모델을 생성한다.

concrete_model <- neuralnet(formula=strength ~ cement + slag + ash  +
water +superplastic + coarseagg  + fineagg  + age,
 data =concrete_train)   


9. 모델(신경망) 을 시각화

plot(concrete_model )

10. 만든 모델로 테스트 데이터를 가지고 테스트 한다

model_results <-  compute(concrete_model, concrete_test[1:8])

predicted_strength <-  model_results$net.result

11.  예측값과 실제값간의 상관관계를 확인 
     
cor(predicted_strength, concrete_test$strength)

 0.806285848

12. 모델의 성능개선

concrete_model2 <- neuralnet(formula=strength ~ cement + slag + ash  +water +superplastic + coarseagg  + fineagg  + age,
 data =concrete_train,  hidden= c(5,2) )  

※ 설명 :  hidden= c(  5,               2     )
                       ↑               ↑
                은닉1층의 노드수    은닉2층의 노드수 

plot(concrete_model2 )

10. 만든 모델로 테스트 데이터를 가지고 테스트 한다

model_results2 <-  compute(concrete_model2, concrete_test[1:8])

predicted_strength2 <-  model_results2$net.result

11.  예측값과 실제값간의 상관관계를 확인 
     
cor(predicted_strength2, concrete_test$strength)


■ 회귀트리일때 사용했던 와인 데이터를 신경망에 넣고 테스트

 회귀트리 모델의 결과 (상관관계) :   0.53 


■ 5. 신경망 실습 2(와인 데이터 )

 신경망 R 패키지 : 1. neuralnet 패키지(콘크리트 데이터)

                   2. nnet  패키지 (와인 데이터)

install.packages("nnet")
library(nnet)

wine <- read.csv("wine.csv")
head(wine)
str(wine)

wine_norm <- cbind(wine[1], scale(wine[-1]) )
size <- nrow(wine_norm)
size

summary(wine_norm)

set.seed(100)
index <- c( sample(1:size, size *0.7) )

train <- wine_norm[index, ]
test  <- wine_norm[-index, ]

wine_model <- nnet(Type ~ ., data = train, size=2,
                   decay=5e-04 , maxit=200 )  

predicted_result <- predict(wine_model, test, type='class')

head(predicted_result)

actual <- test$Type

table(actual, predicted_result)

model.confusion.matrix <- table(actual, predicted_result)


             | predicted_result 
      actual |        t1 |        t2 |        t3 | Row Total | 
-------------|-----------|-----------|-----------|-----------|
          t1 |        20 |         0 |         0 |        20 | 
             |    21.407 |     7.407 |     5.185 |           | 
             |     1.000 |     0.000 |     0.000 |     0.370 | 
             |     1.000 |     0.000 |     0.000 |           | 
             |     0.370 |     0.000 |     0.000 |           | 
-------------|-----------|-----------|-----------|-----------|
          t2 |         0 |        20 |         1 |        21 | 
             |     7.778 |    19.206 |     3.628 |           | 
             |     0.000 |     0.952 |     0.048 |     0.389 | 
             |     0.000 |     1.000 |     0.071 |           | 
             |     0.000 |     0.370 |     0.019 |           | 
-------------|-----------|-----------|-----------|-----------|
          t3 |         0 |         0 |        13 |        13 | 
             |     4.815 |     4.815 |    27.513 |           | 
             |     0.000 |     0.000 |     1.000 |     0.241 | 
             |     0.000 |     0.000 |     0.929 |           | 
             |     0.000 |     0.000 |     0.241 |           | 
-------------|-----------|-----------|-----------|-----------|
Column Total |        20 |        20 |        14 |        54 | 
             |     0.370 |     0.370 |     0.259 |           | 
-------------|-----------|-----------|-----------|-----------|


문제245. 위에 신경망의 뉴런수를 늘려서 1개 못맞춘것도
         맞추는지 100% 의 정확도가 될 수 있도록 성능을 높이시오 ! 
         (오늘의 마지막 문제) 


wine_model <- nnet(Type ~ ., data = train, size=10,
                   decay=5e-04 , maxit=200 ) 


■ 3. 신경망 실습 2 ( 필기체 데이터 )

* mnist 데이터 

  short_prac_train.csv
  short_prac_test.csv

setwd("d://data")
mnist_test <- read.csv("short_prac_test.csv")
mnist_train <- read.csv("short_prac_train.csv")

dim(mnist_train) # 5000  785
dim(mnist_test)  # 1000  785

mnist_test[ , 1]   # 라벨 확인 


문제245. nnet 패키지 신경망에 mnist 필기체 데이터를 넣고
         정확도를 확인하시오 !

setwd("d:\\data")

drat:::addRepo("dmlc")

cran <- getOption("repos")

cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"

options(repos = cran)

install.packages("mxnet",dependencies = T)

library(mxnet)

train<-read.csv('short_prac_train.csv')

test<-read.csv('short_prac_test.csv')


train<-data.matrix(train)  # 행렬 형태로 변환한다.

test<-data.matrix(test)    # 행렬 형태로 변환한다.

train.x<-train[,-1]  # 훈련 데이터

train.y<-train[,1]   # 훈련 데이터의 라벨 

train.x<-t(train.x/255)  # 훈련데이터를 정규화

test_org<-test   # test 원본 데이터 

test<-test[,-1]  # test 데이터의 라벨 

test<-t(test/255)  # 정규화한 테스트 데이터 


# Deep NN

data <- mx.symbol.Variable("data")  # data 라는 변수 생성 

fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128) # 1층

act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu") # relu함수

fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64) # 2층 

act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu") # relu함수

fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10) # 3층

softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm") # 소프트 맥스 함수 

devices <- mx.cpu() # cpu 사용  

mx.set.seed(0)

model <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,                                    
                                     ctx=devices, num.round=10,                                      array.batch.size=100,
                                     
                                     learning.rate=0.07, momentum=0.9,                                        eval.metric=mx.metric.accuracy,                                     
                                     initializer=mx.init.uniform(0.07),
                                     
                 epoch.end.callback=mx.callback.log.train.metric(100))

preds <- predict(model, test)

pred.label <- max.col(t(preds)) - 1

table(test_org[,1],pred.label)

sum(diag(table(test_org[,1],pred.label)))/1000  # 정확도 확인
 
> pred.label <- max.col(t(preds)) - 1
> 
> table(test_org[,1],pred.label)

   pred.label
     0  1  2  3  4  5  6  7  8  9
  0 94  0  0  0  0  1  1  2  1  1
  1  0 97  1  0  0  0  0  1  1  0
  2  0  0 98  0  1  0  0  1  0  0
  3  0  0  1 95  0  2  0  0  0  2
  4  0  1  0  0 90  0  1  2  0  6
  5  0  0  0  3  0 94  1  0  1  1
  6  2  0  0  0  0  2 96  0  0  0
  7  0  0  0  0  1  0  0 98  0  1
  8  0  0  1  1  0  3  1  0 94  0
  9  0  0  0  0  2  1  0  8  0 89
> 
> sum(diag(table(test_org[,1],pred.label)))/1000
[1] 0.945
> 

문제246. 위의 신경망을 3층 신경망인데 4층 신경망으로 늘리면 정확도가
         더 올라가는지 확인하시오 ! 


setwd("d:\\data")

drat:::addRepo("dmlc")

cran <- getOption("repos")

cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"

options(repos = cran)

#install.packages("mxnet",dependencies = T)

library(mxnet)




train<-read.csv('short_prac_train.csv')

test<-read.csv('short_prac_test.csv')







train<-data.matrix(train)

test<-data.matrix(test)

train.x<-train[,-1]

train.y<-train[,1]

train.x<-t(train.x/255)

test_org<-test

test<-test[,-1]

test<-t(test/255)




# Deep NN

data <- mx.symbol.Variable("data")

fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128)

act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")

fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64)

act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu")

fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=64)

act3 <- mx.symbol.Activation(fc3, name="relu2", act_type="relu")

fc4 <- mx.symbol.FullyConnected(act3, name="fc4", num_hidden=10)


softmax <- mx.symbol.SoftmaxOutput(fc4, name="sm")

devices <- mx.cpu()

mx.set.seed(0)

model <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,
                                     
                                     ctx=devices, num.round=10, array.batch.size=100,
                                     
                                     learning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,
                                     
                                     initializer=mx.init.uniform(0.07),
                                     
                                     epoch.end.callback=mx.callback.log.train.metric(100))




preds <- predict(model, test)

pred.label <- max.col(t(preds)) - 1

table(test_org[,1],pred.label)

sum(diag(table(test_org[,1],pred.label)))/1000


문제247. R 로 CNN 을 구현해서 필기체 데이터의 정확도를 올리시오 !

setwd("d:\\data")

drat:::addRepo("dmlc")

cran <- getOption("repos")

cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"

options(repos = cran)

#install.packages("mxnet",dependencies = T)

library(mxnet)


train<-read.csv('short_prac_train.csv')

test<-read.csv('short_prac_test.csv')

train<-data.matrix(train)

test<-data.matrix(test)

train.x<-train[,-1]

train.y<-train[,1]

train.x<-t(train.x/255)

test_org<-test

test<-test[,-1]

test<-t(test/255)


# Convolutional NN
data <- mx.symbol.Variable('data')

# first conv
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2,2), stride=c(2,2))

# second conv
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2,2), stride=c(2,2))

# first fullc
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")


# second fullc
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)

# loss
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
mx.set.seed(0)
tic <- proc.time()
device.cpu <- mx.cpu()

model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
                                     ctx=device.cpu, num.round=20, array.batch.size=100,
                                     learning.rate=0.05, momentum=0.9, wd=0.00001,
                                     eval.metric=mx.metric.accuracy,
                                     epoch.end.callback=mx.callback.log.train.metric(100))

preds <- predict(model, test)

pred.label <- max.col(t(preds)) - 1

table(test_org[,1],pred.label)

sum(diag(table(test_org[,1],pred.label)))/1000

■ 보스톤 집값을 예측하는 신경망 구현하기  

문제248. 회귀트리와 모델트리로 보스톤 집값을 예측하는 모델을 만들었을때
         최종 상관정도 0.50 이었는데 신경망이 이 상관정도를 더 높일수 
         있는지 테스트 하시오 ! 

cor( p.m5p , boston_test$MEDV )

0.506185

답:

#데이터 읽기와 구조 확인

boston<-read.csv("boston.csv")

str(boston)

head(boston)


# 정규화 함수

normalize <- function(x) {  

return((x - min(x)) / (max(x) - min(x)))

}


# 전체 데이터 프레임에 정규화 적용

boston_norm <- as.data.frame(lapply(boston, normalize))

# 0과1 사이에 범위 확인

summary(boston_norm$MEDV)


# 본래 데이터의 최소값, 최대값 비교

summary(boston$MEDV)


# 훈련과 테스트 데이터 생성



dim(boston_norm)


set.seed(1)

s_cnt<-round(0.7*(nrow(boston_norm)))

s_index<-sample(1:nrow(boston_norm), s_cnt, replace=F)

boston_train <- boston_norm[s_index, ]

boston_test <- boston_norm[-s_index, ]


head(boston_train)


## 3단계 : 데이터로 모델 훈련 ----

# neuralnet 모델 훈련

library(neuralnet)


# 하나의 은닉 뉴런에 대한 단순한 ANN

boston_model <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT, data=boston_train, hidden=10)


## 4단계 : 모델 성능 평가 ----


# 모델 결과

model_results <- compute(boston_model, boston_test[1:13])

# 강도값 예측


predicted_strength <- model_results$net.result



# 예측값과 실제값간의 상관 관계 확인

cor(predicted_strength, boston_test$MEDV)

0.9340361

문제249.  hidden=10  파라미터를 조정해서 2층을 3층으로 늘리고 노드수도
          늘려서 정확도가 올라가는지 테스트 하시오 !

hidden= c(  5,               2     )
            ↑               ↑
    은닉1층의 노드수    은닉2층의 노드수 


hidden =c(10 , 5.5)  # 95

문제250. (점심시간 문제) neuralnet 패키지를 이용해서 concreat 데이터로
         신경망 구축했던 코드를 이용해서 샤이니에 추가하시오 !
         ( 다중회귀 밑에 추가하시오! )


concrete_model2 <- neuralnet(formula=strength ~ cement + slag + ash  +water +superplastic + coarseagg  + fineagg  + age,
 data =concrete_train,  hidden= c(5,2) )  

 ui input 에  두개 생성  

   은닉1층 노드수 :   5

   은닉2층 노드수 :   2


 상관계수값 출력 


■ 8장. 연관규칙 

    - 쿠팡의 예 
    - 교보문고 홈페이지
    - 아마존 홈페이지 

 
 * 연관 규칙 ?   분유와 맥주와의 관계를 알아낸 대표적인 기계학습 방법 
  
   관련된 알고리즘 ---> Apriori 알고리즘 

 * Apriori 알고리즘 ?   간단한 성능 측정치를 이용해 거대한 DB 에서
                        데이터간의 연관성을 찾는 알고리즘 

 * Apriori 알고리즘은 어떤 데이터의 패턴을 찾을 때 유용한가 ? (p 354) 

   1. 암 데이터에서 빈번히 발생하는 DNA 패턴과 단백질의 서열을 검색할때 

   2. 사기성 신용카드 및 보험의 이용과 결합되어 발생하는 구매 또는 
      의료비 청구의 패턴 발견 

 * 연관규칙를 사람이 하기 어려운 이유가 무엇인가 ?  (P 356 )

    아이템의 집합을 아이템의 갯수만큼 만들려면 

    아이템의 갯수를 K 라고 하면 2의 K 승개의 아이템 집합이 생성되는데
    아이템이 100개면 2의 100승개의 아이템 집합이 생기므로 
    사람이 그 많은 데이터를 직접 분석하기 어렵다 .


    쿠팡 데이터 센터 사진 첨부 

 * 연관 규칙에서 사용하는 두가지 통계 척도가 무엇인가 ?  p 359 

  지지도 ? 특정 아이템이 데이터에서 발생하는 빈도 

                                 count(x)   <--- 아이템 x 의 거래건수 
   수학공식 :  support(x)  = --------------------------
                                        N   <--- 데이터베이스의 전체 거래
                  ↑                             건수 
     아이템 x 에 대한 지지도 

  신뢰도 ?    예측능력이나 정확도의 측정치 

                                      support(x,y)  
   수학공식 :  confidence(x->y) = ------------------------------
                                       support(x) 

     x, y 를 모두 포함하는 아이템 집합의 지지도를 
     x 만 포함하는 아이템 집합의 지지도로 나눈값 

예:  

Transaction ID     Items Bought
         1           우유, 버터, 시리얼
         2           우유, 시리얼
         3           우유, 빵
         4           버터, 맥주, 오징어

 지지도(우유 -> 시리얼) ? 우유와 시리얼을 동시에 구매할 확률 (결합확률)

 신뢰도(우유 -> 시리얼) ? 우유를 구매할때 시리얼도 같이 구매할 조건부 확률 
                  
 * 전체 아이템에서 우유와 시리얼이 동시에 출현될 확률은 ?

   답 :     2/4 (50%) 

 * 우유를 샀을때 , 시리얼을 살 조건부 확률은 ?

   답 :  2/3  (66%)

 * 이와는 반대로 시리얼을 샀을때 우유를 동시에 구매할 확률은 ?  

   답 :   2/2  (100%) 

 * 우유와 시리얼을 샀을때 지지도와 신뢰도를 각각 구하시오 !

  답 :                           지지도  ,    신뢰도 
             우유 ---> 시리얼 :    50%   ,      66%
             시리얼---> 우유  :    50%   ,      100% 

   
    우유를 x 라고 하고 시리얼을 y 라고 하면 
    x 와 y 의 지지도와 신뢰도를 구하는데 모든 아이템들에 대해서
    다 지지도와 신뢰도를 구한다. 

    그중에 최소 지지도 이상인 데이터만 필터링하고서
    필터링 된것 중에 신뢰도가 가장 좋은것을 찾는다. 
    
 * 어떻게 필터링을 하는가 ?


TID     Items

100    A C D
200    B C E
300    A B C E
400    B E

 * 위의 아이템에 대해서 지지도를 산정해보시오 ! 원래 지지도는
    구매건수/전체 구매건수  로 계산할 수 있지만 여기서는 단순하게
    아이템 갯수로 처리하시오 !

 답:       아이템      지지도 
             A           2
             B           3 
             C           3 
             D           1
             E           3 

  * 이것에 대해서 지지도가 1보다 큰것만 추출해서 다시 정리하시오 !

 
 답:       아이템      지지도 
             A           2
             B           3 
             C           3 
             E           3 

  * 이제 아이템들간의 연관규치을 알아야하므로 다시 아이템들간의 조합으로
    재구성하고 지지도를 다시 구하시오 ! 

  답 :      아이템       지지도 
            A  B            1
            A  C            2 
            A  E            1
            B  C            2
            B  E            3
            C  E            2

참고 :

TID     Items

100    A C D
200    B C E
300    A B C E
400    B E

     *  위의 결과에서 지지도가 1 인것은 제외하시오 ! 


  답 :      아이템       지지도 
          
            A  C            2 
            B  C            2
            B  E            3
            C  E            2

     *  이제 각각의 아이템 목록에서 첫번째 아이템을 기준으로 동일한것을
        찾아보시오 

          답 :    아이템 목록    지지도  
                   B  C  E         2

     질문 :  첫번째 아이템을 기준으로 찾는 이유는 ? 
             두번째인 C 를 기준으로 보면 
  
참고 :

TID     Items

100    A C D
200    B C E
300    A B C E
400    B E

■ apriori 알고리즘 예제 1  (맥주와 기저귀)

  " 맥주와 기저귀 판매 목록 데이터를 가지고 기저귀를 사면 맥주를 
    산다는 연관 규칙을 발견하시오 ! "

1. 데이터를 로드한다. 

x <- data.frame(
beer=c(0,1,1,1,0),
bread=c(1,1,0,1,1),
cola=c(0,0,1,0,1),
diapers=c(0,1,1,1,1),
eggs=c(0,1,0,0,0),
milk=c(1,0,1,1,1) )

x 

  beer bread cola diapers eggs milk
1    0     1    0       0    0    1
2    1     1    0       1    1    0
3    1     0    1       1    0    1
4    1     1    0       1    0    1
5    0     1    1       1    0    1


2. arules 패키지를 설치한다. 

 install.packages("arules")  
 library(arules)

 trans <-  as.matrix( x, "Transaction") 
 trans 

3. apriori 함수를 이용해서 연관관계를 분석한다. 

 rules1 <- apriori(trans, parameter=list(supp=0.2, conf=0.6,                    target="rules") )

 rules1
 

set of 49 rules  <-- 49개의 규칙이 발견되었고

inspect(sort(rules1)) 
                                     지지도    신뢰도     lift     count
[5]  {beer}               => {diapers} 0.6     1.0000000  1.2500000 3    
[6]  {diapers}            => {beer}    0.6     0.7500000  1.2500000 3    
[7]  {milk}               => {bread}   0.6     0.7500000  0.9375000 3    
[8]  {bread}              => {milk}    0.6     0.7500000  0.9375000 3    
[9]  {milk}               => {diapers} 0.6     0.7500000  0.9375000 3    
[10] {diapers}            => {milk}    0.6     0.7500000  0.9375000 3    
[11] {bread}              => {diapers} 0.6     0.7500000  0.9375000 3    
[12] {diapers}            => {bread}   0.6     0.7500000  0.9375000 3    
[13] {cola}               => {milk}    0.4     1.0000000  1.2500000 2    
[14] {cola}               => {diapers} 0.4     1.0000000 

설명 :  신뢰도가 클수록 연관관계가 높다는 의미이다.
        lift 는 상관관계를 나타낸다. 
        연관규칙을 평가하는 지수는 지지도,신뢰도 말고도 많은데
        그중에 꽤 많이 쓰이는것이 lift(향상도) 이다. 

 * 위의 맥주와 기저귀 연관 관계를 시각화 하기 

install.packages("sna")
install.packages("rgl")
library(sna)
library(rgl)


#visualization
b2 <- t(as.matrix(trans)) %*% as.matrix(trans) 
library(sna)
library(rgl)
b2.w <- b2 - diag(diag(b2))
#rownames(b2.w) 
#colnames(b2.w) 
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" , edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2) 


문제251.  상가 건물 데이터의 연관성 분석을 하고 시각화를 하시오 !  
          건물 상가에 서로 연관이 있는 업중이 무엇인가 ?
          건물에 병원이 있으면 약국이 있는가 ?

          " 보습학원이 있는 건물에는 어떤 업종의 매장이 연관되어 있는지
            찾아내시오 ! "


build <- read.csv("building.csv" , header = T)

build[is.na(build)] <- 0  
build <- build[-1]
build 

install.packages("arules")
library(arules) 
trans <- as.matrix(build , "Transaction")

rules1 <- apriori(trans , parameter = list(supp=0.2 , conf = 0.6 ,
               target = "rules"))

rules1 

inspect(sort(rules1))

     lhs                                   rhs              support confidence lift     count
[1]  {일반음식점}                       => {패밀리레스토랑} 0.40    1.0000000  2.222222 8    
[2]  {패밀리레스토랑}                   => {일반음식점}     0.40    0.8888889  2.222222 8    
[3]  {약국}                             => {휴대폰매장}     0.25    1.0000000  3.333333 5    
[4]  {휴대폰매장}                       => {약국}           0.25    0.8333333  3.333333 5    
[5]  {약국}                             => {병원}           0.25    1.0000000  3.333333 5    
[6]  {병원}                             => {약국}           0.25    0.8333333  3.333333 5    
[7]  {휴대폰매장}                       => {병원}           0.25    0.8333333  2.777778 5    
[8]  {병원}                             => {휴대폰매장}     0.25    0.8333333  2.777778 5    
[9]  {편의점}                           => {일반음식점}     0.25    1.0000000  2.500000 5    
[10] {일반음식점}                       => {편의점}         0.25    0.6250000  2.500000 5    
[11] {편의점}                           => {패밀리레스토랑} 0.25    1.0000000  2.222222 5    
[12] {화장품}                           => {패밀리레스토랑} 0.25    0.8333333  1.851852 5    
[13] {약국,휴대폰매장}                  => {병원}           0.25    1.0000000  3.333333 5    
[14] {병원,약국}                        => {휴대폰매장}     0.25    1.0000000  3.333333 5    
[15] {병원,휴대폰매장}                  => {약국}           0.25    1.0000000  4.000000 5    
[16] {일반음식점,편의점}                => {패밀리레스토랑} 0.25    1.0000000  2.222222 5    
[17] {패밀리레스토랑,편의점}            => {일반음식점}     0.25    1.0000000  2.500000 5    
[18] {일반음식점,패밀리레스토랑}        => {편의점}         0.25    0.6250000  2.500000 5    
[19] {보습학원}                         => {은행}           0.20    1.0000000  5.000000 4    
[20] {은행}                             => {보습학원}       0.20    1.0000000  5.000000 4    
[21] {보습학원}                         => {카페}           0.20    1.0000000  4.000000 4    
[22] {카페}                             => {보습학원}       0.20    0.8000000  4.000000 4    
[23] {은행}                             => {카페}           0.20    1.0000000  4.000000 4    
[24] {카페}                             => {은행}           0.20    0.8000000  4.000000 4    
[25] {당구장}                           => {일반음식점}     0.20    0.8000000  2.000000 4    
[26] {당구장}                           => {패밀리레스토랑} 0.20    0.8000000  1.777778 4    
[27] {편의점}                           => {화장품}         0.20    0.8000000  2.666667 4    
[28] {화장품}                           => {편의점}         0.20    0.6666667  2.666667 4    
[29] {화장품}                           => {일반음식점}     0.20    0.6666667  1.666667 4    
[30] {보습학원,은행}                    => {카페}           0.20    1.0000000  4.000000 4    
[31] {카페,보습학원}                    => {은행}           0.20    1.0000000  5.000000 4    
[32] {카페,은행}                        => {보습학원}       0.20    1.0000000  5.000000 4    
[33] {일반음식점,당구장}                => {패밀리레스토랑} 0.20    1.0000000  2.222222 4    
[34] {패밀리레스토랑,당구장}            => {일반음식점}     0.20    1.0000000  2.500000 4    
[35] {편의점,화장품}                    => {일반음식점}     0.20    1.0000000  2.500000 4    
[36] {일반음식점,편의점}                => {화장품}         0.20    0.8000000  2.666667 4    
[37] {일반음식점,화장품}                => {편의점}         0.20    1.0000000  4.000000 4    
[38] {편의점,화장품}                    => {패밀리레스토랑} 0.20    1.0000000  2.222222 4    
[39] {패밀리레스토랑,편의점}            => {화장품}         0.20    0.8000000  2.666667 4    
[40] {패밀리레스토랑,화장품}            => {편의점}         0.20    0.8000000  3.200000 4    
[41] {일반음식점,화장품}                => {패밀리레스토랑} 0.20    1.0000000  2.222222 4    
[42] {패밀리레스토랑,화장품}            => {일반음식점}     0.20    0.8000000  2.000000 4    
[43] {일반음식점,편의점,화장품}         => {패밀리레스토랑} 0.20    1.0000000  2.222222 4    
[44] {패밀리레스토랑,편의점,화장품}     => {일반음식점}     0.20    1.0000000  2.500000 4    
[45] {일반음식점,패밀리레스토랑,편의점} => {화장품}         0.20    0.8000000  2.666667 4    
[46] {일반음식점,패밀리레스토랑,화장품} => {편의점}         0.20    1.0000000  4.000000 4    
> 

* 시각화 코드


rules2 <- subset(rules1 , subset = lhs %pin% '보습학원' & confidence > 0.7)
inspect(sort(rules2)) 

rules3 <- subset(rules1 , subset = rhs %pin% '편의점' & confidence > 0.7)
rules3
inspect(sort(rules3)) 

#visualization
b2 <- t(as.matrix(build)) %*% as.matrix(build) 
install.packages("sna")
install.packages("rgl")
library(sna)
library(rgl)
b2.w <- b2 - diag(diag(b2))
#rownames(b2.w) 
#colnames(b2.w) 
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" , edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2) 

■ 영화 라라랜드의 긍정적 평가와 부정적 평가에 대한 워드 클라우드를 그리고
   연관성 분석을 하는 테스트 


■ 영화 라라랜드의 긍정적 평가와 부정적 평가에 대한  워드 클라우드 


1.  라라랜드 데이터를 로드한다.  

library(KoNLP)
library(wordcloud)

lala <- read.csv('라라랜드.csv', header=T, stringsAsFactors = F)

2. 영화평점이 9점이상은 긍정변수넣고 2점 이하는 부정변수에 넣는다 

lala_positive <- lala[lala$score>=9,c('content')]
lala_negative <- lala[lala$score<=2,c('content')]

head(lala_positive)
head(lala_negative)

3. 긍정게시판 변수에서 명사만 추출하고 데이터 정제 작업을 한다.

po <- sapply(lala_positive, extractNoun, USE.NAMES=F)
po2 <- unlist(po)
po2 <- Filter(function(x){nchar(x)>=2},po2)
po3 <- gsub('\\d+','',po2)
po3 <- gsub('관람객','',po3)
po3 <- gsub('평점', '', po3)
po3 <- gsub('영화', '', po3)
po3 <- gsub('진짜', '', po3)
po3 <- gsub('완전', '', po3)
po3 <- gsub('시간', '', po3)
po3 <- gsub('올해', '', po3)
po3 <- gsub('장면', '', po3)
po3 <- gsub('남자', '', po3)
po3 <- gsub('여자', '', po3)
po3 <- gsub('만큼', '', po3)
po3 <- gsub('니가', '', po3)
po3 <- gsub('년대', '', po3)
po3 <- gsub('옆사람', '', po3)
po3 <- gsub('들이', '', po3)
po3 <- gsub('저녁', '', po3)

write(unlist(po3), 'lala_positive.txt')
po4 <- read.table('lala_positive.txt')
po_wordcount <- table(po4)


4. 라라랜드 영화에 부정적인 평가 게시글들 명사로 변경하고
   정재 작업을 수행한다. 

ne <- sapply(lala_negative, extractNoun, USE.NAMES=F)
ne2 <- unlist(ne)
ne2 <- Filter(function(x){nchar(x)>=2},ne2)
ne3 <- gsub('\\d+','',ne2)
ne3 <- gsub('관람객','',ne3)
ne3 <- gsub('평점', '', ne3)
ne3 <- gsub('영화', '', ne3)
ne3 <- gsub('진짜', '', ne3)
ne3 <- gsub('완전', '', ne3)
ne3 <- gsub('시간', '', ne3)
ne3 <- gsub('올해', '', ne3)
ne3 <- gsub('장면', '', ne3)
ne3 <- gsub('남자', '', ne3)
ne3 <- gsub('여자', '', ne3)
ne3 <- gsub('만큼', '', ne3)
ne3 <- gsub('니가', '', ne3)
ne3 <- gsub('년대', '', ne3)
ne3 <- gsub('옆사람', '', ne3)
ne3 <- gsub('들이', '', ne3)
ne3 <- gsub('저녁', '', ne3)

write(unlist(ne3), 'lala_negative.txt')
ne4 <- read.table('lala_negative.txt')
ne_wordcount <- table(ne4)


5. 긍정 단어와 부정단어를 각각 워드 클라우드로 그려서 한 화면에
   출력한다. 

graphics.off()
palete <- brewer.pal(9,'Set1')
par(new=T, mfrow=c(1,2))

wordcloud(names(po_wordcount), freq=po_wordcount, scale=c(3,1), rot.per=0.1, random.order = F,
          random.color = T, col=rainbow(15))
title(main='라라랜드의 긍정적인 평가', col.main='blue')

wordcloud(names(ne_wordcount), freq=ne_wordcount, scale=c(3,1), rot.per=0.1, random.order = F,
          random.color = T, col=rainbow(15))
title(main='라라랜드의 부정적인 평가', col.main='red')



문제252. 라라랜드의 긍정적 평가 게시판의 글들을 명사만
         추출한 다음 단어들간의 연관관계를 출력하시오


답 :

1. 관련된 패키지 설치 

library(KoNLP)
library(wordcloud)
library(tm)
library(stringr)
library(arules)

2. 명사 추출하는 코드 
lala_positive <- sapply(lala_positive, extractNoun, USE.NAMES=F)
head(lala_positive)

3. unlist 로 변환한후에 철자가 2개이상이고 5개 이하인
   것만 추출 

c <- unlist(lala_positive)
lala_positive2 <- Filter(function(x) { nchar(x) >= 2 & 
                               nchar(x) <= 5 }  , c)

4. 데이터 정재작업( 분석하기에 너무 많이 나오는 단어를
    삭제하는 작업 )

# 숫자제거 
lala_positive2 <- gsub('\\d+','',lala_positive2)


lala_positive2 <- gsub('관람객','',lala_positive2)
lala_positive2 <- gsub('평점', '', lala_positive2)
lala_positive2 <- gsub('영화', '', lala_positive2)
lala_positive2 <- gsub('진짜', '', lala_positive2)
lala_positive2 <- gsub('완전', '', lala_positive2)
lala_positive2 <- gsub('시간', '', lala_positive2)
lala_positive2 <- gsub('올해', '', lala_positive2)
lala_positive2 <- gsub('장면', '', lala_positive2)
lala_positive2 <- gsub('남자', '', lala_positive2)
lala_positive2 <- gsub('여자', '', lala_positive2)
lala_positive2 <- gsub('만큼', '', lala_positive2)
lala_positive2 <- gsub('니가', '', lala_positive2)
lala_positive2 <- gsub('년대', '', lala_positive2)
lala_positive2 <- gsub('옆사람', '', lala_positive2)
lala_positive2 <- gsub('들이', '', lala_positive2)
lala_positive2 <- gsub('저녁', '', lala_positive2)
lala_positive2 <- gsub('영화', '', lala_positive2)

lala_positive2 

5. 한글이 아닌 데이터를 제거하는 작업 
res <- str_replace_all(lala_positive2, "[^[:alpha:]]","")

6.  ""  데이터 제거하는 작업 
res <- res[res != ""] 

7. 단어와 그 건수를 출력하는 작업 
wordcount <- table(res)
wordcount2 <- sort( table(res), decreasing=T)

8. 단어의 건수가 100 보다 큰것만 필터링 
keyword <- names( wordcount2[wordcount2>100] )
length(lala_positive)

9. 아프리오리 분석을 위해서 표형태로 만드는 작업 
contents <- c()
for(i in 1:length(lala_positive)) { 
  inter <- intersect(lala_positive[[i]] , keyword)
  contents <- rbind(contents ,table(inter)[keyword])
}

10. 표의 컬럼명에 단어가 들어가게한다.
colnames(contents) <- keyword

11. na 를 숫자 0 으로 변경한다. 
contents[which(is.na(contents))] <- 0 

dim(lala_positive)

12. 아프리오리 데이터 분석

detach(package:tm, unload=T)
library(arules) 
rules_lala <- apriori(contents , parameter = list(supp = 0.007 , conf = 0.3 , target = "rules"))
rules_lala 
inspect(sort(rules_lala ))


■ 9장. K-means 군집화


* 9장 목차

 1. k-means 군집화 이론 수업
 2. k-means 군집화 실습1 ( 국영수 점수 )
 3. k-means 군집화 실습2 ( 쇼설 미이더에 같은 성향을 갖는 사람들을 분류)

* 머신러닝의 종류 3가지

  1. 지도학습    :
                    분류: 의사결정트리, 나이브베이즈, knn 
                    회귀: 다중 회귀분석 

  2. 비지도 학습 :  k-means   --> 정답(라벨) 없이 기계학습 시키는 학습방법 



  3. 강화학습 

■ 1. k-means 군집화 이론 수업

 * k 평균 군집화 알고리즘이란 ?  (문제21번 답)

   k-평균 알고리즘은 주어진 데이터를 k 개의 클러스터로 묶는 알고리즘으로,
   각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작한다.

   이 알고리즘은 자율 학습의 일종으로 레이블이 달려있지 않은 입력 데이터에
   레이블을 달아주는 역활을 수행한다. 

 
 * 컴퓨터가 어떻게 클러스터 구성에 대한 사전 지식 없이 레이블이 없는 
   데이터에 군집화를 가능하게 할까 ?  (문제22 답)

  클러스터 안에 있는 아이템들은 서로 아주 비슷해야하지만 클러스터 밖에
  있느 아이템들과는 아주 달라야한다는 원칙을 따르면 가능하다. (p 388)

 * k 평균 알고리즘의 목표가 무엇인가 ?  (문제24)

  클러스터 내의 차이를 최소화하고 클러스터간의 차이를 최대화 하는것이다.

 * knn 과 k-means 의 공통점과 차이점 ? (문제25) 

  공통점 ?   거리함수(유클리드 거리)를 이용해서 중심에서 가까운 거리에 
             있는 데이터를 클러스터링 한다.

  차이점 ?   knn 은 라벨(정답) 이 있고 k-means 는 라벨(정답) 이 없다.

■ k-means 기본실습 

 1. 기본 데이터셋을 만든다.
 2. 위에서 만든 데이터 셋으로 plot 그래프를 그린다. 
 3. k-means 패키지를 설치한다.
 4. k-means 함수로 데이터를 분류한다. 
 5. 분류한 파라미터값을 가지고 다시한번 시각화를 한다.
 6. 원래 데이터로 그린 plot 그래프와 분류한 그래프를 같이 출력한다. 

예제:
1. 기본 데이터 셋을 만든다 

 c <- c(3,4,1,5,7,9,5,4,6,8,4,5,9,8,7,8,6,7,2,1)
 row <- c("A","B","C","D","E","F","G","H","I","J")
 col <- c("X","Y")
data <- matrix( c, nrow= 10, ncol=2, byrow=TRUE, dimnames=list(row,col))
data

  X Y
A 3 4
B 1 5
C 7 9
D 5 4
E 6 8
F 4 5
G 9 8
H 7 8
I 6 7
J 2 1

 2. 위에서 만든 데이터셋으로 plot 그래프를 그린다

plot(data)

 3. k-means 패키지를 설치한다

install.packages("stats")
library(stats)

 4. kmeans 함수로 데이터를 분류한다.

 ※ k 개 구하는 공식 : k=sqrt(n/2)   (문제29번)



km <- kmeans(data,2) 

> km
K-means clustering with 2 clusters of sizes 5, 5

Cluster means:  # 각 군집의 중앙 좌표값 
    X     Y
1   7    8.0
2   3    3.8

Clustering vector:
A B C D E F G H I J 
2 2 1 2 1 2 1 1 1 2 

Within cluster sum of squares by cluster:
[1]  8.0 20.8
 (between_SS / total_SS =  74.5 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"    
[5] "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault" 

     
> km$center  # 중앙점이 어딘지 확인한다. 
  X    Y
1 7   8.0
2 3   3.8

> cbind(data, km$cluster)
 X Y  
A 3 4 2
B 1 5 2
C 7 9 1
D 5 4 2
E 6 8 1
F 4 5 2
G 9 8 1
H 7 8 1
I 6 7 1
J 2 1 2


 5. 분류한 파라미터값을 가지고 다시 한번 시각화를
    한다.  

plot(round(km$center), col=km$center, pch=22,
   bg=km$center, xlim=range(0:10),ylim=range(0:10))


 6. 원래 데이터를 그린 plot 그래프와 위의 그래프를
    합쳐서 출력한다.


plot(round(km$center), col=km$center, pch=22,
   bg=km$center, xlim=range(0:10),ylim=range(0:10))

par(new=T)

plot( data, col=km$cluster+1,
      xlim=range(0:10), ylim=range(0:10) )


문제253. 위의 data 를 factoextra 패키지를 이용해서 시각화 하시오 !

install.packages("factoextra")
library(factoextra)

km <- kmeans(data,2)

fviz_cluster( km, data = data, stand=F)

■ k 평균 군집화 실습1 (국영수 점수를 가지고 학생 분류 )
문제254.  국영수 점수 데이터를 가지고 k 값을 4두고 학생들을 분류하시오!
          분류하고 시각화도 하시오 ! (수학점수와 영어점수로만 분류) 

1. 수학,영어 둘다 잘하는 학생들
2. 수학은 잘하는데 영어를 못하는 학생들
3. 영어는 잘하는데 수학을 못하는 학생들
4. 수학,영어 둘다 못하는 학생들 

academy <- read.csv("academy.csv")

academy <- academy[  ,  c(3,4) ]

km <- kmeans( academy,  4)  
km

library(factoextra)
fviz_cluster(km , data=academy,  stand=F) 
 
문제255. 학생번호, 수학점수, 영어점수, 분류번호가 같이 출력되게하시오 !

academy <- read.csv("academy.csv")

cbind( academy[   ,c(1,3,4)], km$cluster) 

   학생번호 수학점수평균 영어점수평균 km$cluster
1         1           75           85          2
2         2           90           60          4
3         3           53           48          3
4         4           96           62          4
5         5           89           80          2
6         6           92           90          2
7         7           70           66          4
8         8           90           70          4
9         9           56           43          3
10       10           67           90          1
11       11           93           77          2
12       12           80           95          2
13       13           67           80          1
14       14           40           30          3
15       15           77           92          2
16       16           88           67          4
17       17           90           70          4
18       18           89           80          2
19       19           90           80          2
20       20           92           80          2
21       21           75           70          4
22       22           92           68          4
23       23           50           70          1
24       24           70           56          4
25       25           88           67          4
26       26           76           59          4
27       27           60           76          1
28       28           88           90          2
29       29           76           56          4
30       30           54           70          1
31       31           90           70          4
32       32           80           85          2
33       33           48           69          1
34       34           77           82          2
35       35           32           45          3
36       36           78           67          4
37       37           95           72          4
38       38           90           89          2
39       39           97           90          2
40       40           63           70          1
41       41           57           78          1
42       42           92           88          2


문제256. 미국 대학 입학 점수를 가지고 academic 점수와 sport 점수를
         x , y 축으로 두고 4가지 클래스로 분류하시오 ! 

 set.seed(11) 
 
 enter_score <-  read.csv("sports.csv")


문제257. 동물 데이터를 가지고 k-means 머신러닝 기법을 수행해서 
         동물 데이터의 라벨과 k-means 의 클러스터가 일치하는지 
         확인해보시오 !

  머신러닝 데이터 게시판 87 동물 데이터  

 
 마지막 컬럼이 라벨

1: 포유류
2 : 조류
3 : 파충류
4 : 어류
5 : 양서류 
6 : 곤충
7 : 갑각류 

zoo <- read.csv("zoo.csv")

 zoo_n <-  zoo[  , 2:17] 

 ncol(zoo_n)

 zoo_model <- kmeans(zoo_n, 7) 

 x <- cbind( zoo[ , 18],  zoo_model$cluster) 

 x2 <- data.frame(x)

 x2

 library(doBy)
 
 orderBy(~X1, x2)




    X1 X2
1    1  5
2    1  5
4    1  5
5    1  5
6    1  5
7    1  5
10   1  5
11   1  5
18   1  5
20   1  4
23   1  5
28   1  2
29   1  5
30   1  2
32   1  5
33   1  2
36   1  5
37   1  5
45   1  5
46   1  5
48   1  5
49   1  5
50   1  5
51   1  5
55   1  5
56   1  5
64   1  5
65   1  5
66   1  5
67   1  4
68   1  5
69   1  5
70   1  5
71   1  5
75   1  4
76   1  2
85   1  2
94   1  2
95   1  5
97   1  2
99   1  5
12   2  6
17   2  6
21   2  6
22   2  6
24   2  6
34   2  6
38   2  6
42   2  6
44   2  6
57   2  6
58   2  6
59   2  6
60   2  6
72   2  6
79   2  6
80   2  6
84   2  6
88   2  6
96   2  6
101  2  6
63   3  7
77   3  3
81   3  7
91   3  5
92   3  5
3    4  3
8    4  3
9    4  3
13   4  3
19   4  3
35   4  3
39   4  3
61   4  3
62   4  3
74   4  3
83   4  3
87   4  3
93   4  3
26   5  5
27   5  5
53   5  5
90   5  5
25   6  1
31   6  1
40   6  1
41   6  1
43   6  1
52   6  1
89   6  1
98   6  1
14   7  7
15   7  1
16   7  1
47   7  1
54   7  1
73   7  1
78   7  7
82   7  7
86   7  1
100  7  7

   
     1  2  3  4  5  6  7
  1  0  0  0  0 30 10  0
  2  0  0  0 20  0  0  0
  3  2  0  0  3  0  0  0
  4  0  0  0 13  0  0  0
  5  4  0  0  0  0  0  0
  6  0  0  0  0  0  0  8
  7  0  4  2  4  0  0  0

1: 포유류
2 : 조류
3 : 파충류
4 : 어류
5 : 양서류 
6 : 곤충
7 : 갑각류 

문제258.(점심시간 문제)  부도여부 데이터(라벨있는 데이터) 를 k-means 
        머신러닝 기법으로 분류해서 동물 데이터 처럼 라벨과 일치하는지
        확인해보시오 !

부도예측데이터3.csv

■ k 평균 군집화 실습 (소개팅 데이터) 

  like.csv

문제259. 소개팅 데이터를 kmeans 로 분석해서 소개팅했던 상대방의
         라벨과 kmeans cluster 와 일치하는지 확인하시오 !

 set.seed(1)

 like <- read.csv("like.csv")

 like_n <- like[  , 1:7 ] 

 like_model <- kmeans( like_n,  3 )

 x <- cbind(like[ , 8] , like_model$cluster)

 x2 <- data.frame(x)

 table(x2)

X1  1 2 3
  1 0 5 0
  2 0 0 4
  3 5 0 0
 



 manytalk likebook liketravel grade tall skin muscle
1        30       80         40    40   90   90     50
2        60       50         70    50   60   60     90


 likelabel
1      1타입
2      2타입
3      2타입
4      3타입
5      1타입
6      3타입
7      1타입
8      2타입
9      3타입
10     1타입
11     3타입
12     1타입
13     2타입
14     3타입

■ k - means 군집화 실습 ( 쇼설 미디어에 같은 성향을 갖는 사람들을 분류)
 


■ 3. k 평균 군집화 실습2(쇼설 미디어에 같은 성향을
                            갖는 사람들을 분류)


 1. 데이터를 로드한다.

 teens <-  read.csv("snsdata.csv")
 str(teens)

 2. 성별이 남자가 몇명이고 여자가 몇명인지 확인한다.

 table(teens$gender) 

    F     M 
22054  5222 

 3. 성별에 NA 가 몇개인지도 출력되게하시오 

  table(teens$gender, useNA="ifany") 

    F     M  <NA> 
22054  5222  2724 

 4. 고등학생 데이터라는 정확한 데이터 분석을 위해서
    나이가 13세 ~ 20세 가 아니면 다 NA 처리해라 !  

 teens$age <- ifelse(teens$age>=13 & teens$age <20, 
                      teens$age, NA)


 summary(teens$age)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
  13.03   16.30   17.27   17.25   18.22   20.00    5523 

5. 정확한 데이터 분석을 위해 성별에 관련한 더미변수 2개를
   생성한다. 


teens$female <- ifelse(teens$gender=="F" & !is.na(teens$gender),
                       1, 0) 

teens$no_gender <- ifelse(is.na(teens$gender),1,0) 


table(teens$gender, useNA="ifany") 

    F     M  <NA> 
22054  5222  2724 

table(teens$female, useNA="ifany")
   0     1 
 7946 22054 

table(teens$no_gender, useNA="ifany")

    0     1 
27276  2724 


6. 나이가 결측치로 나온 데이터를 졸업년도로 나이를 추정해서
   결측치를 채워넣는 작업 


ave_age <- ave(teens$age, teens$gradyear, 
               FUN=function(x) mean(x, na.rm=TRUE) )

ave_age 

teens$age <- ifelse( is.na(teens$age), ave_age, teens$age)

summary(teens$age)

 Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  13.03   16.28   17.24   17.24   18.21   20.00 

7. sns 에 나타났던 관심사 횟수를 표현하는 36개의 수치형 데이터
    컬럼을 정규화 시킨다. 


interests <- teens[5:40]

interests_z <- as.data.frame(lapply(interests, scale)) 

head(interests_z)


8. kmeans 함수로 5개의 클래스로 분류 한다.

set.seed(2345)
teen_clusters <- kmeans(interests_z, 5)

teen_clusters 

9. 각 클래스의 갯수가 각각 어떻게 되는지 확인하시오 

 teen_clusters$size 

[1]   871   600  5981  1034   21514

10. 클러스터의 중심점의 좌표를 확인한다

 teen_clusters$centers 


문제260. k-means  알고리즘을 R 샤이니에 머신러닝 텝에 추가하시오 !
         ( 소개팅 like.csv 데이터를 가지고 코드 구현것으로 구현 )

 UI 화면에 ----> 1. seed 값 선택
                 2. k 값 선택 

 결과 화면은 아래와 같이 출력하시오 !


X1  1 2 3
  1 0 5 0
  2 0 0 4
  3 5 0 0

답:



































        
 
   

       



























































   
 





































































    






















                     



























     






































































                                                        





































































































            


















  



































 



ㅁ








































                















































































  





















  










 



 










































      





























































  

























 

































 














 







































1. 머신러닝이 무엇인지 ?  (1장)  



 




































































































































































































































































































































































































































































































 






































































                    






























































































































































































































 





















































































































































 


















































































































































































































 





