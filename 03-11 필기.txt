■ 10장. 모델 성능 평가





★ 모델 성능 평가가 중요한 이유가 무엇인가?

	머신러닝(학생)이 수행한 결과(분류, 예측)에 대한 공정한 평가를 통해 머신러닝(학생)이 앞으로도 
	미래의 데이터에 대해서 잘 분류하고 예측할 수 있도록 해주고 분류결과가 요행수로 맞힌게 아니다
	라는 것을 확신하게 해주며 분류 결과를 좀 더 일반화 할 수 있기 때문이다.








★ 정확도란 무엇인가?

	학습자가 맞거나 틀린 경우의 비율을 말한다.





☆ 모델 성능평가를 위해 정확도만으로는 충분하지 않은 이유?

	암 판정하는 분류기가 99%의 정확도를 갖고 있다고 하면 1%의 오류율이 있기 때문에 어떤 데이터에 
	대해서는 오류를 범할수도 있게 된다. 그래서 정확도 만으로는 성능을 측정하는데 충분치 않다.
	정확도와 더불어서 유용성에 대한 다른 성능 척도를 정의하는 것이 중요하다.
		" 정확도 + 다른 성능 척도 "





★ 그럼 다른 성능 척도에는 무엇이 있는가?

	1. 카파 통계량
	2. 민감도와 특이도
	3. 정밀도와 재현율
	4. Roc 곡선





★ 이원 교차표에서 정확도와 오류율을 확인하는 방법

그림 10





문제 262. 책 431페이지의 정확도를 보는 공식을 이용하여 아래의 분류결과의 오류율을 각각 확인하시오 !

그림 11

		 1203 + 151
	정확도 = ───── = 0.974
		    1390

	오류율 = 1 - 정확도 = 1 - 0.974 = 0.026





★ 카파 통계량 

    * Cohen's Kappa Coefficient ( 카파 상관계수 ) ?'
	두 관찰자간의 측정 범주 값에 대한 일치도를 측정하는 방법이다.
	      Pr(a) - Pr(e)
	k = ────────
	        1 - Pr(e)

		Pr(a) : 데이터에서 관찰된 2명의 평가자들의 일치 확률
		Pr(e) : 2명의 평가자들이 데이터로부터 계산된 확률적으로 일치할 확률(우연히 일치할 확률)

	k(카파 상관계수)가 0이면 완전 불일치이고, 1이면 완전 일치이다.

	p 436 카파지수

	예시 : 시험을 응시한 학생이 100명이라고 할 때 
		2명의 평가자가 합격, 불합격을 각각 판정하고 두 평가자의 일치도를 아래와 같이 보여주고
		있다. 

		설명 : 평가자 A와 B모두 40명에게 합격을, 30명에게 불합격을 주었다.
			Pr(a)는 2명의 평가자들이 일치할 확률이므로 0.7이 되게된다.
				  40 + 30        70
			Pr(a) = ────── = ─── = 0.7
				    100 	 100

			Pr(a) : 데이터에서 관찰된 2명의 평가자들의 일치 확률
			Pr(e) : 2명의 평가자들의 데이터로부터 계산된 확률적으로 일치할 확률(우연히 일치)

			Pr(e)를 계산하기 위해서는 평가자 A와 평가자 B의 각각의 합격과 불합격을 줄 확률을
			구해야 한다.

			1. 평가자 A : 합격을 60번, 불합격을 40번 주었다.
				평가자 A는 합격을 60/100 = 60%의 확률,
				         불합격을 40/100 = 40%의 확률

			2. 평가자 B : 합격을 50번, 불합격을 50번 주었다.
				평가자 B는 합격을 50/100 = 50%의 확률,
					 불합격을 50/100 = 50%의 확률
				평가자 A와 평가자 B둘 모두 확률적으로 "합격"을 줄 확률은 
				0.6x0.5=0.3이다.
				평가자 A와 평가자 B둘 모두 확률적으로 "불합격"을 줄 확률은 
				0.4x0.5=0.2 이다.

			Pr(e)는 데이터로부터 계산된 확률적으로 일치할 확률이므로 이 둘을 더해서 
			0.3 + 0.2 = 0.5 이다.

			     Pr(a) - Pr(e)    0.7 - 0.5
			k = ─────── = ───── = 0.4 (보통일치 p436)
			       1 - Pr(e)       1 - 0.5



문제 263. 책 437페이지의 이원교차표의 카파통계량을 계산하시오 !

	햄 a : 1203+31/1390  = 0.8877
	스팸 a  : 4+152/1390 = 0.1122
	햄 b : 1203+4/1390   = 0.8683
	스팸 b : 31+152/1390 = 0.1316
	
	0.8877*0.8683 + 0.1122*0.1316
	0.7707 + 0.0147	= 0.7854 ( Pr(e) )
	
	
	0.9748 - 0.7854 / 1- 0.7854
	0.1894 / 0.2146 = 0.8825 (좋은 일치)


	정확도 뿐만아니라 kappa지수도 같이 설명하면서 모델에 나온 정확도가 어쩌다 우연히 나온 결과가
	아니다라는 것을 합리적으로 설명할 수 있어야 한다.





★ 민감도와 특이도 (p440)

	유용한 분류기를 찾으려면 보통 지나치게 보수적인 예측과 지나치게 공격적인 예측 사이에서 균형이
	필요하다.

	보수적인 예측과 공격적인 예측에 대한 것을 정하는 기준이 되는 정보가 민감도와 특이도이다.

그림 12

    * 정확도 = (TP + TN) / (TP+TN+FP+FN)
	     = (20 + 880) / (20 + 90 + 10 + 880)
	     = 90%

    * 민감도 = 실제 참인것 중에서 참이라고 예측하는 비율
	     = 구매할 것으로 예측한 사람들 중에 구매한 사람의 비율
	     = TP / (TP + FN) = 20 / (20 + 10 ) = 0.66

    * 특이도 = 실제 거짓인 것중에 거짓으로 예측하는 비율
	     = 구매 안할것으로 예측한 사람들 중에 구매 안한 사람의 비율
	     = TN / (FP + TN) = 880 / (90 + 880) = 0.907

	책 442 페이지에 나와 있듯이 민감도와 특이도는 0에서 1까지의 범위에 있으며, 값이 1에 가까울 수록
	바람직 하나 실제로는 한쪽이 높으면 한쪽이 낮아져서 둘다 높게 맞출수가 없다.

	그래서 여러 모델중에 하나를 선택해야 한다면 민감도를 최고로 높게 맞춰놓고 그중에 특이도 높은것을
	선택한다.

	민감도가 높으면 특이도가 다소 낮더라도 유용한 모델이라고 볼 수 있는가?

	예를들어 예전에 신종 플루가 처음 유행했을 때 신속검사를 받아본 사람들은
	"이 검사는 정확도가 높지 않으며 추후 2~3일 소요되는 확진검사를 통해 확진판정을 내린다."
	는 말을 들어보았을 것이다.

	이런 상황이라면 민감도가 높으면 특이도가 다소 낮더라도 가치는 충분하다.






★ 정밀도와 재현율(p 442)

	정밀도 = TP / (TP + FP) = 20 / (20+90) = 0.18
	       = 실제로 구매한 사람들 중에서 구매할 것으로 예측한 사람들의 비율

	재현율 = TP / (TP + FN) = 20 / (20 + 10) = 0.66
	       = 구매할 것으로 예측한 사람들 중에서 구매한 사람들의 비율 (민감도와 같다.)

	1. 소극적 예측 : 암이라고 판단하는것 자체를 소극적으로 봐서 확실한 경우가 아니면 암으로 판단
			 하지 않는 것이다. ( 정밀도 ↑, 재현율 ↓)
		병원의 경우는 안좋은 모델이다. 왜냐하면 암인 사람을 암이 라고 판단을 잘 안한다.

	2. 공격적 예측 : 조금만 의심이 가도 다 암이라고 판단한다. ( 정밀도 ↓, 재현율 ↑ )
		이 모델은 암인데 암이라고 판단하면 잘한거고 혹시 암이 아닌 사람을 암으로 예측한 
		것이어서 실제로 검사 결과가 정상이었다면 오히려 다행이고 그냥 다시 재검사 받으면 된다.




문제 264. 아래의 스팸/햄메일을 분류하는 모델의 이원교차표를 보고 정밀도와 재현율을 각각 구하시오 !
	  " 관심 클래스를 스팸을 기준으로 "
그림 13

참고 : 그림 14

	정밀도 = 152 / 156 = 0.9743
	
	재현율 = 152 / 183 = 0.8306



문제 264. (점심시간 문제) 스팸/햄메일을 분류하는 모델의 정밀도와 재현율, 민감도, 특이도를 caret패키지를 
	  이용해서 구하시오 !

	"sms_results.csv를 이용"

	install.packages("caret")
	library(caret)
	sms <- read.csv('sms_results.csv', header = T)
	head(sms)
	nrow(sms)
	table(sms$actual_type, sms$predict_type)
	confusionMatrix(sms$actual_type, sms$predict_type, positive = "spam")
	
	Confusion Matrix and Statistics
	
	          Reference
	Prediction  ham spam
	      ham  1203    4
	      spam   31  152
	                                          
	               Accuracy : 0.9748          
	                 95% CI : (0.9652, 0.9824)
	    No Information Rate : 0.8878          
	    P-Value [Acc > NIR] : < 2.2e-16       
	                                          
	                  Kappa : 0.8825          
	 Mcnemar's Test P-Value : 1.109e-05       '
	                                          
	            Sensitivity : 0.9744          
	            Specificity : 0.9749          
	         Pos Pred Value : 0.8306          
	         Neg Pred Value : 0.9967          
	             Prevalence : 0.1122          
	         Detection Rate : 0.1094          
	   Detection Prevalence : 0.1317          
	      Balanced Accuracy : 0.9746          
	                                          
	       'Positive' Class : spam  






문제 265. (책 439쪽 참고) 파키지 irr을 설치하고 kappa2함수를 이용해서 스팸/햄 메일 분류 모델의 kappa지수를
	  구하시오 !

	install.packages("irr")
	library(irr)
	kappa2(sms[1:2])
	
	 Cohen's Kappa for 2 Raters (Weights: unweighted)                      '
	
	 Subjects = 1390 
	   Raters = 2 
	    Kappa = 0.883 
	
	        z = 33 
	  p-value = 0 





■ F 척도 ( p 445)

	정밀도와 재현율을 하나의 값으로 결합한 성능척도를 F척도라고 말한다.
	F척도는 모델의 성능을 하나의 숫자로 설명하기 때문에 모델을 나란히 비교할 수 있는 편리한 방법을
	제공한다.

		2 x 정밀도 x 재현율          2 x TP
	F척도 = ────────── = ─────────
		   재현율 + 정밀도 	2 x TP + FP + FN

	모델을 평가하려면 특이도와 민감도를 다 보고 평가를 해야하는데 그냥 F척도 하나만 보고 단순히 
	비교할 수 있다.




문제 266. 햄/스팸 메일을 분류하는 모델의 F척도가 어떻게 되는가 ? 

그림 13
	
	정밀도 : 0.9743
	재현율 : 0.8306

		2 x 정밀도 x 재현율    2 * 0.9743 * 0.8306
	F척도 = ────────── = ────────── = 0.8966
		   재현율 + 정밀도 	 0.8306 + 0.9743








■ 성능 트레이드 오프 시각화 (p 446)

	민감도와 특이도 또는 정밀도와 재현율과 같은 통계치가 모델의 성능을 하나의 숫자로 압축시키려고
	한다면 시각화는 다양한 조건에서 학습자가 어떻게 실행 하는지 보여준다.

    * ROC curve (p 447)
	거짓긍정을 피하면서 참 긍정을 탐지하는 것 사이의 트레이드 오프를 관찰하는데 유용하게 사용된다.

그림 15

    * AUC(Area under the ROC curve)란 ?
	ROC곡선이 쓸모없는 분류기를 나타내는 점선보다 완벽한 곡선에 가깝다는 것을 정량적으로 확인할 수 
	있는 수치

그림 16


	## Confusion matrixes in R ----
	sms_results <- read.csv("sms_results.csv")
	
	# the first several test cases
	head(sms_results)
	
	## Visualizing Performance Tradeoffs ----
	library(ROCR)
	pred <- prediction(predictions = sms_results$prob_spam,
	                   labels = sms_results$actual_type)
	
	# ROC curves
	perf <- performance(pred, measure = "tpr", x.measure = "fpr")
	plot(perf, main = "ROC curve for SMS spam filter", col = "blue", lwd = 2)
	
	# add a reference line to the graph
	abline(a = 0, b = 1, lwd = 2, lty = 2)
	
	# calculate AUC
	perf.auc <- performance(pred, measure = "auc")
	str(perf.auc)
	unlist(perf.auc@y.values)







■ 홀드 아웃 방법 ( p453 )

	홀드 아웃 방법이란 훈련 데이터셋과 테스트 데이터셋으로 분할하는 절차를 말한다.

	이 때 훈련 데이터를 2/3, 테스트 데이터를 1/3로 나눠서 훈련 시키는데 이때 훈련 데이터는
	무작위로 샘플링 하여 추출한다.

	그런데 무작위로 추출한 훈련 데이터중에 대표적이지 않은 데이터(Outlier)가 추출되어 훈련될
	가능성이 있다. (Outlier가 섞임)

	이런 가능성을 예방하기 위한 방법으로 반복 홀드 아웃이라는 기법이 있다.





■ k-폴드 교차 검증 테스트

	데이터 : credit.csv (부도예측)
	모델 : C5.0 의사결정트리 모델
		"k-폴드 교차검증을 통해서 credit데이터로 부도 예측을 하는 모델의 평균 카파지수를 확인
		 하는게 목표가 된다."

그림 17
그림 18


## Estimating Future Performance ----

# partitioning data
library(caret)
credit <- read.csv("credit.csv")

# Holdout method
# using random IDs
random_ids <- order(runif(1000))
credit_train <- credit[random_ids[1:500],]
credit_validate <- credit[random_ids[501:750], ]
credit_test <- credit[random_ids[751:1000], ]

# using caret function
in_train <- createDataPartition(credit$default, p = 0.75, list = FALSE)
credit_train <- credit[in_train, ]
credit_test <- credit[-in_train, ]

# 10-fold CV
folds <- createFolds(credit$default, k = 10)
str(folds)
credit01_test <- credit[folds$Fold01, ]
credit01_train <- credit[-folds$Fold01, ]

## Automating 10-fold CV for a C5.0 Decision Tree using lapply() ----
library(caret)
library(C50)
library(irr)

credit <- read.csv("credit.csv")

set.seed(123)
folds <- createFolds(credit$default, k = 10)

cv_results <- lapply(folds, function(x) {
  credit_train <- credit[-x, ]
  credit_test <- credit[x, ]
  credit_model <- C5.0(default ~ ., data = credit_train)
  credit_pred <- predict(credit_model, credit_test)
  credit_actual <- credit_test$default
  kappa <- kappa2(data.frame(credit_actual, credit_pred))$value
  return(kappa)
})

str(cv_results)
mean(unlist(cv_results))

  



 

■ 11장. 모델 성능 개선

    * 11장 목차
	1. caret패키지를 이용한 모델 파라미터 자동 튜닝
	2. 앙상블 기법
		- bagging
		- boosting





★ 정확도를 올리기 위한 방법에 대한 질문 3가지 (p 468)
	1. 데이터에 대해 어떤 종류의 머신러닝 모델을 사용할 것인가?
		예 : 독버섯 데이터의 경우 나이브베이즈보다 규칙기반 리퍼 알고리즘이 더 정확도가 좋았다.

	2. 해당모델에 대해서 파라미터 튜닝은 어떻게 할 것인가?
		예 : knn의 k값 파라미터 
		해결 방법 ? caret 패키지의 자동 파라미터 튜닝 기법을 사용

	3. 데이터를 가지고 여러 모델을 만들었을 때 이 모델중 하나를 선택해야해서 모델을 평가해야 한다면
	   어떠한 기준을 사용할 것인가?
		예 : 10장 정확도에 따른 척도 (카파계수, 정밀도, 재현율)
		해결 방법 ? caret 패키지를 활용하면 된다.




■ caret 패키지를 이용한 모델 파라미터 자동 튜닝

	" 독일은행의 채무 불이행자를 예측하는 모델 (C5.0 모델) 생성하는데 자동 파라미터 튜닝 안했을	때의 
	  정확도를 비교해보는 테스트 "





1. 자동 파라미터 튜닝 안했을때의 코드

■ 독일 은행의 대출 여부 데이터로 의사결정 트리 실습 


	 데이터 :   credit.csv ( 데이터 게시판 71번)

	1. 데이터를 로드한다.

	credit <- read.csv("credit.csv")
	str(credit) 

	2. 데이터에 각 컬럼들을 이해한다.

	 - 라벨컬럼:   default  -->  yes  : 대출금 상환 안함 
                             no   : 대출금 상환

	  예 :   prop.table( table(credit$default) ) 

	 no yes 
	0.7 0.3 


	 - 계좌 소개 :  checking_balance --> 예금계좌
	                saving_balance --->  적금계좌 

	 - amount  :   대출 금액  250마르크 ~ 18424 마르크 
	               ( 100 마르크 우리나라돈 6~7만원)

	  예:   summary( credit$amount)  
	  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
	    250    1366    2320    3271    3972   18424 
	> 


	 과거의 데이터를 분석해보니 대출금 상환 불이행자가
	 30% 나 되어서 앞으로는 30% 이내로 떨어뜨리게 
	 은행의 목표가 되겠금 model 을 생성해야한다.      

	3. 데이터가 명목형 데이터인지 확인해본다

	  str(credit) 

	4. 데이터를 shuffle 시킨다

	set.seed(31)
	credit_shuffle <-  credit[sample(nrow(credit)),  ]

	5. 데이터를  9 대 1로 나눈다

	train_num<-round(0.9*nrow(credit_shuffle),0)

	credit_train <- credit_shuffle[1:train_num,]
	credit_test <- credit_shuffle[(train_num+1):nrow(credit_shuffle),]  

	6. C5.0 패키지와 훈련데이터를 이용해서 모델을 생성한다.

	library(C50)
	credit_model <- C5.0(credit_train[, -17], credit_train[,17] )

	7. 모델과 테스트 데이터로 결과를 예측한다.

	credit_result <- predict( credit_model, credit_test[ , -17] )


	8. 이원 교차표로 결과를 살펴본다. 

	library(gmodels)
	CrossTable( credit_test[ , 17] , credit_result )
	                실제                예측 


	 - 라벨컬럼:   default  -->  yes  : 대출금 상환 안함 
	                             no   : 대출금 상환

	 
	                  | credit_result 
	credit_test[, 17] |        no |       yes | Row Total | 
	------------------|-----------|-----------|-----------|
	               no |        66 |         9 |        75 | 
	                  |     1.179 |     3.946 |           | 
	                  |     0.880 |     0.120 |     0.750 | 
	                  |     0.857 |     0.391 |           | 
	                  |     0.660 |     0.090 |           | 
	------------------|-----------|-----------|-----------|
	              yes |        11 |        14 |        25 | 
	                  |     3.536 |    11.837 |           | 
	                  |     0.440 |     0.560 |     0.250 | 
	                  |     0.143 |     0.609 |           | 
	                  |     0.110 |     0.140 |           | 
	------------------|-----------|-----------|-----------|
	     Column Total |        77 |        23 |       100 | 
	                  |     0.770 |     0.230 |           | 
	------------------|-----------|-----------|-----------|




■ 2. 자동 파라미터 튜닝했을때 정확도

	library(caret)

	m <- train(default ~. , data =credit_train  , method="C5.0")

	m

	p <- predict(m, credit_test[ , -17])

	table( p, credit_test[,17])

	p     no yes
	  no  68  11
	  yes  7  14