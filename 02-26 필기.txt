■ 나이브 베이즈 복습 

	  1. knn 과 나이브 베이즈의 차이 ?
	
	      knn 은 데이터간의 거리를 계산해서 가장 가까운 거리에 있는 
	      데이터가 나의 이웃이라고 분류하는 분류 방법이고
	
	      나이브 베이즈는 확률을 이용하여 분류하는 분류 기법 
	
	  2. 언제 knn 을 사용하고 언제 나이브베이즈를 사용해서 분류해야하는가 ? 
	
	      -  knn -->  분석하려는 데이터가 수치형 데이터일때 
	 
	          예:  유방암 데이터       
	
	      -  naive bayes -> 분석하려는 데이터가 명목형 데이터 일때 
	
	          예:  영화 선호도 데이터 
	
	  3. knn 과 나이브 베이즈로 분석하려는 질문 리스트 ?
	
	    - knn 의 질문 ?
	
	         1. 종양의 크기와 모양만 보고 악성 종양인지 양성 종양인지를
	            분류할 수 있을까 ?
	       
	         2. 붓꽃의 모양만 보고 붓꽃의 종류를 알아맞힐수 있을까 ?  
	
	    - naive bayes 의 질문 ?
	
	        1. 직업과 나이, 성별, 직업유무를 가지고 어느 영화를 더 
	           선호할지 선호도를 알아 맞힐수 있을까 ?
	
	           예: movie.csv 
	
	        2. 버섯의 모양, 색깔, 향기 등의 정보를 가지고 독버섯인지
	           일반 버섯인지 알아 맞힐 수 있을까 ?  
	
	           예: mushrooms.csv 
	
	  4. 나이브 베이즈의 원리  복습  
	
	 
	        비아그라 
	        예    아니오   총계
	 스팸   4       16     20
	 햄     1       79     80
	        5        95   100
	
	
	  P(스팸 | 비아그라 ∩ 돈 ∩ 식료품 ∩ 주소삭제)  =   확률 ?
	
	            스팸의 우도 
	   -----------------------------
	     햄의 우도 + 스팸의 우도 
	
	  P(공포 | '30대' ∩ '여자' ∩'IT' ∩ '미혼') =  확률 ?  
	
	        공포의 우도 
	  -------------------------------------------------
	  공포 + 로멘틱 + 코믹 + 무협 + 스릴러 + 액션 + SF 
	





문제194. 나이, 성별, 직업, 결혼여부, 이성친구의 여부에 따라서
         선호하는 영화 장르가 어떻게 되는지 예측하는 모델을 생성하시오!

	install.packages("e1071")
	
	library(e1071)
	
	movie <- read.csv("movie.csv", header=T)
	
	model <- naiveBayes(movie[ ,1:5], movie$장르, laplace=0)
	                       ↑           ↑
	                   훈련 데이터   훈련 데이터 라벨 
	model 
	
	test_data <- read.csv("n_test1.csv", header=TRUE) 
	
	result <- predict(model, test_data[1:5])
	
	
	result 
	
	로맨틱 





문제195. 나이가 20대이고 성별이 남자이고 직업이 학생이고 
         결혼 아직 안했고 이성친구가 없는 재혁이가 선호하는 영화는
         무엇이겠는지 나이브 베이즈로 예측 하시오 !







문제196. 독버섯과 정상 버섯을 예측하는 나이브 베이즈 모델을 생성하시오

	1. 버섯 데이터를 R 로 로드한다. 
	
	 mushroom <- read.csv("mushrooms.csv", header=T, stringsAsFactors=TRUE)
	
	 View(mushroom)
	
	2. 8124 독버섯 데이터만 따로 빼서 mush_test.csv 로 저장한다. 
	
	 mush_test <- mushroom[8123, ]
	 
	 mush_test 
	
	 write.csv( mush_test, "mush_test.csv",row.names=FALSE )
	
	3. 8124 독버섯 데이터를 훈련 데이터에서 제외 시키시오 !
	
	 nrow(mushroom)
	 mushrooms <- mushroom[ -8123,  ] 
	 nrow(mushrooms)
	
	4. mushrooms 데이터를 훈련 데이터와 테스트 데이터로 나눈다 
	    ( 훈련 데이터는 75%,  테스트 데이터는 25% )

	set.seed(1)
	dim(mushrooms)
	
	train_cnt <- round( 0.75*dim(mushrooms)[1] )
	train_cnt 
	
	train_index <- sample( 1:dim(mushrooms)[1], train_cnt, replace=F)
	
	mushrooms_train <- mushrooms[ train_index,  ]
	mushrooms_test <- mushrooms[- train_index,  ] 
	
	nrow(mushrooms_train)  #  6092
	nrow(mushrooms_test)    #  2031 
	
	str(mushrooms_train)
	
	5. 나이브 베이즈 알고리즘으로 독버섯과 일반 버섯을 분류하는 모델을 
	   생성한다.
	
	library(e1071)         모든 컬럼들
	                          ↓
	model1 <- naiveBayes(type~ . ,  data=mushrooms_train)
	                      ↑
	                   라벨 컬럼명 
	
	model1
	
	6. 위에서 만든 모델과 테스트 데이터를 가지고 독버섯과 일반버섯을 
	   잘 분류하는지 예측해 본다.
	
	result1 <- predict( model1, mushrooms_test[  , -1] )
	
	result1 
	
	7. 이원 교차표를 그려서 최종 분류 결과를 확인한다. 
	
	library(gmodels)
	
	CrossTable( mushrooms_test[  ,1], result1) 
	                   ↑                ↑
	                  실제              예측 
	
	
	                    | result1 
	mushrooms_test[, 1] |    edible | poisonous | Row Total | 
	--------------------|-----------|-----------|-----------|
	             edible |      1049 |         7 |      1056 | 
	                    |   347.908 |   447.814 |           | 
	                    |     0.993 |     0.007 |     0.520 | 
	                    |     0.918 |     0.008 |           | 
	                    |     0.516 |     0.003 |           | 
	--------------------|-----------|-----------|-----------|
	          poisonous |        94 |       881 |       975 | 
	                    |   376.811 |   485.017 |           | 
	                    |     0.096 |     0.904 |     0.480 | 
	                    |     0.082 |     0.992 |           | 
	                    |     0.046 |     0.434 |           | 
	--------------------|-----------|-----------|-----------|
	       Column Total |      1143 |       888 |      2031 | 
	                    |     0.563 |     0.437 |           | 
	--------------------|-----------|-----------|-----------|
	
	8. 위의 모델의 성능을 올리시오 !
	
	model2 <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0004)
	
	result2 <- predict( model2, mushrooms_test[ , -1] )
	
	CrossTable( mushrooms_test[ ,1], result2) 
	
	mushrooms_test[, 1] |    edible | poisonous | Row Total | 
	--------------------|-----------|-----------|-----------|
	             edible |      1050 |         6 |      1056 | 
	                    |   459.814 |   496.053 |           | 
	                    |     0.994 |     0.006 |     0.520 | 
	                    |     0.996 |     0.006 |           | 
	                    |     0.517 |     0.003 |           | 
	--------------------|-----------|-----------|-----------|
	          poisonous |         4 |       971 |       975 | 
	                    |   498.014 |   537.264 |           | 
	                    |     0.004 |     0.996 |     0.480 | 
	                    |     0.004 |     0.994 |           | 
	                    |     0.002 |     0.478 |           | 
	--------------------|-----------|-----------|-----------|
	       Column Total |      1054 |       977 |      2031 | 
	                    |     0.519 |     0.481 |           | 
	--------------------|-----------|-----------|-----------|






문제197.  위의 모델에  별도로 구분해 놓은 테스트 데이터 한개(독버섯)
          8123 번 데이터를 넣어서 독버섯인지 정상인지 확인하시오 ! 

	result3 <- predict( model2, mush_test )







문제198. (점심시간 문제)  set.seed(1) 을 정확히 다시 설정하고
         laplace 값을 0.0001 ~  0.0017 까지 주고 FN 값을 확인하시오 !

	   laplace     FN
	    0.0001      0
	     :         :
	     :         :







■ 라플라스 추정기 (P 155 페이지) 


	       비아그라(w1)      돈(w2)        식료품(w3)    주소삭제(w4)
	 우도    Yes    No      Yes    No        Yes    No    Yes    No   
	 스팸    4/20  16/20   10/20  10/20   0/20  20/20   12/20  8/20     20 
	 햄      1/80  79/80   14/80  66/80   8/80  71/80   23/80  57/80    80  
	 총합  5/100  95/100  24/100 76/100  8/100  9/100  35/100  65/100  100 
	    
	스팸의 우도 ?  
	
	  P(비아그라|스팸) * P(돈|스팸) * P(식료품|스팸) * P(주소삭제|스팸) * P(스팸)
	
	      4/20             10/20         0/20 
	      
	 ※ 설명: 식료품으로 인해서 다른 증거들이 다 무효가 되어벼렸다.
	          이를 해결하기 위해서 프랑스의 수학자 피에르 시몬 라플라스가
	          확률이 0 이 되지 않기 위해서 빈도표의 각 값에 작은 수를 
	          추가를 했다.  
	     
	    4/20       x     10/20    x     0/20    x   12/20  x    20/100 
	
	                               ↓
	
	    5/24       x     11/24    x     1/24    x   13/24  x    20/100 
	
	
	햄의 우도 ? 
	
	  P(비아그라|햄) * P(돈|햄) * P(식료품|햄) * P(주소삭제|햄) * P(햄)
	
	                       스팸의 우도 
	   스팸일 확률 = ----------------------
	                  스팸의 우도 + 햄의 우도
	






문제199. 나이브 베이즈의 알고리즘을 R 샤이니로 구현하시오 !







문제200. knn 과 나이브 베이즈 두개를 R 샤이니에 추가 시키시오 !  

그래프_테이블_KNN까지 구현한 샤이니 코드_수정본_20190225.txt 
      ↑
 위의 코드에 나이브 베이즈 코드를 추가한다.  
  








■ 5장. 의사결정 트리와 규칙 기반 분류 

	 의사결정트리란 ?
	
	  의학에서 질병에 대한 진행 과정을 바탕으로 올바른 처방을
	  위해 결정해야하는 경우나 은행에서 대출을 해줄때 대출을 
	  해줄지 말지의 여부를 기업 데이터를 보고 결정해야 하는 경우등에
	  사용하는 지도학습 머신러닝 알고리즘  






■ 엔트로피와 정보 획득량 

	 결정트리를 만들때 가장 먼저 해야할 일이 무엇인가 ?
	    
	               ↓
	
	    중요한 컬럼(변수) 를 찾는 것이다.
	
	               ↓
	
	    정보 획득량이 높은 변수 
	
	               ↓
	
	     엔트로피 함수를 사용 






■ 엔트로피(entropy) 함수

	  " 데이터의 불확실성이 얼마나 되는가 ? "  
	
	  엔트로피 지수가 높다는것은 불확실성이 높다는 것 
	
	      그림 
	
	
	  정보획득량 = 분할전 엔트로피 - 분할후 엔트로피 







문제201. 구매여부 데이터의 변수들 중에 정보획득량이 가장 높은게 
         무엇인지 알아내시오 ! (buy2.csv) 

	buy <- read.csv("buy2.csv", header=T)
	
	install.packages("FSelector")
	library(FSelector)
	
	weights <-  information.gain(buy_yn~. , buy)
	
	print(weights)
	
	cust_name          0.50040242
	card_yn            0.50040242
	review_yn          0.22314355
	before_buy_yn      0.05053431







문제202. 백화점 화장품 고객 데이터(skin.csv) 를 R 로 로드하고
         skin 데이터셋 변수들의 정보 획득량을 구하시오 !

	skin <- read.csv("skin.csv", header=T)
	skin <- skin[, -1] 
	
	x <- information.gain( cupon_react ~ ., skin)
	
	x 
	
	gender     0.080610238
	age        0.000000000
	job        0.013737789
	marry      0.224337222
	car    	   0.006023806
	





문제203. skin 데이터를 의사 결정 트리로 시각화 하시오 !

	install.pacakges("rpart")
	library(rpart)
	
	tree1 <- rpart( cupon_react ~ ., data=skin[ , -1], 
	                 control=rpart.control(minsplit=2) )  
	
	plot( tree1, compress=T, uniform=T, margin=0.1)
	text( tree1, use.n = T, col="blue")
	





문제204. 지방간 환자들 데이터의 정보획득량을 구해서 
         지방간을 일으키는데 가장 영향력이 큰 변수가 무엇인지 알아내시오 !

	fatliver <- read.csv("fatliver2.csv", header=T)
	
	library(FSelector)
	
	x <-  information.gain(FATLIVER~. , fatliver )
	
	print(x)
	
	      attr_importance
	AGE         0.022358781
	GENDER      0.019859086
	DRINK       0.008449112
	SMOKING     0.006801213





문제205. 심장질환 데이터를 내려받고 심장질환에 있어 가장 영향력이 큰 
         변수가 무엇인지 정보 획득량을 구해서 알아내시오 !






문제206. (오늘의 마지막 문제)  기업이 부도가 나는데 가장 크게 영향을
         미치는 요소가 무엇인지 부도 예측 데이터의 정보 획득량을 구해서
         알아내시오 !  ( 카페에 뎃글로 올리세요 ~)

	bankrupt <- read.csv("부도예측데이터3.csv", header=F)
	
	x <- information.gain(V1~., bankrupt) 
	
	 0 : 부도
	 1 : 건전 

	colnames(bankrupt) <- c("class","매출액","자기자본","총자본투자효율","부가가치율","매출액증가율","재고자산증가율","총자산증가율","금융비용대매출액비율","대출효율성계수","매출액순이익률","매출원가율","손익분기점율","순금융비용대매출액비율","이자보상배율","자기자본순이익률","총자본경상이익률","총자본순이익률","고정장기적합율의역","단기부채대총차입금","당좌비율","매출채권대매입채무","순운전자본비율","유동비율","유동부채대총자본","유보액대총자산비율","자기자본비율","차입금의존도","총차입금대매출액","금융비용부담율증가분","매입채무회전율","순운전자본대매출액","운전자금대매출액","재고자산회전율","총자본회전율","현금흐름지표(1)","현금흐름지표(2)","현금흐름지표(3)","현금흐름지표(4)","현금흐름지표(5)","현금흐름지표(6)","현금흐름지표(7)","현금흐름지표(8)","현금흐름지표(9)" )