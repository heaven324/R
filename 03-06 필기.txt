■ 3. 신경망 실습 2 ( 필기체 데이터 )

	* mnist 데이터 
	
	  short_prac_train.csv
	  short_prac_test.csv
	
	setwd("d://data")
	mnist_test <- read.csv("short_prac_test.csv")
	mnist_train <- read.csv("short_prac_train.csv")
	
	dim(mnist_train) # 5000  785
	dim(mnist_test)  # 1000  785
	
	mnist_test[ , 1]   # 라벨 확인 





문제246. nnet 패키지 신경망에 mnist 필기체 데이터를 넣고
         정확도를 확인하시오 !

	setwd("d:\\data")
	
	drat:::addRepo("dmlc")
	
	cran <- getOption("repos")
	
	cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"
	
	options(repos = cran)
	
	install.packages("mxnet",dependencies = T)
	
	library(mxnet)
	
	train<-read.csv('short_prac_train.csv')
	
	test<-read.csv('short_prac_test.csv')
	
	
	train<-data.matrix(train)  # 행렬 형태로 변환한다.
	
	test<-data.matrix(test)    # 행렬 형태로 변환한다.
	
	train.x<-train[,-1]  # 훈련 데이터
	
	train.y<-train[,1]   # 훈련 데이터의 라벨 
	
	train.x<-t(train.x/255)  # 훈련데이터를 정규화
	
	test_org<-test   # test 원본 데이터 
	
	test<-test[,-1]  # test 데이터의 라벨 
	
	test<-t(test/255)  # 정규화한 테스트 데이터 
	
	
	# Deep NN
	
	data <- mx.symbol.Variable("data")  # data 라는 변수 생성 
	
	fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128) # 1층
	
	act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu") # relu함수
	
	fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64) # 2층 
	
	act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu") # relu함수
	
	fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10) # 3층
	
	softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm") # 소프트 맥스 함수 
	
	devices <- mx.cpu() # cpu 사용  
	
	mx.set.seed(0)
	
	model <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,                                    
	                                     ctx=devices, num.round=10,                                      	array.batch.size=100,
	                                     
	                                     learning.rate=0.07, momentum=0.9,                                      	  eval.metric=mx.metric.accuracy,                                     
	                                     initializer=mx.init.uniform(0.07),
	                                     
	                 epoch.end.callback=mx.callback.log.train.metric(100))
	
	preds <- predict(model, test)
	
	pred.label <- max.col(t(preds)) - 1
	
	table(test_org[,1],pred.label)
	
	sum(diag(table(test_org[,1],pred.label)))/1000  # 정확도 확인
	 
	> pred.label <- max.col(t(preds)) - 1
	> 
	> table(test_org[,1],pred.label)
	
	   pred.label
	     0  1  2  3  4  5  6  7  8  9
	  0 94  0  0  0  0  1  1  2  1  1
	  1  0 97  1  0  0  0  0  1  1  0
	  2  0  0 98  0  1  0  0  1  0  0
	  3  0  0  1 95  0  2  0  0  0  2
	  4  0  1  0  0 90  0  1  2  0  6
	  5  0  0  0  3  0 94  1  0  1  1
	  6  2  0  0  0  0  2 96  0  0  0
	  7  0  0  0  0  1  0  0 98  0  1
	  8  0  0  1  1  0  3  1  0 94  0
	  9  0  0  0  0  2  1  0  8  0 89
	> 
	> sum(diag(table(test_org[,1],pred.label)))/1000
	[1] 0.945
	> 






문제246. 위의 신경망을 3층 신경망인데 4층 신경망으로 늘리면 정확도가
         더 올라가는지 확인하시오 ! 


	setwd("d:\\data")
	
	drat:::addRepo("dmlc")
	
	cran <- getOption("repos")
	
	cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"
	
	options(repos = cran)
	
	#install.packages("mxnet",dependencies = T)
	
	library(mxnet)
	
	
	train<-read.csv('short_prac_train.csv')
	
	test<-read.csv('short_prac_test.csv')
	

	train<-data.matrix(train)
	
	test<-data.matrix(test)
	
	train.x<-train[,-1]
	
	train.y<-train[,1]
	
	train.x<-t(train.x/255)
	
	test_org<-test
	
	test<-test[,-1]
	
	test<-t(test/255)
	

	# Deep NN
	
	data <- mx.symbol.Variable("data")
	
	fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128)
	
	act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")
	
	fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64)
	
	act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu")
	
	fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=64)
	
	act3 <- mx.symbol.Activation(fc3, name="relu2", act_type="relu")
	
	fc4 <- mx.symbol.FullyConnected(act3, name="fc4", num_hidden=10)
	
	
	softmax <- mx.symbol.SoftmaxOutput(fc4, name="sm")
	
	devices <- mx.cpu()
	
	mx.set.seed(0)
	
	model <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,
	                                     
	                                     ctx=devices, num.round=10, array.batch.size=100,
	                                     
	                                     learning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,
	                                     
	                                     initializer=mx.init.uniform(0.07),
	                                     
	                                     epoch.end.callback=mx.callback.log.train.metric(100))
	
	
	
	
	preds <- predict(model, test)
	
	pred.label <- max.col(t(preds)) - 1
	
	table(test_org[,1],pred.label)
	
	sum(diag(table(test_org[,1],pred.label)))/1000








문제247. R 로 CNN 을 구현해서 필기체 데이터의 정확도를 올리시오 !

	setwd("d:\\data")
	
	drat:::addRepo("dmlc")
	
	cran <- getOption("repos")
	
	cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"
	
	options(repos = cran)
	
	#install.packages("mxnet",dependencies = T)
	
	library(mxnet)
	
	
	train<-read.csv('short_prac_train.csv')
	
	test<-read.csv('short_prac_test.csv')
	
	train<-data.matrix(train)
	
	test<-data.matrix(test)
	
	train.x<-train[,-1]
	
	train.y<-train[,1]
	
	train.x<-t(train.x/255)
	
	test_org<-test
	
	test<-test[,-1]
	
	test<-t(test/255)
	
		
	# Convolutional NN
	data <- mx.symbol.Variable('data')
	
	# first conv
	conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)
	tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
	pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2,2), stride=c(2,2))
	
	# second conv
	conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
	tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
	pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2,2), stride=c(2,2))
	
	# first fullc
	flatten <- mx.symbol.Flatten(data=pool2)
	fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
	tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
	
	
	# second fullc
	fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
	
	# loss
	lenet <- mx.symbol.SoftmaxOutput(data=fc2)	
	train.array <- train.x
	dim(train.array) <- c(28, 28, 1, ncol(train.x))
	test.array <- test
	dim(test.array) <- c(28, 28, 1, ncol(test))
	mx.set.seed(0)
	tic <- proc.time()
	device.cpu <- mx.cpu()
	
	model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
	                                     ctx=device.cpu, num.round=20, array.batch.size=100,
	                                     learning.rate=0.05, momentum=0.9, wd=0.00001,
	                                     eval.metric=mx.metric.accuracy,
	                                     epoch.end.callback=mx.callback.log.train.metric(100))
	
	preds <- predict(model, test)
	
	pred.label <- max.col(t(preds)) - 1
	
	table(test_org[,1],pred.label)
	
	sum(diag(table(test_org[,1],pred.label)))/1000






■ 보스톤 집값을 예측하는 신경망 구현하기  




문제248. 회귀트리와 모델트리로 보스톤 집값을 예측하는 모델을 만들었을때
         최종 상관정도 0.50 이었는데 신경망이 이 상관정도를 더 높일수 
         있는지 테스트 하시오 ! 

	cor( p.m5p , boston_test$MEDV )
	
	0.506185
	
	답:
	
	#데이터 읽기와 구조 확인
	
	boston<-read.csv("boston.csv")
	
	str(boston)
	
	head(boston)
	
	
	# 정규화 함수
	
	normalize <- function(x) {  
	
	return((x - min(x)) / (max(x) - min(x)))
	
	}
	
	
	# 전체 데이터 프레임에 정규화 적용
	
	boston_norm <- as.data.frame(lapply(boston, normalize))
	
	# 0과1 사이에 범위 확인
	
	summary(boston_norm$MEDV)
	
	
	# 본래 데이터의 최소값, 최대값 비교
	
	summary(boston$MEDV)
	
	
	# 훈련과 테스트 데이터 생성
	
	
	
	dim(boston_norm)
	
	
	set.seed(1)
	
	s_cnt<-round(0.7*(nrow(boston_norm)))
	
	s_index<-sample(1:nrow(boston_norm), s_cnt, replace=F)
	
	boston_train <- boston_norm[s_index, ]
	
	boston_test <- boston_norm[-s_index, ]
	
	
	head(boston_train)
	
	
	## 3단계 : 데이터로 모델 훈련 ----
	
	# neuralnet 모델 훈련
	
	library(neuralnet)
	
	
	# 하나의 은닉 뉴런에 대한 단순한 ANN
	
	boston_model <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT, 	data=boston_train, hidden=10)
	
	
	## 4단계 : 모델 성능 평가 ----
	
	
	# 모델 결과
	
	model_results <- compute(boston_model, boston_test[1:13])
	
	# 강도값 예측
	
	
	predicted_strength <- model_results$net.result
	
	
	
	# 예측값과 실제값간의 상관 관계 확인
	
	cor(predicted_strength, boston_test$MEDV)
	
	0.9340361






문제249.  hidden=10  파라미터를 조정해서 2층을 3층으로 늘리고 노드수도
          늘려서 정확도가 올라가는지 테스트 하시오 !

	hidden= c(  5,               2     )
	            ↑               ↑
	    은닉1층의 노드수    은닉2층의 노드수 
	
	
	hidden =c(10 , 5.5)  # 95






문제250. (점심시간 문제) neuralnet 패키지를 이용해서 concreat 데이터로
         신경망 구축했던 코드를 이용해서 샤이니에 추가하시오 !
         ( 다중회귀 밑에 추가하시오! )


	concrete_model2 <- neuralnet(formula=strength ~ cement + slag + ash  +water +superplastic + coarseagg  + 	fineagg  + age,
	 data =concrete_train,  hidden= c(5,2) )  
	
	 ui input 에  두개 생성  
	
	   은닉1층 노드수 :   5
	
	   은닉2층 노드수 :   2
	
	
	 상관계수값 출력 
	








■ 8장. 연관규칙 

	    - 쿠팡의 예 
	    - 교보문고 홈페이지
	    - 아마존 홈페이지 
	
	 
	 * 연관 규칙 ?   분유와 맥주와의 관계를 알아낸 대표적인 기계학습 방법 
	  
	   관련된 알고리즘 ---> Apriori 알고리즘 
	
	 * Apriori 알고리즘 ?   간단한 성능 측정치를 이용해 거대한 DB 에서
	                        데이터간의 연관성을 찾는 알고리즘 
	
	 * Apriori 알고리즘은 어떤 데이터의 패턴을 찾을 때 유용한가 ? (p 354) 
	
	   1. 암 데이터에서 빈번히 발생하는 DNA 패턴과 단백질의 서열을 검색할때 
	
	   2. 사기성 신용카드 및 보험의 이용과 결합되어 발생하는 구매 또는 
	      의료비 청구의 패턴 발견 
	
	 * 연관규칙를 사람이 하기 어려운 이유가 무엇인가 ?  (P 356 )
	
	    아이템의 집합을 아이템의 갯수만큼 만들려면 
	
	    아이템의 갯수를 K 라고 하면 2의 K 승개의 아이템 집합이 생성되는데
	    아이템이 100개면 2의 100승개의 아이템 집합이 생기므로 
	    사람이 그 많은 데이터를 직접 분석하기 어렵다 .
	
	
	    쿠팡 데이터 센터 사진 첨부 
	
	 * 연관 규칙에서 사용하는 두가지 통계 척도가 무엇인가 ?  p 359 
	
	  지지도 ? 특정 아이템이 데이터에서 발생하는 빈도 
	
	                                 count(x)   <--- 아이템 x 의 거래건수 
	   수학공식 :  support(x)  = --------------------------
	                                        N   <--- 데이터베이스의 전체 거래
	                  ↑                             건수 
	     아이템 x 에 대한 지지도 
	
	  신뢰도 ?    예측능력이나 정확도의 측정치 
	
	                                      support(x,y)  
	   수학공식 :  confidence(x->y) = ------------------------------
	                                       support(x) 
	
	     x, y 를 모두 포함하는 아이템 집합의 지지도를 
	     x 만 포함하는 아이템 집합의 지지도로 나눈값 
	
	예:  
	
	Transaction ID     Items Bought
	         1           우유, 버터, 시리얼
	         2           우유, 시리얼
	         3           우유, 빵
	         4           버터, 맥주, 오징어
	
	 지지도(우유 -> 시리얼) ? 우유와 시리얼을 동시에 구매할 확률 (결합확률)
	
	 신뢰도(우유 -> 시리얼) ? 우유를 구매할때 시리얼도 같이 구매할 조건부 확률 
	                  
	 * 전체 아이템에서 우유와 시리얼이 동시에 출현될 확률은 ?
	
	   답 :     2/4 (50%) 
	
	 * 우유를 샀을때 , 시리얼을 살 조건부 확률은 ?
	
	   답 :  2/3  (66%)
	
	 * 이와는 반대로 시리얼을 샀을때 우유를 동시에 구매할 확률은 ?  
	
	   답 :   2/2  (100%) 
	
	 * 우유와 시리얼을 샀을때 지지도와 신뢰도를 각각 구하시오 !
	
	  답 :                           지지도  ,    신뢰도 
	             우유 ---> 시리얼 :    50%   ,      66%
	             시리얼---> 우유  :    50%   ,      100% 
	
	   
	    우유를 x 라고 하고 시리얼을 y 라고 하면 
	    x 와 y 의 지지도와 신뢰도를 구하는데 모든 아이템들에 대해서
	    다 지지도와 신뢰도를 구한다. 
	
	    그중에 최소 지지도 이상인 데이터만 필터링하고서
	    필터링 된것 중에 신뢰도가 가장 좋은것을 찾는다. 
	    
	 * 어떻게 필터링을 하는가 ?
	
	
	TID     Items
	
	100    A C D
	200    B C E
	300    A B C E
	400    B E
	
	 * 위의 아이템에 대해서 지지도를 산정해보시오 ! 원래 지지도는
	    구매건수/전체 구매건수  로 계산할 수 있지만 여기서는 단순하게
	    아이템 갯수로 처리하시오 !
	
	 답:       아이템      지지도 
	             A           2
	             B           3 
	             C           3 
	             D           1
	             E           3 
	
	  * 이것에 대해서 지지도가 1보다 큰것만 추출해서 다시 정리하시오 !
	
	 
	 답:       아이템      지지도 
	             A           2
	             B           3 
	             C           3 
	             E           3 
	
	  * 이제 아이템들간의 연관규치을 알아야하므로 다시 아이템들간의 조합으로
	    재구성하고 지지도를 다시 구하시오 ! 
	
	  답 :      아이템       지지도 
	            A  B            1
	            A  C            2 
	            A  E            1
	            B  C            2
	            B  E            3
	            C  E            2
	
	참고 :
	
	TID     Items
	
	100    A C D
	200    B C E
	300    A B C E
	400    B E
	
	     *  위의 결과에서 지지도가 1 인것은 제외하시오 ! 
	
	
	  답 :      아이템       지지도 
	          
	            A  C            2 
	            B  C            2
	            B  E            3
	            C  E            2
	
	     *  이제 각각의 아이템 목록에서 첫번째 아이템을 기준으로 동일한것을
	        찾아보시오 
	
	          답 :    아이템 목록    지지도  
	                   B  C  E         2
	
	     질문 :  첫번째 아이템을 기준으로 찾는 이유는 ? 
	             두번째인 C 를 기준으로 보면 
	  
	참고 :
	
	TID     Items
	
	100    A C D
	200    B C E
	300    A B C E
	400    B E






★ apriori 알고리즘 예제 1  (맥주와 기저귀)

	  " 맥주와 기저귀 판매 목록 데이터를 가지고 기저귀를 사면 맥주를 
	    산다는 연관 규칙을 발견하시오 ! "
	
	1. 데이터를 로드한다. 
	
	x <- data.frame(
	beer=c(0,1,1,1,0),
	bread=c(1,1,0,1,1),
	cola=c(0,0,1,0,1),
	diapers=c(0,1,1,1,1),
	eggs=c(0,1,0,0,0),
	milk=c(1,0,1,1,1) )
	
	x 
	
	  beer bread cola diapers eggs milk
	1    0     1    0       0    0    1
	2    1     1    0       1    1    0
	3    1     0    1       1    0    1
	4    1     1    0       1    0    1
	5    0     1    1       1    0    1
	
	
	2. arules 패키지를 설치한다. 
	
	 install.packages("arules")  
	 library(arules)
	
	 trans <-  as.matrix( x, "Transaction") 
	 trans 
	
	3. apriori 함수를 이용해서 연관관계를 분석한다. 
	
	 rules1 <- apriori(trans, parameter=list(supp=0.2, conf=0.6,                    target="rules") )
	
	 rules1
	 
	
	set of 49 rules  <-- 49개의 규칙이 발견되었고
	
	inspect(sort(rules1)) 
	                                     지지도    신뢰도     lift     count
	[5]  {beer}               => {diapers} 0.6     1.0000000  1.2500000 3    
	[6]  {diapers}            => {beer}    0.6     0.7500000  1.2500000 3    
	[7]  {milk}               => {bread}   0.6     0.7500000  0.9375000 3    
	[8]  {bread}              => {milk}    0.6     0.7500000  0.9375000 3    
	[9]  {milk}               => {diapers} 0.6     0.7500000  0.9375000 3    
	[10] {diapers}            => {milk}    0.6     0.7500000  0.9375000 3    
	[11] {bread}              => {diapers} 0.6     0.7500000  0.9375000 3    
	[12] {diapers}            => {bread}   0.6     0.7500000  0.9375000 3    
	[13] {cola}               => {milk}    0.4     1.0000000  1.2500000 2    
	[14] {cola}               => {diapers} 0.4     1.0000000 
	
	설명 :  신뢰도가 클수록 연관관계가 높다는 의미이다.
	        lift 는 상관관계를 나타낸다. 
	        연관규칙을 평가하는 지수는 지지도,신뢰도 말고도 많은데
	        그중에 꽤 많이 쓰이는것이 lift(향상도) 이다. 
	
	 * 위의 맥주와 기저귀 연관 관계를 시각화 하기 
	
	install.packages("sna")
	install.packages("rgl")
	library(sna)
	library(rgl)
	
	
	#visualization
	b2 <- t(as.matrix(trans)) %*% as.matrix(trans) 
	library(sna)
	library(rgl)
	b2.w <- b2 - diag(diag(b2))
	#rownames(b2.w) 
	#colnames(b2.w) 
	gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" , edge.col="blue" , 	boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2) 
	
	






문제251.  상가 건물 데이터의 연관성 분석을 하고 시각화를 하시오 !  
          건물 상가에 서로 연관이 있는 업중이 무엇인가 ?
          건물에 병원이 있으면 약국이 있는가 ?

          " 보습학원이 있는 건물에는 어떤 업종의 매장이 연관되어 있는지
            찾아내시오 ! "


	build <- read.csv("building.csv" , header = T)
	
	build[is.na(build)] <- 0  
	build <- build[-1]
	build 
	
	install.packages("arules")
	library(arules) 
	trans <- as.matrix(build , "Transaction")
	
	rules1 <- apriori(trans , parameter = list(supp=0.2 , conf = 0.6 ,
	               target = "rules"))
	
	rules1 
	
	inspect(sort(rules1))
	
	     lhs                                   rhs              support confidence lift     count
	[1]  {일반음식점}                       => {패밀리레스토랑} 0.40    1.0000000  2.222222 8    
	[2]  {패밀리레스토랑}                   => {일반음식점}     0.40    0.8888889  2.222222 8    
	[3]  {약국}                             => {휴대폰매장}     0.25    1.0000000  3.333333 5    
	[4]  {휴대폰매장}                       => {약국}           0.25    0.8333333  3.333333 5    
	[5]  {약국}                             => {병원}           0.25    1.0000000  3.333333 5    
	[6]  {병원}                             => {약국}           0.25    0.8333333  3.333333 5    
	[7]  {휴대폰매장}                       => {병원}           0.25    0.8333333  2.777778 5    
	[8]  {병원}                             => {휴대폰매장}     0.25    0.8333333  2.777778 5    
	[9]  {편의점}                           => {일반음식점}     0.25    1.0000000  2.500000 5    
	[10] {일반음식점}                       => {편의점}         0.25    0.6250000  2.500000 5    
	[11] {편의점}                           => {패밀리레스토랑} 0.25    1.0000000  2.222222 5    
	[12] {화장품}                           => {패밀리레스토랑} 0.25    0.8333333  1.851852 5    
	[13] {약국,휴대폰매장}                  => {병원}           0.25    1.0000000  3.333333 5    
	[14] {병원,약국}                        => {휴대폰매장}     0.25    1.0000000  3.333333 5    
	[15] {병원,휴대폰매장}                  => {약국}           0.25    1.0000000  4.000000 5    
	[16] {일반음식점,편의점}                => {패밀리레스토랑} 0.25    1.0000000  2.222222 5    
	[17] {패밀리레스토랑,편의점}            => {일반음식점}     0.25    1.0000000  2.500000 5    
	[18] {일반음식점,패밀리레스토랑}        => {편의점}         0.25    0.6250000  2.500000 5    
	[19] {보습학원}                         => {은행}           0.20    1.0000000  5.000000 4    
	[20] {은행}                             => {보습학원}       0.20    1.0000000  5.000000 4    
	[21] {보습학원}                         => {카페}           0.20    1.0000000  4.000000 4    
	[22] {카페}                             => {보습학원}       0.20    0.8000000  4.000000 4    
	[23] {은행}                             => {카페}           0.20    1.0000000  4.000000 4    
	[24] {카페}                             => {은행}           0.20    0.8000000  4.000000 4    
	[25] {당구장}                           => {일반음식점}     0.20    0.8000000  2.000000 4    
	[26] {당구장}                           => {패밀리레스토랑} 0.20    0.8000000  1.777778 4    
	[27] {편의점}                           => {화장품}         0.20    0.8000000  2.666667 4    
	[28] {화장품}                           => {편의점}         0.20    0.6666667  2.666667 4    
	[29] {화장품}                           => {일반음식점}     0.20    0.6666667  1.666667 4    
	[30] {보습학원,은행}                    => {카페}           0.20    1.0000000  4.000000 4    
	[31] {카페,보습학원}                    => {은행}           0.20    1.0000000  5.000000 4    
	[32] {카페,은행}                        => {보습학원}       0.20    1.0000000  5.000000 4    
	[33] {일반음식점,당구장}                => {패밀리레스토랑} 0.20    1.0000000  2.222222 4    
	[34] {패밀리레스토랑,당구장}            => {일반음식점}     0.20    1.0000000  2.500000 4    
	[35] {편의점,화장품}                    => {일반음식점}     0.20    1.0000000  2.500000 4    
	[36] {일반음식점,편의점}                => {화장품}         0.20    0.8000000  2.666667 4    
	[37] {일반음식점,화장품}                => {편의점}         0.20    1.0000000  4.000000 4    
	[38] {편의점,화장품}                    => {패밀리레스토랑} 0.20    1.0000000  2.222222 4    
	[39] {패밀리레스토랑,편의점}            => {화장품}         0.20    0.8000000  2.666667 4    
	[40] {패밀리레스토랑,화장품}            => {편의점}         0.20    0.8000000  3.200000 4    
	[41] {일반음식점,화장품}                => {패밀리레스토랑} 0.20    1.0000000  2.222222 4    
	[42] {패밀리레스토랑,화장품}            => {일반음식점}     0.20    0.8000000  2.000000 4    
	[43] {일반음식점,편의점,화장품}         => {패밀리레스토랑} 0.20    1.0000000  2.222222 4    
	[44] {패밀리레스토랑,편의점,화장품}     => {일반음식점}     0.20    1.0000000  2.500000 4    
	[45] {일반음식점,패밀리레스토랑,편의점} => {화장품}         0.20    0.8000000  2.666667 4    
	[46] {일반음식점,패밀리레스토랑,화장품} => {편의점}         0.20    1.0000000  4.000000 4    
	> 
	
	* 시각화 코드
	
	
	rules2 <- subset(rules1 , subset = lhs %pin% '보습학원' & confidence > 0.7)
	inspect(sort(rules2)) 
	
	rules3 <- subset(rules1 , subset = rhs %pin% '편의점' & confidence > 0.7)
	rules3
	inspect(sort(rules3)) 
	
	#visualization
	b2 <- t(as.matrix(build)) %*% as.matrix(build) 
	install.packages("sna")
	install.packages("rgl")
	library(sna)
	library(rgl)
	b2.w <- b2 - diag(diag(b2))
	#rownames(b2.w) 
	#colnames(b2.w) 
	gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" , edge.col="blue" , 	boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2) 






★ 영화 라라랜드의 긍정적 평가와 부정적 평가에 대한 워드 클라우드를 그리고
   연관성 분석을 하는 테스트 




★ 영화 라라랜드의 긍정적 평가와 부정적 평가에 대한  워드 클라우드 


	1.  라라랜드 데이터를 로드한다.  
	
	library(KoNLP)
	library(wordcloud)
	
	lala <- read.csv('라라랜드.csv', header=T, stringsAsFactors = F)
	
	2. 영화평점이 9점이상은 긍정변수넣고 2점 이하는 부정변수에 넣는다 
	
	lala_positive <- lala[lala$score>=9,c('content')]
	lala_negative <- lala[lala$score<=2,c('content')]
	
	head(lala_positive)
	head(lala_negative)
	
	3. 긍정게시판 변수에서 명사만 추출하고 데이터 정제 작업을 한다.
	
	po <- sapply(lala_positive, extractNoun, USE.NAMES=F)
	po2 <- unlist(po)
	po2 <- Filter(function(x){nchar(x)>=2},po2)
	po3 <- gsub('\\d+','',po2)
	po3 <- gsub('관람객','',po3)
	po3 <- gsub('평점', '', po3)
	po3 <- gsub('영화', '', po3)
	po3 <- gsub('진짜', '', po3)
	po3 <- gsub('완전', '', po3)
	po3 <- gsub('시간', '', po3)
	po3 <- gsub('올해', '', po3)
	po3 <- gsub('장면', '', po3)
	po3 <- gsub('남자', '', po3)
	po3 <- gsub('여자', '', po3)
	po3 <- gsub('만큼', '', po3)
	po3 <- gsub('니가', '', po3)
	po3 <- gsub('년대', '', po3)
	po3 <- gsub('옆사람', '', po3)
	po3 <- gsub('들이', '', po3)
	po3 <- gsub('저녁', '', po3)
	
	write(unlist(po3), 'lala_positive.txt')
	po4 <- read.table('lala_positive.txt')
	po_wordcount <- table(po4)
	
	
	4. 라라랜드 영화에 부정적인 평가 게시글들 명사로 변경하고
	   정재 작업을 수행한다. 
	
	ne <- sapply(lala_negative, extractNoun, USE.NAMES=F)
	ne2 <- unlist(ne)
	ne2 <- Filter(function(x){nchar(x)>=2},ne2)
	ne3 <- gsub('\\d+','',ne2)
	ne3 <- gsub('관람객','',ne3)
	ne3 <- gsub('평점', '', ne3)
	ne3 <- gsub('영화', '', ne3)
	ne3 <- gsub('진짜', '', ne3)
	ne3 <- gsub('완전', '', ne3)
	ne3 <- gsub('시간', '', ne3)
	ne3 <- gsub('올해', '', ne3)
	ne3 <- gsub('장면', '', ne3)
	ne3 <- gsub('남자', '', ne3)
	ne3 <- gsub('여자', '', ne3)
	ne3 <- gsub('만큼', '', ne3)
	ne3 <- gsub('니가', '', ne3)
	ne3 <- gsub('년대', '', ne3)
	ne3 <- gsub('옆사람', '', ne3)
	ne3 <- gsub('들이', '', ne3)
	ne3 <- gsub('저녁', '', ne3)
	
	write(unlist(ne3), 'lala_negative.txt')
	ne4 <- read.table('lala_negative.txt')
	ne_wordcount <- table(ne4)
	
	
	5. 긍정 단어와 부정단어를 각각 워드 클라우드로 그려서 한 화면에
	   출력한다. 
	
	graphics.off()
	palete <- brewer.pal(9,'Set1')
	par(new=T, mfrow=c(1,2))
	
	wordcloud(names(po_wordcount), freq=po_wordcount, scale=c(3,1), rot.per=0.1, random.order = F,
	          random.color = T, col=rainbow(15))
	title(main='라라랜드의 긍정적인 평가', col.main='blue')
	
	wordcloud(names(ne_wordcount), freq=ne_wordcount, scale=c(3,1), rot.per=0.1, random.order = F,
	          random.color = T, col=rainbow(15))
	title(main='라라랜드의 부정적인 평가', col.main='red')
	
	
	





문제252. 라라랜드의 긍정적 평가 게시판의 글들을 명사만
         추출한 다음 단어들간의 연관관계를 출력하시오


	답 :
	
	1. 관련된 패키지 설치 
	
	library(KoNLP)
	library(wordcloud)
	library(tm)
	library(stringr)
	library(arules)
	
	2. 명사 추출하는 코드 
	lala_positive <- sapply(lala_positive, extractNoun, USE.NAMES=F)
	head(lala_positive)
	
	3. unlist 로 변환한후에 철자가 2개이상이고 5개 이하인
	   것만 추출 
	
	c <- unlist(lala_positive)
	lala_positive2 <- Filter(function(x) { nchar(x) >= 2 & 
	                               nchar(x) <= 5 }  , c)
	
	4. 데이터 정재작업( 분석하기에 너무 많이 나오는 단어를
	    삭제하는 작업 )
	
	# 숫자제거 
	lala_positive2 <- gsub('\\d+','',lala_positive2)
	
	
	lala_positive2 <- gsub('관람객','',lala_positive2)
	lala_positive2 <- gsub('평점', '', lala_positive2)
	lala_positive2 <- gsub('영화', '', lala_positive2)
	lala_positive2 <- gsub('진짜', '', lala_positive2)
	lala_positive2 <- gsub('완전', '', lala_positive2)
	lala_positive2 <- gsub('시간', '', lala_positive2)
	lala_positive2 <- gsub('올해', '', lala_positive2)
	lala_positive2 <- gsub('장면', '', lala_positive2)
	lala_positive2 <- gsub('남자', '', lala_positive2)
	lala_positive2 <- gsub('여자', '', lala_positive2)
	lala_positive2 <- gsub('만큼', '', lala_positive2)
	lala_positive2 <- gsub('니가', '', lala_positive2)
	lala_positive2 <- gsub('년대', '', lala_positive2)
	lala_positive2 <- gsub('옆사람', '', lala_positive2)
	lala_positive2 <- gsub('들이', '', lala_positive2)
	lala_positive2 <- gsub('저녁', '', lala_positive2)
	lala_positive2 <- gsub('영화', '', lala_positive2)
	
	lala_positive2 
	
	5. 한글이 아닌 데이터를 제거하는 작업 
	res <- str_replace_all(lala_positive2, "[^[:alpha:]]","")
	
	6.  ""  데이터 제거하는 작업 
		res <- res[res != ""] 
	
	7. 단어와 그 건수를 출력하는 작업 
	wordcount <- table(res)
	wordcount2 <- sort( table(res), decreasing=T)
		
	8. 단어의 건수가 100 보다 큰것만 필터링 
	keyword <- names( wordcount2[wordcount2>100] )
	length(lala_positive)
	
	9. 아프리오리 분석을 위해서 표형태로 만드는 작업 
	contents <- c()
	for(i in 1:length(lala_positive)) { 
	  inter <- intersect(lala_positive[[i]] , keyword)
	  contents <- rbind(contents ,table(inter)[keyword])
	}
	
	10. 표의 컬럼명에 단어가 들어가게한다.
	colnames(contents) <- keyword
	
	11. na 를 숫자 0 으로 변경한다. 
	contents[which(is.na(contents))] <- 0 
	
	dim(lala_positive)
	
	12. 아프리오리 데이터 분석
	
	detach(package:tm, unload=T)
	library(arules) 
	rules_lala <- apriori(contents , parameter = list(supp = 0.007 , conf = 0.3 , target = "rules"))
	rules_lala 
	inspect(sort(rules_lala ))